pair_id,insight,paper1_scores,paper2_scores,joint_scores,no_context_scores,contrastive_loss,paper1_scores_avg,paper2_scores_avg,joint_scores_avg,no_context_scores_avg,abstract,joint-paper1-paper2,joint+nocontext-paper1-paper2,joint-max_nocontext_paper1_paper2,contrastive_loss_avg,jointavg-paper1avg-paper2avg,jointavg+nocontextavg-paper1avg-paper2avg,jointavg-max_nocontextavg_paper1avg_paper2avg,forum_id_1,forum_id_2,keywords,if_keep,abstract1,abstract2,pair_insight_id
1,"<insight>The effectiveness of incorporating new information from synthetic sources into a model, theoretically determined by the information gain from those sources, may be enhanced by using update methods that adapt internal representations based on the input, thereby potentially improving both the integration of new knowledge and the preservation of existing, unrelated capabilities.</insight>",-471.0625,-689.03125,-722.5625,-654.484375,1092.015625,-8.26425457,-12.0882673264,-12.6765346527,-11.4821815491,"['TOWARDS A THEORETICAL UNDERSTANDING OF SYN-\nTHETIC DATA IN LLM POST-TRAINING:\nA REVERSE-BOTTLENECK PERSPECTIVE\n\nZeyu Gan, Yong Liu∗\nGaoling School of Artificial Intelligence\nRenmin University of China\nBeijing, China\n{zygan,liuyonggsai}@ruc.edu.cn\n\nABSTRACT\n\nSynthetic data has become a pivotal resource in post-training tasks for large lan-\nguage models (LLMs) due to the scarcity of high-quality, specific data. While\nvarious methods have been developed to generate synthetic data, there remains a\ndiscernible gap between the practical effects of synthetic data and our theoretical\ncomprehension. To address this challenge, we commence by presenting a detailed\nmodeling of the prevalent synthetic data generation process. Building upon this\nmodeling, we demonstrate that the generalization capability of the post-trained\nmodel is critically determined by the information gain derived from the generative\nmodel, as analyzed from a novel reverse-bottleneck perspective. Moreover, we in-\ntroduce the concept of Generalization Gain via Mutual Information (GGMI) and\nelucidate the relationship between generalization gain and information gain. This\nanalysis serves as a theoretical foundation for synthetic data generation and further\nhighlights its connection with the generalization capability of post-trained models,\noffering an understanding about the design of synthetic data generation techniques\nand the optimization of the post-training process. We open-source our code at\nhttps://github.com/ZyGan1999/Towards-a-Theoretical-U\nnderstanding-of-Synthetic-Data-in-LLM-Post-Training.', 'UNLOCKING EFFICIENT, SCALABLE, AND CONTIN-\nUAL KNOWLEDGE EDITING WITH BASIS-LEVEL REP-\nRESENTATION FINE-TUNING\n\nTianci Liu1, Ruirui Li2, Yunzhe Qi3, Hui Liu2, Xianfeng Tang2, Tianqi Zheng2, Qingyu Yin2,\nMonica Cheng2, Jun Huan4, Haoyu Wang5, Jing Gao1\n1Purdue University\n1{liu3351,jinggao}@purdue.edu\n\n2ruirul@amazon.com\n\n3UIUC 4AWS AI Lab\n\n5SUNY Albany\n\n2Amazon\n\n5hwang28@albany.edu\n\nABSTRACT\n\nLarge language models (LLMs) have achieved remarkable performance on vari-\nous natural language tasks. However, they are trained on static corpora and their\nknowledge can become outdated quickly in the fast-changing world. This moti-\nvates the development of knowledge editing methods designed to update certain\nknowledge in LLMs without changing unrelated others. To make selective edits,\nprevious efforts often sought to update a small amount of parameters in some spe-\ncific layer(s) of a LLM. Nonetheless, in challenging scenarios, they still fall short\nin making successful edits while preserving knowledge irrelevant to the updates\nsimultaneously, resulting in a notable editing-locality trade-off. In this work, we\nquestion if the trade-offs are caused by the fact that parameter-based updates have\na global effect, i.e., edited parameters affect all inputs indiscriminately. In light of\nthis, we explore the feasibility of representation fine-tuning, which applied some\nlinear update to a few representations in a learned subspace, for knowledge edit-\ning. While being effective to enhance an LLM’s general ability as demonstrated in\nthe previous work, we theoretically show that this linear update imposes a tension\nin editing-locality trade-off. Subsequently, BaFT is proposed to break the linear-\nity. BaFT computes a weight for each basis that spans a dimension of the subspace\nbased on the input representation. This input-dependent weighting mechanism al-\nlows BaFT to manage different types of knowledge in an adaptive way, thereby\nachieving a better editing-locality trade-off. Experiments on three LLMs with five\nediting benchmarks in diverse scenarios show the superiority of our method.']",437.53125,-216.953125,-251.5,19.1581687928,7.675987243700002,-3.8061943053999965,-4.412280082699999,UxkznlcnHf,PITFO1ddeh,"['large language models', 'synthetic data', 'information bottleneck', 'knowledge editing', 'representation fine-tuning', 'large language models']",True,"TOWARDS A THEORETICAL UNDERSTANDING OF SYN-
THETIC DATA IN LLM POST-TRAINING:
A REVERSE-BOTTLENECK PERSPECTIVE

Zeyu Gan, Yong Liu∗
Gaoling School of Artificial Intelligence
Renmin University of China
Beijing, China
{zygan,liuyonggsai}@ruc.edu.cn

ABSTRACT

Synthetic data has become a pivotal resource in post-training tasks for large lan-
guage models (LLMs) due to the scarcity of high-quality, specific data. While
various methods have been developed to generate synthetic data, there remains a
discernible gap between the practical effects of synthetic data and our theoretical
comprehension. To address this challenge, we commence by presenting a detailed
modeling of the prevalent synthetic data generation process. Building upon this
modeling, we demonstrate that the generalization capability of the post-trained
model is critically determined by the information gain derived from the generative
model, as analyzed from a novel reverse-bottleneck perspective. Moreover, we in-
troduce the concept of Generalization Gain via Mutual Information (GGMI) and
elucidate the relationship between generalization gain and information gain. This
analysis serves as a theoretical foundation for synthetic data generation and further
highlights its connection with the generalization capability of post-trained models,
offering an understanding about the design of synthetic data generation techniques
and the optimization of the post-training process. We open-source our code at
https://github.com/ZyGan1999/Towards-a-Theoretical-U
nderstanding-of-Synthetic-Data-in-LLM-Post-Training.","UNLOCKING EFFICIENT, SCALABLE, AND CONTIN-
UAL KNOWLEDGE EDITING WITH BASIS-LEVEL REP-
RESENTATION FINE-TUNING

Tianci Liu1, Ruirui Li2, Yunzhe Qi3, Hui Liu2, Xianfeng Tang2, Tianqi Zheng2, Qingyu Yin2,
Monica Cheng2, Jun Huan4, Haoyu Wang5, Jing Gao1
1Purdue University
1{liu3351,jinggao}@purdue.edu

2ruirul@amazon.com

3UIUC 4AWS AI Lab

5SUNY Albany

2Amazon

5hwang28@albany.edu

ABSTRACT

Large language models (LLMs) have achieved remarkable performance on vari-
ous natural language tasks. However, they are trained on static corpora and their
knowledge can become outdated quickly in the fast-changing world. This moti-
vates the development of knowledge editing methods designed to update certain
knowledge in LLMs without changing unrelated others. To make selective edits,
previous efforts often sought to update a small amount of parameters in some spe-
cific layer(s) of a LLM. Nonetheless, in challenging scenarios, they still fall short
in making successful edits while preserving knowledge irrelevant to the updates
simultaneously, resulting in a notable editing-locality trade-off. In this work, we
question if the trade-offs are caused by the fact that parameter-based updates have
a global effect, i.e., edited parameters affect all inputs indiscriminately. In light of
this, we explore the feasibility of representation fine-tuning, which applied some
linear update to a few representations in a learned subspace, for knowledge edit-
ing. While being effective to enhance an LLM’s general ability as demonstrated in
the previous work, we theoretically show that this linear update imposes a tension
in editing-locality trade-off. Subsequently, BaFT is proposed to break the linear-
ity. BaFT computes a weight for each basis that spans a dimension of the subspace
based on the input representation. This input-dependent weighting mechanism al-
lows BaFT to manage different types of knowledge in an adaptive way, thereby
achieving a better editing-locality trade-off. Experiments on three LLMs with five
editing benchmarks in diverse scenarios show the superiority of our method.",0
1,"<insight>Employing input-dependent mechanisms to adjust internal representations during post-training could enhance a model's ability to effectively assimilate the information gain provided by synthetic data, thereby improving its overall generalization capabilities.</insight>",-330.09375,-393.5,-425.234375,-324.6484375,623.0078125,-8.6866779327,-10.3552627563,-11.1903781891,-8.5433797836,"['TOWARDS A THEORETICAL UNDERSTANDING OF SYN-\nTHETIC DATA IN LLM POST-TRAINING:\nA REVERSE-BOTTLENECK PERSPECTIVE\n\nZeyu Gan, Yong Liu∗\nGaoling School of Artificial Intelligence\nRenmin University of China\nBeijing, China\n{zygan,liuyonggsai}@ruc.edu.cn\n\nABSTRACT\n\nSynthetic data has become a pivotal resource in post-training tasks for large lan-\nguage models (LLMs) due to the scarcity of high-quality, specific data. While\nvarious methods have been developed to generate synthetic data, there remains a\ndiscernible gap between the practical effects of synthetic data and our theoretical\ncomprehension. To address this challenge, we commence by presenting a detailed\nmodeling of the prevalent synthetic data generation process. Building upon this\nmodeling, we demonstrate that the generalization capability of the post-trained\nmodel is critically determined by the information gain derived from the generative\nmodel, as analyzed from a novel reverse-bottleneck perspective. Moreover, we in-\ntroduce the concept of Generalization Gain via Mutual Information (GGMI) and\nelucidate the relationship between generalization gain and information gain. This\nanalysis serves as a theoretical foundation for synthetic data generation and further\nhighlights its connection with the generalization capability of post-trained models,\noffering an understanding about the design of synthetic data generation techniques\nand the optimization of the post-training process. We open-source our code at\nhttps://github.com/ZyGan1999/Towards-a-Theoretical-U\nnderstanding-of-Synthetic-Data-in-LLM-Post-Training.', 'UNLOCKING EFFICIENT, SCALABLE, AND CONTIN-\nUAL KNOWLEDGE EDITING WITH BASIS-LEVEL REP-\nRESENTATION FINE-TUNING\n\nTianci Liu1, Ruirui Li2, Yunzhe Qi3, Hui Liu2, Xianfeng Tang2, Tianqi Zheng2, Qingyu Yin2,\nMonica Cheng2, Jun Huan4, Haoyu Wang5, Jing Gao1\n1Purdue University\n1{liu3351,jinggao}@purdue.edu\n\n2ruirul@amazon.com\n\n3UIUC 4AWS AI Lab\n\n5SUNY Albany\n\n2Amazon\n\n5hwang28@albany.edu\n\nABSTRACT\n\nLarge language models (LLMs) have achieved remarkable performance on vari-\nous natural language tasks. However, they are trained on static corpora and their\nknowledge can become outdated quickly in the fast-changing world. This moti-\nvates the development of knowledge editing methods designed to update certain\nknowledge in LLMs without changing unrelated others. To make selective edits,\nprevious efforts often sought to update a small amount of parameters in some spe-\ncific layer(s) of a LLM. Nonetheless, in challenging scenarios, they still fall short\nin making successful edits while preserving knowledge irrelevant to the updates\nsimultaneously, resulting in a notable editing-locality trade-off. In this work, we\nquestion if the trade-offs are caused by the fact that parameter-based updates have\na global effect, i.e., edited parameters affect all inputs indiscriminately. In light of\nthis, we explore the feasibility of representation fine-tuning, which applied some\nlinear update to a few representations in a learned subspace, for knowledge edit-\ning. While being effective to enhance an LLM’s general ability as demonstrated in\nthe previous work, we theoretically show that this linear update imposes a tension\nin editing-locality trade-off. Subsequently, BaFT is proposed to break the linear-\nity. BaFT computes a weight for each basis that spans a dimension of the subspace\nbased on the input representation. This input-dependent weighting mechanism al-\nlows BaFT to manage different types of knowledge in an adaptive way, thereby\nachieving a better editing-locality trade-off. Experiments on three LLMs with five\nediting benchmarks in diverse scenarios show the superiority of our method.']",298.359375,-26.2890625,-100.5859375,16.3949422835,7.8515624999,-0.6918172837000007,-2.6469984054999998,UxkznlcnHf,PITFO1ddeh,"['large language models', 'synthetic data', 'information bottleneck', 'knowledge editing', 'representation fine-tuning', 'large language models']",True,"TOWARDS A THEORETICAL UNDERSTANDING OF SYN-
THETIC DATA IN LLM POST-TRAINING:
A REVERSE-BOTTLENECK PERSPECTIVE

Zeyu Gan, Yong Liu∗
Gaoling School of Artificial Intelligence
Renmin University of China
Beijing, China
{zygan,liuyonggsai}@ruc.edu.cn

ABSTRACT

Synthetic data has become a pivotal resource in post-training tasks for large lan-
guage models (LLMs) due to the scarcity of high-quality, specific data. While
various methods have been developed to generate synthetic data, there remains a
discernible gap between the practical effects of synthetic data and our theoretical
comprehension. To address this challenge, we commence by presenting a detailed
modeling of the prevalent synthetic data generation process. Building upon this
modeling, we demonstrate that the generalization capability of the post-trained
model is critically determined by the information gain derived from the generative
model, as analyzed from a novel reverse-bottleneck perspective. Moreover, we in-
troduce the concept of Generalization Gain via Mutual Information (GGMI) and
elucidate the relationship between generalization gain and information gain. This
analysis serves as a theoretical foundation for synthetic data generation and further
highlights its connection with the generalization capability of post-trained models,
offering an understanding about the design of synthetic data generation techniques
and the optimization of the post-training process. We open-source our code at
https://github.com/ZyGan1999/Towards-a-Theoretical-U
nderstanding-of-Synthetic-Data-in-LLM-Post-Training.","UNLOCKING EFFICIENT, SCALABLE, AND CONTIN-
UAL KNOWLEDGE EDITING WITH BASIS-LEVEL REP-
RESENTATION FINE-TUNING

Tianci Liu1, Ruirui Li2, Yunzhe Qi3, Hui Liu2, Xianfeng Tang2, Tianqi Zheng2, Qingyu Yin2,
Monica Cheng2, Jun Huan4, Haoyu Wang5, Jing Gao1
1Purdue University
1{liu3351,jinggao}@purdue.edu

2ruirul@amazon.com

3UIUC 4AWS AI Lab

5SUNY Albany

2Amazon

5hwang28@albany.edu

ABSTRACT

Large language models (LLMs) have achieved remarkable performance on vari-
ous natural language tasks. However, they are trained on static corpora and their
knowledge can become outdated quickly in the fast-changing world. This moti-
vates the development of knowledge editing methods designed to update certain
knowledge in LLMs without changing unrelated others. To make selective edits,
previous efforts often sought to update a small amount of parameters in some spe-
cific layer(s) of a LLM. Nonetheless, in challenging scenarios, they still fall short
in making successful edits while preserving knowledge irrelevant to the updates
simultaneously, resulting in a notable editing-locality trade-off. In this work, we
question if the trade-offs are caused by the fact that parameter-based updates have
a global effect, i.e., edited parameters affect all inputs indiscriminately. In light of
this, we explore the feasibility of representation fine-tuning, which applied some
linear update to a few representations in a learned subspace, for knowledge edit-
ing. While being effective to enhance an LLM’s general ability as demonstrated in
the previous work, we theoretically show that this linear update imposes a tension
in editing-locality trade-off. Subsequently, BaFT is proposed to break the linear-
ity. BaFT computes a weight for each basis that spans a dimension of the subspace
based on the input representation. This input-dependent weighting mechanism al-
lows BaFT to manage different types of knowledge in an adaptive way, thereby
achieving a better editing-locality trade-off. Experiments on three LLMs with five
editing benchmarks in diverse scenarios show the superiority of our method.",1
7,"<insight>Understanding the emergent generalization properties arising from the interplay of aggregation, fine-tuning, and optimization steps in complex, iterative learning systems requires reasoning capabilities similar to those needed for tracking the indirect, cumulative effects (ramifications) of sequential actions in dynamic environments.</insight>",-687.625,-674.09375,-694.0,-694.0,1361.71875,-12.9740562439,-12.71875,-13.0943393707,-13.0943393707,"['ACTIONREASONINGBENCH: REASONING ABOUT AC-\nTIONS WITH AND WITHOUT RAMIFICATION CON-\nSTRAINTS\n\nDivij Handa∗1, Pavel Dolin∗1, Shrinidhi Kumbhar∗ 1, Tran Cao Son2, Chitta Baral1\n1Arizona State University, 2New Mexico State University\n{dhanda,pdolin,skumbha4,chitta}@asu.edu, stran@nmsu.edu\n\nABSTRACT\n\nReasoning about Actions and Change (RAC) has historically played a pivotal role\nin solving foundational AI problems, such as the frame problem. It has driven\nadvancements in AI fields, such as non-monotonic and commonsense reasoning.\nRAC remains crucial for AI systems that operate in dynamic environments, en-\ngage in interactive scenarios, or rely on commonsense reasoning. Despite sub-\nstantial advances made by Large Language Models (LLMs) in various AI do-\nmains, their performance in RAC remains underexplored. To address this gap,\nwe introduce a new diagnostic benchmark, ACTIONREASONINGBENCH, which\nencompasses 8 domains and includes questions for up to 19 action sequences.\nThis benchmark rigorously evaluates LLMs across six key RAC dimensions: Flu-\nent Tracking, State Tracking, Action Executability, Effects of Actions, Numerical\nRAC, and Composite Questions. LLMs demonstrate average accuracy rates of\n73.55%, 65.63%, 58.73%, and 62.38% on the former four dimensions, which are\nfrequently discussed in RAC literature. However, the performance on the lat-\nter two dimensions, which introduce complex and novel reasoning questions, the\naverage performance of LLMs is lowered to 33.16% and 51.19%, respectively,\nreflecting a 17.9% performance decline. We also introduce new ramification con-\nstraints to capture the indirect effects of actions, providing deeper insights into\nRAC challenges. Our evaluation of state-of-the-art LLMs, including both open-\nsource and commercial models, reveals challenges across all RAC dimensions,\nparticularly in handling ramifications, with GPT-4o failing to solve any question\nand o1-preview achieving a score of only 18.4%.', 'UNDERSTANDING THE STABILITY-BASED GENERAL-\nIZATION OF PERSONALIZED FEDERATED LEARNING\n\nJie Tan4 Yifan Shi Li Shen1,2∗ Xiaochun Cao1\n\nYingqi Liu1,2 Qinglun Li3\n1School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, China\n2Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), China\n3College of Systems Engineering, National University of Defense Technology, China\n4Intelligent Game and Decision Lab, China\nliuyingqi1199@gmail.com; liqinglun@nudt.edu.cn; j.tanjie@outlook.com\nmathshenli@gmail.com; caoxiaochun@mail.sysu.edu.cn\n\nABSTRACT\n\nDespite great achievements in algorithm design for Personalized Federated Learn-\ning (PFL), research on the theoretical analysis of generalization is still in its early\nstages. Some theoretical results have investigated the generalization performance of\npersonalized models under the problem setting and hypothesis in convex conditions,\nwhich can not reflect the real iteration performance during non-convex training. To\nfurther understand the real performance from a generalization perspective, we pro-\npose the first algorithm-dependent generalization analysis with uniform stability for\nthe typical PFL method, Partial Model Personalization, on smooth and non-convex\nobjectives. Specifically, we decompose the generalization errors into aggregation\nerrors and fine-tuning errors, then creatively establish a generalization analysis\nframework corresponding to the gradient estimation process of the personalized\ntraining. This framework builds up the bridge among PFL, FL and Pure Local Train-\ning for personalized aims in heterogeneous scenarios, which clearly demonstrates\nthe effectiveness of PFL from the generalization perspective. Moreover, we demon-\nstrate the impact of trivial factors like learning steps, stepsizes and communication\ntopologies and obtain the excess risk analysis with optimization errors for PFL.\nPromising experiments on CIFAR datasets also corroborate our theoretical insights.\nOur code can be seen in https://github.com/YingqiLiu1999/Understanding-the-\nStability-based-Generalization-of-Personalized-Federated-Learning.']",667.71875,-26.28125,-19.90625,25.6928062439,12.5984668732,-0.4958724975000006,-0.3755893707000002,NUD03NBDOE,znhZbonEoe,"['reasoning about actions and change (rac)', 'benchmark', 'large language models (llms)', 'stability analysis+generalization gap+excess risk+personalized federated learning']",True,"ACTIONREASONINGBENCH: REASONING ABOUT AC-
TIONS WITH AND WITHOUT RAMIFICATION CON-
STRAINTS

Divij Handa∗1, Pavel Dolin∗1, Shrinidhi Kumbhar∗ 1, Tran Cao Son2, Chitta Baral1
1Arizona State University, 2New Mexico State University
{dhanda,pdolin,skumbha4,chitta}@asu.edu, stran@nmsu.edu

ABSTRACT

Reasoning about Actions and Change (RAC) has historically played a pivotal role
in solving foundational AI problems, such as the frame problem. It has driven
advancements in AI fields, such as non-monotonic and commonsense reasoning.
RAC remains crucial for AI systems that operate in dynamic environments, en-
gage in interactive scenarios, or rely on commonsense reasoning. Despite sub-
stantial advances made by Large Language Models (LLMs) in various AI do-
mains, their performance in RAC remains underexplored. To address this gap,
we introduce a new diagnostic benchmark, ACTIONREASONINGBENCH, which
encompasses 8 domains and includes questions for up to 19 action sequences.
This benchmark rigorously evaluates LLMs across six key RAC dimensions: Flu-
ent Tracking, State Tracking, Action Executability, Effects of Actions, Numerical
RAC, and Composite Questions. LLMs demonstrate average accuracy rates of
73.55%, 65.63%, 58.73%, and 62.38% on the former four dimensions, which are
frequently discussed in RAC literature. However, the performance on the lat-
ter two dimensions, which introduce complex and novel reasoning questions, the
average performance of LLMs is lowered to 33.16% and 51.19%, respectively,
reflecting a 17.9% performance decline. We also introduce new ramification con-
straints to capture the indirect effects of actions, providing deeper insights into
RAC challenges. Our evaluation of state-of-the-art LLMs, including both open-
source and commercial models, reveals challenges across all RAC dimensions,
particularly in handling ramifications, with GPT-4o failing to solve any question
and o1-preview achieving a score of only 18.4%.","UNDERSTANDING THE STABILITY-BASED GENERAL-
IZATION OF PERSONALIZED FEDERATED LEARNING

Jie Tan4 Yifan Shi Li Shen1,2∗ Xiaochun Cao1

Yingqi Liu1,2 Qinglun Li3
1School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, China
2Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), China
3College of Systems Engineering, National University of Defense Technology, China
4Intelligent Game and Decision Lab, China
liuyingqi1199@gmail.com; liqinglun@nudt.edu.cn; j.tanjie@outlook.com
mathshenli@gmail.com; caoxiaochun@mail.sysu.edu.cn

ABSTRACT

Despite great achievements in algorithm design for Personalized Federated Learn-
ing (PFL), research on the theoretical analysis of generalization is still in its early
stages. Some theoretical results have investigated the generalization performance of
personalized models under the problem setting and hypothesis in convex conditions,
which can not reflect the real iteration performance during non-convex training. To
further understand the real performance from a generalization perspective, we pro-
pose the first algorithm-dependent generalization analysis with uniform stability for
the typical PFL method, Partial Model Personalization, on smooth and non-convex
objectives. Specifically, we decompose the generalization errors into aggregation
errors and fine-tuning errors, then creatively establish a generalization analysis
framework corresponding to the gradient estimation process of the personalized
training. This framework builds up the bridge among PFL, FL and Pure Local Train-
ing for personalized aims in heterogeneous scenarios, which clearly demonstrates
the effectiveness of PFL from the generalization perspective. Moreover, we demon-
strate the impact of trivial factors like learning steps, stepsizes and communication
topologies and obtain the excess risk analysis with optimization errors for PFL.
Promising experiments on CIFAR datasets also corroborate our theoretical insights.
Our code can be seen in https://github.com/YingqiLiu1999/Understanding-the-
Stability-based-Generalization-of-Personalized-Federated-Learning.",2
7,"<insight>Ensuring the stability and predictability of computational reasoning, especially when dealing with indirect consequences and evolving states, may be fundamental to achieving reliable performance in artificial intelligence systems designed for complex, dynamic situations.</insight>",-423.84375,-408.546875,-438.09375,-427.34375,821.640625,-10.8677883148,-10.475561142,-11.2331733704,-10.957531929,"['ACTIONREASONINGBENCH: REASONING ABOUT AC-\nTIONS WITH AND WITHOUT RAMIFICATION CON-\nSTRAINTS\n\nDivij Handa∗1, Pavel Dolin∗1, Shrinidhi Kumbhar∗ 1, Tran Cao Son2, Chitta Baral1\n1Arizona State University, 2New Mexico State University\n{dhanda,pdolin,skumbha4,chitta}@asu.edu, stran@nmsu.edu\n\nABSTRACT\n\nReasoning about Actions and Change (RAC) has historically played a pivotal role\nin solving foundational AI problems, such as the frame problem. It has driven\nadvancements in AI fields, such as non-monotonic and commonsense reasoning.\nRAC remains crucial for AI systems that operate in dynamic environments, en-\ngage in interactive scenarios, or rely on commonsense reasoning. Despite sub-\nstantial advances made by Large Language Models (LLMs) in various AI do-\nmains, their performance in RAC remains underexplored. To address this gap,\nwe introduce a new diagnostic benchmark, ACTIONREASONINGBENCH, which\nencompasses 8 domains and includes questions for up to 19 action sequences.\nThis benchmark rigorously evaluates LLMs across six key RAC dimensions: Flu-\nent Tracking, State Tracking, Action Executability, Effects of Actions, Numerical\nRAC, and Composite Questions. LLMs demonstrate average accuracy rates of\n73.55%, 65.63%, 58.73%, and 62.38% on the former four dimensions, which are\nfrequently discussed in RAC literature. However, the performance on the lat-\nter two dimensions, which introduce complex and novel reasoning questions, the\naverage performance of LLMs is lowered to 33.16% and 51.19%, respectively,\nreflecting a 17.9% performance decline. We also introduce new ramification con-\nstraints to capture the indirect effects of actions, providing deeper insights into\nRAC challenges. Our evaluation of state-of-the-art LLMs, including both open-\nsource and commercial models, reveals challenges across all RAC dimensions,\nparticularly in handling ramifications, with GPT-4o failing to solve any question\nand o1-preview achieving a score of only 18.4%.', 'UNDERSTANDING THE STABILITY-BASED GENERAL-\nIZATION OF PERSONALIZED FEDERATED LEARNING\n\nJie Tan4 Yifan Shi Li Shen1,2∗ Xiaochun Cao1\n\nYingqi Liu1,2 Qinglun Li3\n1School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, China\n2Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), China\n3College of Systems Engineering, National University of Defense Technology, China\n4Intelligent Game and Decision Lab, China\nliuyingqi1199@gmail.com; liqinglun@nudt.edu.cn; j.tanjie@outlook.com\nmathshenli@gmail.com; caoxiaochun@mail.sysu.edu.cn\n\nABSTRACT\n\nDespite great achievements in algorithm design for Personalized Federated Learn-\ning (PFL), research on the theoretical analysis of generalization is still in its early\nstages. Some theoretical results have investigated the generalization performance of\npersonalized models under the problem setting and hypothesis in convex conditions,\nwhich can not reflect the real iteration performance during non-convex training. To\nfurther understand the real performance from a generalization perspective, we pro-\npose the first algorithm-dependent generalization analysis with uniform stability for\nthe typical PFL method, Partial Model Personalization, on smooth and non-convex\nobjectives. Specifically, we decompose the generalization errors into aggregation\nerrors and fine-tuning errors, then creatively establish a generalization analysis\nframework corresponding to the gradient estimation process of the personalized\ntraining. This framework builds up the bridge among PFL, FL and Pure Local Train-\ning for personalized aims in heterogeneous scenarios, which clearly demonstrates\nthe effectiveness of PFL from the generalization perspective. Moreover, we demon-\nstrate the impact of trivial factors like learning steps, stepsizes and communication\ntopologies and obtain the excess risk analysis with optimization errors for PFL.\nPromising experiments on CIFAR datasets also corroborate our theoretical insights.\nOur code can be seen in https://github.com/YingqiLiu1999/Understanding-the-\nStability-based-Generalization-of-Personalized-Federated-Learning.']",394.296875,-33.046875,-29.546875,21.0677080154,10.110176086400001,-0.8473558426000007,-0.7576122283999993,NUD03NBDOE,znhZbonEoe,"['reasoning about actions and change (rac)', 'benchmark', 'large language models (llms)', 'stability analysis+generalization gap+excess risk+personalized federated learning']",True,"ACTIONREASONINGBENCH: REASONING ABOUT AC-
TIONS WITH AND WITHOUT RAMIFICATION CON-
STRAINTS

Divij Handa∗1, Pavel Dolin∗1, Shrinidhi Kumbhar∗ 1, Tran Cao Son2, Chitta Baral1
1Arizona State University, 2New Mexico State University
{dhanda,pdolin,skumbha4,chitta}@asu.edu, stran@nmsu.edu

ABSTRACT

Reasoning about Actions and Change (RAC) has historically played a pivotal role
in solving foundational AI problems, such as the frame problem. It has driven
advancements in AI fields, such as non-monotonic and commonsense reasoning.
RAC remains crucial for AI systems that operate in dynamic environments, en-
gage in interactive scenarios, or rely on commonsense reasoning. Despite sub-
stantial advances made by Large Language Models (LLMs) in various AI do-
mains, their performance in RAC remains underexplored. To address this gap,
we introduce a new diagnostic benchmark, ACTIONREASONINGBENCH, which
encompasses 8 domains and includes questions for up to 19 action sequences.
This benchmark rigorously evaluates LLMs across six key RAC dimensions: Flu-
ent Tracking, State Tracking, Action Executability, Effects of Actions, Numerical
RAC, and Composite Questions. LLMs demonstrate average accuracy rates of
73.55%, 65.63%, 58.73%, and 62.38% on the former four dimensions, which are
frequently discussed in RAC literature. However, the performance on the lat-
ter two dimensions, which introduce complex and novel reasoning questions, the
average performance of LLMs is lowered to 33.16% and 51.19%, respectively,
reflecting a 17.9% performance decline. We also introduce new ramification con-
straints to capture the indirect effects of actions, providing deeper insights into
RAC challenges. Our evaluation of state-of-the-art LLMs, including both open-
source and commercial models, reveals challenges across all RAC dimensions,
particularly in handling ramifications, with GPT-4o failing to solve any question
and o1-preview achieving a score of only 18.4%.","UNDERSTANDING THE STABILITY-BASED GENERAL-
IZATION OF PERSONALIZED FEDERATED LEARNING

Jie Tan4 Yifan Shi Li Shen1,2∗ Xiaochun Cao1

Yingqi Liu1,2 Qinglun Li3
1School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, China
2Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), China
3College of Systems Engineering, National University of Defense Technology, China
4Intelligent Game and Decision Lab, China
liuyingqi1199@gmail.com; liqinglun@nudt.edu.cn; j.tanjie@outlook.com
mathshenli@gmail.com; caoxiaochun@mail.sysu.edu.cn

ABSTRACT

Despite great achievements in algorithm design for Personalized Federated Learn-
ing (PFL), research on the theoretical analysis of generalization is still in its early
stages. Some theoretical results have investigated the generalization performance of
personalized models under the problem setting and hypothesis in convex conditions,
which can not reflect the real iteration performance during non-convex training. To
further understand the real performance from a generalization perspective, we pro-
pose the first algorithm-dependent generalization analysis with uniform stability for
the typical PFL method, Partial Model Personalization, on smooth and non-convex
objectives. Specifically, we decompose the generalization errors into aggregation
errors and fine-tuning errors, then creatively establish a generalization analysis
framework corresponding to the gradient estimation process of the personalized
training. This framework builds up the bridge among PFL, FL and Pure Local Train-
ing for personalized aims in heterogeneous scenarios, which clearly demonstrates
the effectiveness of PFL from the generalization perspective. Moreover, we demon-
strate the impact of trivial factors like learning steps, stepsizes and communication
topologies and obtain the excess risk analysis with optimization errors for PFL.
Promising experiments on CIFAR datasets also corroborate our theoretical insights.
Our code can be seen in https://github.com/YingqiLiu1999/Understanding-the-
Stability-based-Generalization-of-Personalized-Federated-Learning.",3
17,"<insight>The ease with which supposedly ""unlearned"" information can be reactivated by related data suggests that current removal methods may only superficially alter model outputs or shallow representations, failing to modify the deeper, more foundational knowledge representations within the network layers where such information might persist, aligning with the principle that different network depths capture different levels of feature abstraction.</insight>",-580.75,-875.0,-458.0,-836.6875,1834.4375,-8.2964286804,-12.5,-6.5428571701,-11.9526786804,"['UNLEARNING OR OBFUSCATING? JOGGING THE MEM-\nORY OF UNLEARNED LLMS VIA BENIGN RELEARNING\nCONTENT WARNING: THIS MANUSCRIPT CONTAINS EXAMPLES OF HARMFUL/HAZARDOUS TEXT\n\nShengyuan Hu, Yiwei Fu, Zhiwei Steven Wu & Virginia Smith\nCarnegie Mellon University\n{shengyuanhu, zstevenwu, smithv}@cmu.edu, yiweif@andrew.cmu.edu\n\nABSTRACT\n\nMachine unlearning is a promising approach to mitigate undesirable memorization\nof training data in ML models. However, in this work we show that existing\napproaches for unlearning in LLMs are surprisingly susceptible to a simple set\nof benign relearning attacks. With access to only a small and potentially loosely\nrelated set of data, we find that we can “jog” the memory of unlearned models to\nreverse the effects of unlearning. For example, we show that relearning on public\nmedical articles can lead an unlearned LLM to output harmful knowledge about\nbioweapons, and relearning general wiki information about the book series Harry\nPotter can force the model to output verbatim memorized text. We formalize this\nunlearning-relearning pipeline, explore the attack across three popular unlearning\nbenchmarks, and discuss future directions and guidelines that result from our study.\nOur work indicates that current approximate unlearning methods simply suppress\nthe model outputs and fail to robustly forget target knowledge in the LLMs.', 'LINES: POST-TRAINING LAYER SCALING PREVENTS\nFORGETTING AND ENHANCES MODEL MERGING\n\nKe Wang∗\nEPFL\nk.wang@epfl.ch\n\nNikolaos Dimitriadis∗\nEPFL\nnikolaos.dimitriadis@epfl.ch\n\nAlessandro Favero\nEPFL\nalessandro.favero@epfl.ch\n\nGuillermo Ortiz-Jimenez\nGoogle DeepMind\ngortizj@google.com\n\nFranc¸ois Fleuret\nUniversity of Geneva, Meta FAIR\nfrancois.fleuret@unige.ch\n\nPascal Frossard\nEPFL\npascal.frossard@epfl.ch\n\nABSTRACT\n\nFine-tuning pre-trained models has become the standard approach to endow them\nwith specialized knowledge, but it poses fundamental challenges. In particular,\n(i) fine-tuning often leads to catastrophic forgetting, where improvements on a\ntarget domain degrade generalization on other tasks, and (ii) merging fine-tuned\ncheckpoints from disparate tasks can lead to significant performance loss. To\naddress these challenges, we introduce LiNeS, Layer-increasing Network Scaling,\na post-training editing technique designed to preserve pre-trained generalization\nwhile enhancing fine-tuned task performance. LiNeS scales parameter updates\nlinearly based on their layer depth within the network, maintaining shallow layers\nclose to their pre-trained values to preserve general features while allowing deeper\nlayers to retain task-specific representations. In multi-task model merging scenar-\nios, layer-wise scaling of merged parameters reduces negative task interference.\nLiNeS demonstrates significant improvements in both single-task and multi-task\nsettings across various benchmarks in vision and natural language processing.\nintegrates\nIt mitigates forgetting, enhances out-of-distribution generalization,\nseamlessly with existing multi-task model merging baselines improving their per-\nformance across benchmarks and model sizes, and can boost generalization when\nmerging LLM policies aligned with different rewards via RLHF. Our method\nis simple to implement, computationally efficient and complementary to many\nexisting techniques. Our source code is available at github.com/wang-kee/LiNeS.']",997.75,161.0625,122.75,26.2062501907,14.2535715103,2.300892829900002,1.7535715103000005,fMNRYBvcQN,J5sUOvlLbQ,"['machine unlearning', 'large language model', 'model merging', 'model editing', 'catastrophic forgetting', 'multi-task learning', 'ood generalization']",True,"UNLEARNING OR OBFUSCATING? JOGGING THE MEM-
ORY OF UNLEARNED LLMS VIA BENIGN RELEARNING
CONTENT WARNING: THIS MANUSCRIPT CONTAINS EXAMPLES OF HARMFUL/HAZARDOUS TEXT

Shengyuan Hu, Yiwei Fu, Zhiwei Steven Wu & Virginia Smith
Carnegie Mellon University
{shengyuanhu, zstevenwu, smithv}@cmu.edu, yiweif@andrew.cmu.edu

ABSTRACT

Machine unlearning is a promising approach to mitigate undesirable memorization
of training data in ML models. However, in this work we show that existing
approaches for unlearning in LLMs are surprisingly susceptible to a simple set
of benign relearning attacks. With access to only a small and potentially loosely
related set of data, we find that we can “jog” the memory of unlearned models to
reverse the effects of unlearning. For example, we show that relearning on public
medical articles can lead an unlearned LLM to output harmful knowledge about
bioweapons, and relearning general wiki information about the book series Harry
Potter can force the model to output verbatim memorized text. We formalize this
unlearning-relearning pipeline, explore the attack across three popular unlearning
benchmarks, and discuss future directions and guidelines that result from our study.
Our work indicates that current approximate unlearning methods simply suppress
the model outputs and fail to robustly forget target knowledge in the LLMs.","LINES: POST-TRAINING LAYER SCALING PREVENTS
FORGETTING AND ENHANCES MODEL MERGING

Ke Wang∗
EPFL
k.wang@epfl.ch

Nikolaos Dimitriadis∗
EPFL
nikolaos.dimitriadis@epfl.ch

Alessandro Favero
EPFL
alessandro.favero@epfl.ch

Guillermo Ortiz-Jimenez
Google DeepMind
gortizj@google.com

Franc¸ois Fleuret
University of Geneva, Meta FAIR
francois.fleuret@unige.ch

Pascal Frossard
EPFL
pascal.frossard@epfl.ch

ABSTRACT

Fine-tuning pre-trained models has become the standard approach to endow them
with specialized knowledge, but it poses fundamental challenges. In particular,
(i) fine-tuning often leads to catastrophic forgetting, where improvements on a
target domain degrade generalization on other tasks, and (ii) merging fine-tuned
checkpoints from disparate tasks can lead to significant performance loss. To
address these challenges, we introduce LiNeS, Layer-increasing Network Scaling,
a post-training editing technique designed to preserve pre-trained generalization
while enhancing fine-tuned task performance. LiNeS scales parameter updates
linearly based on their layer depth within the network, maintaining shallow layers
close to their pre-trained values to preserve general features while allowing deeper
layers to retain task-specific representations. In multi-task model merging scenar-
ios, layer-wise scaling of merged parameters reduces negative task interference.
LiNeS demonstrates significant improvements in both single-task and multi-task
settings across various benchmarks in vision and natural language processing.
integrates
It mitigates forgetting, enhances out-of-distribution generalization,
seamlessly with existing multi-task model merging baselines improving their per-
formance across benchmarks and model sizes, and can boost generalization when
merging LLM policies aligned with different rewards via RLHF. Our method
is simple to implement, computationally efficient and complementary to many
existing techniques. Our source code is available at github.com/wang-kee/LiNeS.",4
17,"<insight>The tendency for models to easily recall information after supposed unlearning suggests that such techniques may only superficially suppress outputs without fundamentally altering the deeper, task-specific knowledge representations within the network, leaving them vulnerable to reactivation.</insight>",-370.59375,-436.4375,-373.40625,-305.9453125,739.5703125,-8.6184597015,-10.1497097015,-8.6838665009,-7.1150074005,"['UNLEARNING OR OBFUSCATING? JOGGING THE MEM-\nORY OF UNLEARNED LLMS VIA BENIGN RELEARNING\nCONTENT WARNING: THIS MANUSCRIPT CONTAINS EXAMPLES OF HARMFUL/HAZARDOUS TEXT\n\nShengyuan Hu, Yiwei Fu, Zhiwei Steven Wu & Virginia Smith\nCarnegie Mellon University\n{shengyuanhu, zstevenwu, smithv}@cmu.edu, yiweif@andrew.cmu.edu\n\nABSTRACT\n\nMachine unlearning is a promising approach to mitigate undesirable memorization\nof training data in ML models. However, in this work we show that existing\napproaches for unlearning in LLMs are surprisingly susceptible to a simple set\nof benign relearning attacks. With access to only a small and potentially loosely\nrelated set of data, we find that we can “jog” the memory of unlearned models to\nreverse the effects of unlearning. For example, we show that relearning on public\nmedical articles can lead an unlearned LLM to output harmful knowledge about\nbioweapons, and relearning general wiki information about the book series Harry\nPotter can force the model to output verbatim memorized text. We formalize this\nunlearning-relearning pipeline, explore the attack across three popular unlearning\nbenchmarks, and discuss future directions and guidelines that result from our study.\nOur work indicates that current approximate unlearning methods simply suppress\nthe model outputs and fail to robustly forget target knowledge in the LLMs.', 'LINES: POST-TRAINING LAYER SCALING PREVENTS\nFORGETTING AND ENHANCES MODEL MERGING\n\nKe Wang∗\nEPFL\nk.wang@epfl.ch\n\nNikolaos Dimitriadis∗\nEPFL\nnikolaos.dimitriadis@epfl.ch\n\nAlessandro Favero\nEPFL\nalessandro.favero@epfl.ch\n\nGuillermo Ortiz-Jimenez\nGoogle DeepMind\ngortizj@google.com\n\nFranc¸ois Fleuret\nUniversity of Geneva, Meta FAIR\nfrancois.fleuret@unige.ch\n\nPascal Frossard\nEPFL\npascal.frossard@epfl.ch\n\nABSTRACT\n\nFine-tuning pre-trained models has become the standard approach to endow them\nwith specialized knowledge, but it poses fundamental challenges. In particular,\n(i) fine-tuning often leads to catastrophic forgetting, where improvements on a\ntarget domain degrade generalization on other tasks, and (ii) merging fine-tuned\ncheckpoints from disparate tasks can lead to significant performance loss. To\naddress these challenges, we introduce LiNeS, Layer-increasing Network Scaling,\na post-training editing technique designed to preserve pre-trained generalization\nwhile enhancing fine-tuned task performance. LiNeS scales parameter updates\nlinearly based on their layer depth within the network, maintaining shallow layers\nclose to their pre-trained values to preserve general features while allowing deeper\nlayers to retain task-specific representations. In multi-task model merging scenar-\nios, layer-wise scaling of merged parameters reduces negative task interference.\nLiNeS demonstrates significant improvements in both single-task and multi-task\nsettings across various benchmarks in vision and natural language processing.\nintegrates\nIt mitigates forgetting, enhances out-of-distribution generalization,\nseamlessly with existing multi-task model merging baselines improving their per-\nformance across benchmarks and model sizes, and can boost generalization when\nmerging LLM policies aligned with different rewards via RLHF. Our method\nis simple to implement, computationally efficient and complementary to many\nexisting techniques. Our source code is available at github.com/wang-kee/LiNeS.']",433.625,127.6796875,-67.4609375,17.1993103026,10.084302902100001,2.9692955016000013,-1.568859100400001,fMNRYBvcQN,J5sUOvlLbQ,"['machine unlearning', 'large language model', 'model merging', 'model editing', 'catastrophic forgetting', 'multi-task learning', 'ood generalization']",True,"UNLEARNING OR OBFUSCATING? JOGGING THE MEM-
ORY OF UNLEARNED LLMS VIA BENIGN RELEARNING
CONTENT WARNING: THIS MANUSCRIPT CONTAINS EXAMPLES OF HARMFUL/HAZARDOUS TEXT

Shengyuan Hu, Yiwei Fu, Zhiwei Steven Wu & Virginia Smith
Carnegie Mellon University
{shengyuanhu, zstevenwu, smithv}@cmu.edu, yiweif@andrew.cmu.edu

ABSTRACT

Machine unlearning is a promising approach to mitigate undesirable memorization
of training data in ML models. However, in this work we show that existing
approaches for unlearning in LLMs are surprisingly susceptible to a simple set
of benign relearning attacks. With access to only a small and potentially loosely
related set of data, we find that we can “jog” the memory of unlearned models to
reverse the effects of unlearning. For example, we show that relearning on public
medical articles can lead an unlearned LLM to output harmful knowledge about
bioweapons, and relearning general wiki information about the book series Harry
Potter can force the model to output verbatim memorized text. We formalize this
unlearning-relearning pipeline, explore the attack across three popular unlearning
benchmarks, and discuss future directions and guidelines that result from our study.
Our work indicates that current approximate unlearning methods simply suppress
the model outputs and fail to robustly forget target knowledge in the LLMs.","LINES: POST-TRAINING LAYER SCALING PREVENTS
FORGETTING AND ENHANCES MODEL MERGING

Ke Wang∗
EPFL
k.wang@epfl.ch

Nikolaos Dimitriadis∗
EPFL
nikolaos.dimitriadis@epfl.ch

Alessandro Favero
EPFL
alessandro.favero@epfl.ch

Guillermo Ortiz-Jimenez
Google DeepMind
gortizj@google.com

Franc¸ois Fleuret
University of Geneva, Meta FAIR
francois.fleuret@unige.ch

Pascal Frossard
EPFL
pascal.frossard@epfl.ch

ABSTRACT

Fine-tuning pre-trained models has become the standard approach to endow them
with specialized knowledge, but it poses fundamental challenges. In particular,
(i) fine-tuning often leads to catastrophic forgetting, where improvements on a
target domain degrade generalization on other tasks, and (ii) merging fine-tuned
checkpoints from disparate tasks can lead to significant performance loss. To
address these challenges, we introduce LiNeS, Layer-increasing Network Scaling,
a post-training editing technique designed to preserve pre-trained generalization
while enhancing fine-tuned task performance. LiNeS scales parameter updates
linearly based on their layer depth within the network, maintaining shallow layers
close to their pre-trained values to preserve general features while allowing deeper
layers to retain task-specific representations. In multi-task model merging scenar-
ios, layer-wise scaling of merged parameters reduces negative task interference.
LiNeS demonstrates significant improvements in both single-task and multi-task
settings across various benchmarks in vision and natural language processing.
integrates
It mitigates forgetting, enhances out-of-distribution generalization,
seamlessly with existing multi-task model merging baselines improving their per-
formance across benchmarks and model sizes, and can boost generalization when
merging LLM policies aligned with different rewards via RLHF. Our method
is simple to implement, computationally efficient and complementary to many
existing techniques. Our source code is available at github.com/wang-kee/LiNeS.",5
20,"<insight>Large language model capabilities can be significantly enhanced through two complementary strategies: refining performance on complex symbolic tasks within their native text domain by leveraging large-scale, carefully curated instruction datasets, and extending their application to entirely new non-linguistic data modalities by first translating those signals into a discrete tokenized format compatible with the model's architecture and then applying multi-task instruction tuning.</insight>",-931.875,-924.0625,-933.84375,-990.3125,1912.40625,-12.5929050446,-12.4873313904,-12.619509697,-13.382601738,"['OPENMATHINSTRUCT-2: ACCELERATING AI FOR\nMATH WITH MASSIVE OPEN-SOURCE INSTRUCTION\nDATA\n\nShubham Toshniwal1 Wei Du1\nBranislav Kisacanin1,2,3 Alexan Ayrapetyan1\n{stoshniwal,wedu,imoshkov,bkisacanin,aayrapetyan,igitman}@nvidia.com\n\nIvan Moshkov1\n\nIgor Gitman1\n\n1NVIDIA\n2Institute for AI R&D of Serbia\n3Faculty of Technical Sciences, University of Novi Sad\n\nABSTRACT\n\nMathematical reasoning continues to be a critical challenge in large language\nmodel (LLM) development with significant interest. However, most of the cutting-\nedge progress in mathematical reasoning with LLMs has become closed-source\ndue to lack of access to training data. This lack of data access limits researchers\nfrom understanding the impact of different choices for synthesizing and utilizing\nthe data. With the goal of creating a high-quality finetuning (SFT) dataset for\nmath reasoning, we conduct careful ablation experiments on data synthesis us-\ning the recently released Llama3.1 family of models. Our experiments show\nthat: (a) solution format matters, with excessively verbose solutions proving detri-\nmental to SFT performance, (b) data generated by a strong teacher outperforms\nequally-sized data generated by a weak student model, (c) SFT is robust to low-\nquality solutions, allowing for imprecise data filtering, and (d) question diversity\nis crucial for achieving data scaling gains. Based on these insights, we create\nthe OpenMathInstruct-2 dataset which consists of 14M question-solution pairs (≈\n600K unique questions), making it nearly eight times larger than the previous largest\nopen-source math reasoning dataset. Finetuning the Llama-3.1-8B-Base us-\ning OpenMathInstruct-2 outperforms Llama3.1-8B-Instruct on MATH by\nan absolute 15.9% (51.9% → 67.8%). Finally, to accelerate the open-source efforts,\nwe release the code, the finetuned models, and the OpenMathInstruct-2 dataset\nunder a commercially permissive license.1', 'NEUROLM: A UNIVERSAL MULTI-TASK FOUNDATION\nMODEL FOR BRIDGING THE GAP BETWEEN LAN-\nGUAGE AND EEG SIGNALS\n\nWei-Bang Jiang1∗, Yansen Wang2, Bao-Liang Lu1, Dongsheng Li2\n1Shanghai Jiao Tong University\n2Microsoft Research Asia\n{935963004,bllu}@sjtu.edu.cn,{yansenwang,dongsli}@microsoft.com\nhttps://github.com/935963004/NeuroLM\n\nABSTRACT\n\nRecent advancements for large-scale pre-training with neural signals such as elec-\ntroencephalogram (EEG) have shown promising results, significantly boosting\nthe development of brain-computer interfaces (BCIs) and healthcare. However,\nthese pre-trained models often require full fine-tuning on each downstream task\nto achieve substantial improvements, limiting their versatility and usability, and\nleading to considerable resource wastage. To tackle these challenges, we propose\nNeuroLM, the first multi-task foundation model that leverages the capabilities\nof Large Language Models (LLMs) by regarding EEG signals as a foreign lan-\nguage, endowing the model with multi-task learning and inference capabilities.\nOur approach begins with learning a text-aligned neural tokenizer through vector-\nquantized temporal-frequency prediction, which encodes EEG signals into dis-\ncrete neural tokens. These EEG tokens, generated by the frozen vector-quantized\n(VQ) encoder, are then fed into an LLM that learns causal EEG information via\nmulti-channel autoregression. Consequently, NeuroLM can understand both EEG\nand language modalities. Finally, multi-task instruction tuning adapts NeuroLM\nto various downstream tasks. We are the first to demonstrate that, by specific\nincorporation with LLMs, NeuroLM unifies diverse EEG tasks within a single\nmodel through instruction tuning. The largest variant NeuroLM-XL has record-\nbreaking 1.7B parameters for EEG signal processing, and is pre-trained on a large-\nscale corpus comprising approximately 25,000-hour EEG data. When evaluated\non six diverse downstream datasets, NeuroLM showcases the huge potential of\nthis multi-task learning paradigm.']",922.09375,-68.21875,-9.78125,25.843328476,12.460726738,-0.921875,-0.13217830660000018,mTCbq2QssD,Io9yFt7XH7,"['math reasoning', 'synthetic data', 'eeg', 'large language model', 'multi-task learning']",True,"OPENMATHINSTRUCT-2: ACCELERATING AI FOR
MATH WITH MASSIVE OPEN-SOURCE INSTRUCTION
DATA

Shubham Toshniwal1 Wei Du1
Branislav Kisacanin1,2,3 Alexan Ayrapetyan1
{stoshniwal,wedu,imoshkov,bkisacanin,aayrapetyan,igitman}@nvidia.com

Ivan Moshkov1

Igor Gitman1

1NVIDIA
2Institute for AI R&D of Serbia
3Faculty of Technical Sciences, University of Novi Sad

ABSTRACT

Mathematical reasoning continues to be a critical challenge in large language
model (LLM) development with significant interest. However, most of the cutting-
edge progress in mathematical reasoning with LLMs has become closed-source
due to lack of access to training data. This lack of data access limits researchers
from understanding the impact of different choices for synthesizing and utilizing
the data. With the goal of creating a high-quality finetuning (SFT) dataset for
math reasoning, we conduct careful ablation experiments on data synthesis us-
ing the recently released Llama3.1 family of models. Our experiments show
that: (a) solution format matters, with excessively verbose solutions proving detri-
mental to SFT performance, (b) data generated by a strong teacher outperforms
equally-sized data generated by a weak student model, (c) SFT is robust to low-
quality solutions, allowing for imprecise data filtering, and (d) question diversity
is crucial for achieving data scaling gains. Based on these insights, we create
the OpenMathInstruct-2 dataset which consists of 14M question-solution pairs (≈
600K unique questions), making it nearly eight times larger than the previous largest
open-source math reasoning dataset. Finetuning the Llama-3.1-8B-Base us-
ing OpenMathInstruct-2 outperforms Llama3.1-8B-Instruct on MATH by
an absolute 15.9% (51.9% → 67.8%). Finally, to accelerate the open-source efforts,
we release the code, the finetuned models, and the OpenMathInstruct-2 dataset
under a commercially permissive license.1","NEUROLM: A UNIVERSAL MULTI-TASK FOUNDATION
MODEL FOR BRIDGING THE GAP BETWEEN LAN-
GUAGE AND EEG SIGNALS

Wei-Bang Jiang1∗, Yansen Wang2, Bao-Liang Lu1, Dongsheng Li2
1Shanghai Jiao Tong University
2Microsoft Research Asia
{935963004,bllu}@sjtu.edu.cn,{yansenwang,dongsli}@microsoft.com
https://github.com/935963004/NeuroLM

ABSTRACT

Recent advancements for large-scale pre-training with neural signals such as elec-
troencephalogram (EEG) have shown promising results, significantly boosting
the development of brain-computer interfaces (BCIs) and healthcare. However,
these pre-trained models often require full fine-tuning on each downstream task
to achieve substantial improvements, limiting their versatility and usability, and
leading to considerable resource wastage. To tackle these challenges, we propose
NeuroLM, the first multi-task foundation model that leverages the capabilities
of Large Language Models (LLMs) by regarding EEG signals as a foreign lan-
guage, endowing the model with multi-task learning and inference capabilities.
Our approach begins with learning a text-aligned neural tokenizer through vector-
quantized temporal-frequency prediction, which encodes EEG signals into dis-
crete neural tokens. These EEG tokens, generated by the frozen vector-quantized
(VQ) encoder, are then fed into an LLM that learns causal EEG information via
multi-channel autoregression. Consequently, NeuroLM can understand both EEG
and language modalities. Finally, multi-task instruction tuning adapts NeuroLM
to various downstream tasks. We are the first to demonstrate that, by specific
incorporation with LLMs, NeuroLM unifies diverse EEG tasks within a single
model through instruction tuning. The largest variant NeuroLM-XL has record-
breaking 1.7B parameters for EEG signal processing, and is pre-trained on a large-
scale corpus comprising approximately 25,000-hour EEG data. When evaluated
on six diverse downstream datasets, NeuroLM showcases the huge potential of
this multi-task learning paradigm.",6
20,"<insight>Optimizing the scale, diversity, and format of instruction datasets is likely a critical factor for successfully extending large language models via instruction tuning to interpret specialized, non-linguistic data streams, just as it is for enhancing their performance on complex reasoning tasks.</insight>",-540.375,-527.59375,-568.75,-529.984375,1029.203125,-10.3918266296,-10.146033287,-10.9375,-10.1920070648,"['OPENMATHINSTRUCT-2: ACCELERATING AI FOR\nMATH WITH MASSIVE OPEN-SOURCE INSTRUCTION\nDATA\n\nShubham Toshniwal1 Wei Du1\nBranislav Kisacanin1,2,3 Alexan Ayrapetyan1\n{stoshniwal,wedu,imoshkov,bkisacanin,aayrapetyan,igitman}@nvidia.com\n\nIvan Moshkov1\n\nIgor Gitman1\n\n1NVIDIA\n2Institute for AI R&D of Serbia\n3Faculty of Technical Sciences, University of Novi Sad\n\nABSTRACT\n\nMathematical reasoning continues to be a critical challenge in large language\nmodel (LLM) development with significant interest. However, most of the cutting-\nedge progress in mathematical reasoning with LLMs has become closed-source\ndue to lack of access to training data. This lack of data access limits researchers\nfrom understanding the impact of different choices for synthesizing and utilizing\nthe data. With the goal of creating a high-quality finetuning (SFT) dataset for\nmath reasoning, we conduct careful ablation experiments on data synthesis us-\ning the recently released Llama3.1 family of models. Our experiments show\nthat: (a) solution format matters, with excessively verbose solutions proving detri-\nmental to SFT performance, (b) data generated by a strong teacher outperforms\nequally-sized data generated by a weak student model, (c) SFT is robust to low-\nquality solutions, allowing for imprecise data filtering, and (d) question diversity\nis crucial for achieving data scaling gains. Based on these insights, we create\nthe OpenMathInstruct-2 dataset which consists of 14M question-solution pairs (≈\n600K unique questions), making it nearly eight times larger than the previous largest\nopen-source math reasoning dataset. Finetuning the Llama-3.1-8B-Base us-\ning OpenMathInstruct-2 outperforms Llama3.1-8B-Instruct on MATH by\nan absolute 15.9% (51.9% → 67.8%). Finally, to accelerate the open-source efforts,\nwe release the code, the finetuned models, and the OpenMathInstruct-2 dataset\nunder a commercially permissive license.1', 'NEUROLM: A UNIVERSAL MULTI-TASK FOUNDATION\nMODEL FOR BRIDGING THE GAP BETWEEN LAN-\nGUAGE AND EEG SIGNALS\n\nWei-Bang Jiang1∗, Yansen Wang2, Bao-Liang Lu1, Dongsheng Li2\n1Shanghai Jiao Tong University\n2Microsoft Research Asia\n{935963004,bllu}@sjtu.edu.cn,{yansenwang,dongsli}@microsoft.com\nhttps://github.com/935963004/NeuroLM\n\nABSTRACT\n\nRecent advancements for large-scale pre-training with neural signals such as elec-\ntroencephalogram (EEG) have shown promising results, significantly boosting\nthe development of brain-computer interfaces (BCIs) and healthcare. However,\nthese pre-trained models often require full fine-tuning on each downstream task\nto achieve substantial improvements, limiting their versatility and usability, and\nleading to considerable resource wastage. To tackle these challenges, we propose\nNeuroLM, the first multi-task foundation model that leverages the capabilities\nof Large Language Models (LLMs) by regarding EEG signals as a foreign lan-\nguage, endowing the model with multi-task learning and inference capabilities.\nOur approach begins with learning a text-aligned neural tokenizer through vector-\nquantized temporal-frequency prediction, which encodes EEG signals into dis-\ncrete neural tokens. These EEG tokens, generated by the frozen vector-quantized\n(VQ) encoder, are then fed into an LLM that learns causal EEG information via\nmulti-channel autoregression. Consequently, NeuroLM can understand both EEG\nand language modalities. Finally, multi-task instruction tuning adapts NeuroLM\nto various downstream tasks. We are the first to demonstrate that, by specific\nincorporation with LLMs, NeuroLM unifies diverse EEG tasks within a single\nmodel through instruction tuning. The largest variant NeuroLM-XL has record-\nbreaking 1.7B parameters for EEG signal processing, and is pre-trained on a large-\nscale corpus comprising approximately 25,000-hour EEG data. When evaluated\non six diverse downstream datasets, NeuroLM showcases the huge potential of\nthis multi-task learning paradigm.']",499.21875,-30.765625,-41.15625,19.7923669814,9.6003599166,-0.5916471482000016,-0.7914667130000002,mTCbq2QssD,Io9yFt7XH7,"['math reasoning', 'synthetic data', 'eeg', 'large language model', 'multi-task learning']",True,"OPENMATHINSTRUCT-2: ACCELERATING AI FOR
MATH WITH MASSIVE OPEN-SOURCE INSTRUCTION
DATA

Shubham Toshniwal1 Wei Du1
Branislav Kisacanin1,2,3 Alexan Ayrapetyan1
{stoshniwal,wedu,imoshkov,bkisacanin,aayrapetyan,igitman}@nvidia.com

Ivan Moshkov1

Igor Gitman1

1NVIDIA
2Institute for AI R&D of Serbia
3Faculty of Technical Sciences, University of Novi Sad

ABSTRACT

Mathematical reasoning continues to be a critical challenge in large language
model (LLM) development with significant interest. However, most of the cutting-
edge progress in mathematical reasoning with LLMs has become closed-source
due to lack of access to training data. This lack of data access limits researchers
from understanding the impact of different choices for synthesizing and utilizing
the data. With the goal of creating a high-quality finetuning (SFT) dataset for
math reasoning, we conduct careful ablation experiments on data synthesis us-
ing the recently released Llama3.1 family of models. Our experiments show
that: (a) solution format matters, with excessively verbose solutions proving detri-
mental to SFT performance, (b) data generated by a strong teacher outperforms
equally-sized data generated by a weak student model, (c) SFT is robust to low-
quality solutions, allowing for imprecise data filtering, and (d) question diversity
is crucial for achieving data scaling gains. Based on these insights, we create
the OpenMathInstruct-2 dataset which consists of 14M question-solution pairs (≈
600K unique questions), making it nearly eight times larger than the previous largest
open-source math reasoning dataset. Finetuning the Llama-3.1-8B-Base us-
ing OpenMathInstruct-2 outperforms Llama3.1-8B-Instruct on MATH by
an absolute 15.9% (51.9% → 67.8%). Finally, to accelerate the open-source efforts,
we release the code, the finetuned models, and the OpenMathInstruct-2 dataset
under a commercially permissive license.1","NEUROLM: A UNIVERSAL MULTI-TASK FOUNDATION
MODEL FOR BRIDGING THE GAP BETWEEN LAN-
GUAGE AND EEG SIGNALS

Wei-Bang Jiang1∗, Yansen Wang2, Bao-Liang Lu1, Dongsheng Li2
1Shanghai Jiao Tong University
2Microsoft Research Asia
{935963004,bllu}@sjtu.edu.cn,{yansenwang,dongsli}@microsoft.com
https://github.com/935963004/NeuroLM

ABSTRACT

Recent advancements for large-scale pre-training with neural signals such as elec-
troencephalogram (EEG) have shown promising results, significantly boosting
the development of brain-computer interfaces (BCIs) and healthcare. However,
these pre-trained models often require full fine-tuning on each downstream task
to achieve substantial improvements, limiting their versatility and usability, and
leading to considerable resource wastage. To tackle these challenges, we propose
NeuroLM, the first multi-task foundation model that leverages the capabilities
of Large Language Models (LLMs) by regarding EEG signals as a foreign lan-
guage, endowing the model with multi-task learning and inference capabilities.
Our approach begins with learning a text-aligned neural tokenizer through vector-
quantized temporal-frequency prediction, which encodes EEG signals into dis-
crete neural tokens. These EEG tokens, generated by the frozen vector-quantized
(VQ) encoder, are then fed into an LLM that learns causal EEG information via
multi-channel autoregression. Consequently, NeuroLM can understand both EEG
and language modalities. Finally, multi-task instruction tuning adapts NeuroLM
to various downstream tasks. We are the first to demonstrate that, by specific
incorporation with LLMs, NeuroLM unifies diverse EEG tasks within a single
model through instruction tuning. The largest variant NeuroLM-XL has record-
breaking 1.7B parameters for EEG signal processing, and is pre-trained on a large-
scale corpus comprising approximately 25,000-hour EEG data. When evaluated
on six diverse downstream datasets, NeuroLM showcases the huge potential of
this multi-task learning paradigm.",7
98,"<insight>The intricate architectures of advanced AI models may contain components superfluous for tasks solvable through simpler underlying mechanisms like pattern association, yet simultaneously lack the necessary structures or processes for effective multi-step, causal reasoning, particularly when tracking the indirect consequences of actions.</insight>",-419.859375,-652.625,-460.34375,-672.5,1284.640625,-8.397187233,-13.0524997711,-9.2068748474,-13.4499998093,"['DECONSTRUCTING DENOISING DIFFUSION MODELS\nFOR SELF-SUPERVISED LEARNING\n\nXinlei Chen1\n\nZhuang Liu1,2\n\nSaining Xie3\n\nKaiming He1,4\n\n1FAIR, Meta\n\n2Princeton University\n\n3New York University\n\n4MIT\n\nABSTRACT\n\nIn this study, we examine the representation learning abilities of Denoising Dif-\nfusion Models (DDM) that were originally purposed for image generation. Our\nphilosophy is to deconstruct a DDM, gradually transforming it into a classical\nDenoising Autoencoder (DAE). This deconstructive process allows us to explore\nhow various components of modern DDMs influence self-supervised representation\nlearning. We observe that only a very few modern components are critical for\nlearning good representations, while many others are nonessential. Our study\nultimately arrives at an approach that is highly simplified and to a large extent\nresembles a classical DAE. We hope our study will rekindle interest in a family of\nclassical methods within the realm of modern self-supervised learning.', 'ACTIONREASONINGBENCH: REASONING ABOUT AC-\nTIONS WITH AND WITHOUT RAMIFICATION CON-\nSTRAINTS\n\nDivij Handa∗1, Pavel Dolin∗1, Shrinidhi Kumbhar∗ 1, Tran Cao Son2, Chitta Baral1\n1Arizona State University, 2New Mexico State University\n{dhanda,pdolin,skumbha4,chitta}@asu.edu, stran@nmsu.edu\n\nABSTRACT\n\nReasoning about Actions and Change (RAC) has historically played a pivotal role\nin solving foundational AI problems, such as the frame problem. It has driven\nadvancements in AI fields, such as non-monotonic and commonsense reasoning.\nRAC remains crucial for AI systems that operate in dynamic environments, en-\ngage in interactive scenarios, or rely on commonsense reasoning. Despite sub-\nstantial advances made by Large Language Models (LLMs) in various AI do-\nmains, their performance in RAC remains underexplored. To address this gap,\nwe introduce a new diagnostic benchmark, ACTIONREASONINGBENCH, which\nencompasses 8 domains and includes questions for up to 19 action sequences.\nThis benchmark rigorously evaluates LLMs across six key RAC dimensions: Flu-\nent Tracking, State Tracking, Action Executability, Effects of Actions, Numerical\nRAC, and Composite Questions. LLMs demonstrate average accuracy rates of\n73.55%, 65.63%, 58.73%, and 62.38% on the former four dimensions, which are\nfrequently discussed in RAC literature. However, the performance on the lat-\nter two dimensions, which introduce complex and novel reasoning questions, the\naverage performance of LLMs is lowered to 33.16% and 51.19%, respectively,\nreflecting a 17.9% performance decline. We also introduce new ramification con-\nstraints to capture the indirect effects of actions, providing deeper insights into\nRAC challenges. Our evaluation of state-of-the-art LLMs, including both open-\nsource and commercial models, reveals challenges across all RAC dimensions,\nparticularly in handling ramifications, with GPT-4o failing to solve any question\nand o1-preview achieving a score of only 18.4%.']",612.140625,-60.359375,-40.484375,25.692811966,12.242812156700001,-1.2071876525999965,-0.8096876143999996,9oMB6wnFYM,NUD03NBDOE,"['denoising diffusion models', 'denoising autoencoder', 'self-supervised learning', 'reasoning about actions and change (rac)', 'benchmark', 'large language models (llms)']",True,"DECONSTRUCTING DENOISING DIFFUSION MODELS
FOR SELF-SUPERVISED LEARNING

Xinlei Chen1

Zhuang Liu1,2

Saining Xie3

Kaiming He1,4

1FAIR, Meta

2Princeton University

3New York University

4MIT

ABSTRACT

In this study, we examine the representation learning abilities of Denoising Dif-
fusion Models (DDM) that were originally purposed for image generation. Our
philosophy is to deconstruct a DDM, gradually transforming it into a classical
Denoising Autoencoder (DAE). This deconstructive process allows us to explore
how various components of modern DDMs influence self-supervised representation
learning. We observe that only a very few modern components are critical for
learning good representations, while many others are nonessential. Our study
ultimately arrives at an approach that is highly simplified and to a large extent
resembles a classical DAE. We hope our study will rekindle interest in a family of
classical methods within the realm of modern self-supervised learning.","ACTIONREASONINGBENCH: REASONING ABOUT AC-
TIONS WITH AND WITHOUT RAMIFICATION CON-
STRAINTS

Divij Handa∗1, Pavel Dolin∗1, Shrinidhi Kumbhar∗ 1, Tran Cao Son2, Chitta Baral1
1Arizona State University, 2New Mexico State University
{dhanda,pdolin,skumbha4,chitta}@asu.edu, stran@nmsu.edu

ABSTRACT

Reasoning about Actions and Change (RAC) has historically played a pivotal role
in solving foundational AI problems, such as the frame problem. It has driven
advancements in AI fields, such as non-monotonic and commonsense reasoning.
RAC remains crucial for AI systems that operate in dynamic environments, en-
gage in interactive scenarios, or rely on commonsense reasoning. Despite sub-
stantial advances made by Large Language Models (LLMs) in various AI do-
mains, their performance in RAC remains underexplored. To address this gap,
we introduce a new diagnostic benchmark, ACTIONREASONINGBENCH, which
encompasses 8 domains and includes questions for up to 19 action sequences.
This benchmark rigorously evaluates LLMs across six key RAC dimensions: Flu-
ent Tracking, State Tracking, Action Executability, Effects of Actions, Numerical
RAC, and Composite Questions. LLMs demonstrate average accuracy rates of
73.55%, 65.63%, 58.73%, and 62.38% on the former four dimensions, which are
frequently discussed in RAC literature. However, the performance on the lat-
ter two dimensions, which introduce complex and novel reasoning questions, the
average performance of LLMs is lowered to 33.16% and 51.19%, respectively,
reflecting a 17.9% performance decline. We also introduce new ramification con-
straints to capture the indirect effects of actions, providing deeper insights into
RAC challenges. Our evaluation of state-of-the-art LLMs, including both open-
source and commercial models, reveals challenges across all RAC dimensions,
particularly in handling ramifications, with GPT-4o failing to solve any question
and o1-preview achieving a score of only 18.4%.",8
98,"<insight>Core computational processes effective for learning data representations, even when derived from highly complex models, may not inherently support sophisticated reasoning about the indirect consequences of actions, leading to weaknesses in dynamic or causal scenarios.</insight>",-296.46875,-414.5625,-348.828125,-407.421875,769.625,-7.6017627716,-10.6298074722,-8.944311142,-10.4467144012,"['DECONSTRUCTING DENOISING DIFFUSION MODELS\nFOR SELF-SUPERVISED LEARNING\n\nXinlei Chen1\n\nZhuang Liu1,2\n\nSaining Xie3\n\nKaiming He1,4\n\n1FAIR, Meta\n\n2Princeton University\n\n3New York University\n\n4MIT\n\nABSTRACT\n\nIn this study, we examine the representation learning abilities of Denoising Dif-\nfusion Models (DDM) that were originally purposed for image generation. Our\nphilosophy is to deconstruct a DDM, gradually transforming it into a classical\nDenoising Autoencoder (DAE). This deconstructive process allows us to explore\nhow various components of modern DDMs influence self-supervised representation\nlearning. We observe that only a very few modern components are critical for\nlearning good representations, while many others are nonessential. Our study\nultimately arrives at an approach that is highly simplified and to a large extent\nresembles a classical DAE. We hope our study will rekindle interest in a family of\nclassical methods within the realm of modern self-supervised learning.', 'ACTIONREASONINGBENCH: REASONING ABOUT AC-\nTIONS WITH AND WITHOUT RAMIFICATION CON-\nSTRAINTS\n\nDivij Handa∗1, Pavel Dolin∗1, Shrinidhi Kumbhar∗ 1, Tran Cao Son2, Chitta Baral1\n1Arizona State University, 2New Mexico State University\n{dhanda,pdolin,skumbha4,chitta}@asu.edu, stran@nmsu.edu\n\nABSTRACT\n\nReasoning about Actions and Change (RAC) has historically played a pivotal role\nin solving foundational AI problems, such as the frame problem. It has driven\nadvancements in AI fields, such as non-monotonic and commonsense reasoning.\nRAC remains crucial for AI systems that operate in dynamic environments, en-\ngage in interactive scenarios, or rely on commonsense reasoning. Despite sub-\nstantial advances made by Large Language Models (LLMs) in various AI do-\nmains, their performance in RAC remains underexplored. To address this gap,\nwe introduce a new diagnostic benchmark, ACTIONREASONINGBENCH, which\nencompasses 8 domains and includes questions for up to 19 action sequences.\nThis benchmark rigorously evaluates LLMs across six key RAC dimensions: Flu-\nent Tracking, State Tracking, Action Executability, Effects of Actions, Numerical\nRAC, and Composite Questions. LLMs demonstrate average accuracy rates of\n73.55%, 65.63%, 58.73%, and 62.38% on the former four dimensions, which are\nfrequently discussed in RAC literature. However, the performance on the lat-\nter two dimensions, which introduce complex and novel reasoning questions, the\naverage performance of LLMs is lowered to 33.16% and 51.19%, respectively,\nreflecting a 17.9% performance decline. We also introduce new ramification con-\nstraints to capture the indirect effects of actions, providing deeper insights into\nRAC challenges. Our evaluation of state-of-the-art LLMs, including both open-\nsource and commercial models, reveals challenges across all RAC dimensions,\nparticularly in handling ramifications, with GPT-4o failing to solve any question\nand o1-preview achieving a score of only 18.4%.']",362.203125,-45.21875,-52.359375,19.733973503,9.2872591018,-1.1594552994000011,-1.3425483704000003,9oMB6wnFYM,NUD03NBDOE,"['denoising diffusion models', 'denoising autoencoder', 'self-supervised learning', 'reasoning about actions and change (rac)', 'benchmark', 'large language models (llms)']",True,"DECONSTRUCTING DENOISING DIFFUSION MODELS
FOR SELF-SUPERVISED LEARNING

Xinlei Chen1

Zhuang Liu1,2

Saining Xie3

Kaiming He1,4

1FAIR, Meta

2Princeton University

3New York University

4MIT

ABSTRACT

In this study, we examine the representation learning abilities of Denoising Dif-
fusion Models (DDM) that were originally purposed for image generation. Our
philosophy is to deconstruct a DDM, gradually transforming it into a classical
Denoising Autoencoder (DAE). This deconstructive process allows us to explore
how various components of modern DDMs influence self-supervised representation
learning. We observe that only a very few modern components are critical for
learning good representations, while many others are nonessential. Our study
ultimately arrives at an approach that is highly simplified and to a large extent
resembles a classical DAE. We hope our study will rekindle interest in a family of
classical methods within the realm of modern self-supervised learning.","ACTIONREASONINGBENCH: REASONING ABOUT AC-
TIONS WITH AND WITHOUT RAMIFICATION CON-
STRAINTS

Divij Handa∗1, Pavel Dolin∗1, Shrinidhi Kumbhar∗ 1, Tran Cao Son2, Chitta Baral1
1Arizona State University, 2New Mexico State University
{dhanda,pdolin,skumbha4,chitta}@asu.edu, stran@nmsu.edu

ABSTRACT

Reasoning about Actions and Change (RAC) has historically played a pivotal role
in solving foundational AI problems, such as the frame problem. It has driven
advancements in AI fields, such as non-monotonic and commonsense reasoning.
RAC remains crucial for AI systems that operate in dynamic environments, en-
gage in interactive scenarios, or rely on commonsense reasoning. Despite sub-
stantial advances made by Large Language Models (LLMs) in various AI do-
mains, their performance in RAC remains underexplored. To address this gap,
we introduce a new diagnostic benchmark, ACTIONREASONINGBENCH, which
encompasses 8 domains and includes questions for up to 19 action sequences.
This benchmark rigorously evaluates LLMs across six key RAC dimensions: Flu-
ent Tracking, State Tracking, Action Executability, Effects of Actions, Numerical
RAC, and Composite Questions. LLMs demonstrate average accuracy rates of
73.55%, 65.63%, 58.73%, and 62.38% on the former four dimensions, which are
frequently discussed in RAC literature. However, the performance on the lat-
ter two dimensions, which introduce complex and novel reasoning questions, the
average performance of LLMs is lowered to 33.16% and 51.19%, respectively,
reflecting a 17.9% performance decline. We also introduce new ramification con-
straints to capture the indirect effects of actions, providing deeper insights into
RAC challenges. Our evaluation of state-of-the-art LLMs, including both open-
source and commercial models, reveals challenges across all RAC dimensions,
particularly in handling ramifications, with GPT-4o failing to solve any question
and o1-preview achieving a score of only 18.4%.",9
114,"<insight>Optimizing machine learning model performance often involves adapting the training process itself—either by aligning training conditions more closely with deployment realities to improve efficiency, or by adjusting learning targets based on intermediate model behavior to enhance accuracy, especially when dealing with challenging data distributions.</insight>",-675.65625,-702.0625,-697.4375,-699.65625,1379.9375,-12.9933891296,-13.5012016296,-13.412260055499999,-13.454927444500001,"['LEARNING HARMONIZED REPRESENTATIONS\nSPECULATIVE SAMPLING\n\nFOR\n\nLefan Zhang, Xiaodan Wang, Yanhua Huang∗, Ruiwen Xu\nXiaohongshu Inc.\nShanghai, China\n{lefan,xiaodan2,yanhuahuang,ruiwenxu}@xiaohongshu.com\n\nABSTRACT\n\nSpeculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage tar-\nget LLM’s contextual information, such as hidden states and KV cache, have\nshown significant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing specula-\ntive sampling methods.\nIn this work, we propose a solution named HArmo-\nnized Speculative Sampling (HASS) that learns harmonized representations to\naddress these issues. HASS accelerates the decoding stage without adding infer-\nence overhead through harmonized objective distillation and harmonized context\nalignment. Experiments on four LLaMA models demonstrate that HASS achieves\n2.81x-4.05x wall-clock time speedup ratio averaging across three datasets, sur-\npassing EAGLE-2 by 8%-20%. The code is available at https://github.\ncom/HArmonizedSS/HASS.', 'RETHINKING CLASSIFIER RE-TRAINING IN LONG-\nTAILED RECOGNITION:\nLABEL OVER-SMOOTH CAN BALANCE\n\nSiyu Sun1,2#, Han Lu1,2#, Jiangtong Li3, Yichen Xie4, Tianjiao Li2\nXiaokang Yang1, Liqing Zhang1, Junchi Yan1∗\n1Department of CSE & MoE Key Lab of AI, Shanghai Jiao Tong University\n2Bilibili Inc\n{sunsiyu, sjtu luhan, xkyang, yanjunchi}@sjtu.edu.cn\njiangtongli@tongji.edu.cn, zhang-lq@cs.sjtu.edu.cn\nyichen xie@berkeley.edu, litianjiao01@bilibili.com\nhttps://github.com/Thinklab-SJTU/LOS\nCode:\n\n3Tongji University\n\n4UC Berkeley\n\nABSTRACT\n\nIn the field of long-tailed recognition, the Decoupled Training paradigm has shown\nexceptional promise by dividing training into two stages: representation learning\nand classifier re-training. While previous work has tried to improve both stages\nsimultaneously, this complicates isolating the effect of classifier re-training. Recent\nstudies reveal that simple regularization can produce strong feature representations,\nhighlighting the need to reassess classifier re-training methods. In this study, we\nrevisit classifier re-training methods based on a unified feature representation and\nre-evaluate their performances. We propose two new metrics, Logits Magnitude\nand Regularized Standard Deviation, to compare the differences and similarities\nbetween various methods. Using these two newly proposed metrics, we demonstrate\nthat when the Logits Magnitude across classes is nearly balanced, further reducing\nits overall value can effectively decrease errors and disturbances during training,\nleading to better model performance. Based on our analysis using these metrics,\nwe observe that adjusting the logits could improve model performance, leading\nus to develop a simple label over-smoothing approach to adjust the logits without\nrequiring prior knowledge of class distribution. This method softens the original\none-hot labels by assigning a probability slightly higher than 1\nK to the true class\nand slightly lower than 1\nK to the other classes, where K is the number of classes.\nOur method achieves state-of-the-art performance on various imbalanced datasets,\nincluding CIFAR100-LT, ImageNet-LT, and iNaturalist2018.']",680.28125,-19.375,-21.78125,26.537258148200003,13.082330703700002,-0.3725967407999988,-0.4188709258999985,T9u56s7mbk,OeKp3AdiVO,"['speculative sampling', 'large language model', 'long-tailed recognition and decoupled training']",True,"LEARNING HARMONIZED REPRESENTATIONS
SPECULATIVE SAMPLING

FOR

Lefan Zhang, Xiaodan Wang, Yanhua Huang∗, Ruiwen Xu
Xiaohongshu Inc.
Shanghai, China
{lefan,xiaodan2,yanhuahuang,ruiwenxu}@xiaohongshu.com

ABSTRACT

Speculative sampling is a promising approach to accelerate the decoding stage
for Large Language Models (LLMs). Recent advancements that leverage tar-
get LLM’s contextual information, such as hidden states and KV cache, have
shown significant practical improvements. However, these approaches suffer from
inconsistent context between training and decoding. We also observe another
discrepancy between the training and decoding objectives in existing specula-
tive sampling methods.
In this work, we propose a solution named HArmo-
nized Speculative Sampling (HASS) that learns harmonized representations to
address these issues. HASS accelerates the decoding stage without adding infer-
ence overhead through harmonized objective distillation and harmonized context
alignment. Experiments on four LLaMA models demonstrate that HASS achieves
2.81x-4.05x wall-clock time speedup ratio averaging across three datasets, sur-
passing EAGLE-2 by 8%-20%. The code is available at https://github.
com/HArmonizedSS/HASS.","RETHINKING CLASSIFIER RE-TRAINING IN LONG-
TAILED RECOGNITION:
LABEL OVER-SMOOTH CAN BALANCE

Siyu Sun1,2#, Han Lu1,2#, Jiangtong Li3, Yichen Xie4, Tianjiao Li2
Xiaokang Yang1, Liqing Zhang1, Junchi Yan1∗
1Department of CSE & MoE Key Lab of AI, Shanghai Jiao Tong University
2Bilibili Inc
{sunsiyu, sjtu luhan, xkyang, yanjunchi}@sjtu.edu.cn
jiangtongli@tongji.edu.cn, zhang-lq@cs.sjtu.edu.cn
yichen xie@berkeley.edu, litianjiao01@bilibili.com
https://github.com/Thinklab-SJTU/LOS
Code:

3Tongji University

4UC Berkeley

ABSTRACT

In the field of long-tailed recognition, the Decoupled Training paradigm has shown
exceptional promise by dividing training into two stages: representation learning
and classifier re-training. While previous work has tried to improve both stages
simultaneously, this complicates isolating the effect of classifier re-training. Recent
studies reveal that simple regularization can produce strong feature representations,
highlighting the need to reassess classifier re-training methods. In this study, we
revisit classifier re-training methods based on a unified feature representation and
re-evaluate their performances. We propose two new metrics, Logits Magnitude
and Regularized Standard Deviation, to compare the differences and similarities
between various methods. Using these two newly proposed metrics, we demonstrate
that when the Logits Magnitude across classes is nearly balanced, further reducing
its overall value can effectively decrease errors and disturbances during training,
leading to better model performance. Based on our analysis using these metrics,
we observe that adjusting the logits could improve model performance, leading
us to develop a simple label over-smoothing approach to adjust the logits without
requiring prior knowledge of class distribution. This method softens the original
one-hot labels by assigning a probability slightly higher than 1
K to the true class
and slightly lower than 1
K to the other classes, where K is the number of classes.
Our method achieves state-of-the-art performance on various imbalanced datasets,
including CIFAR100-LT, ImageNet-LT, and iNaturalist2018.",10
114,<insight>Addressing inconsistencies between different stages of machine learning model development—whether aligning training conditions with inference requirements for faster generation or adjusting learning objectives during fine-tuning for better classification on imbalanced data—can significantly improve overall model performance and efficiency.</insight>,-379.625,-517.78125,-513.9375,-517.5625,901.03125,-7.9088540077000005,-10.787109375,-10.70703125,-10.782551765400001,"['LEARNING HARMONIZED REPRESENTATIONS\nSPECULATIVE SAMPLING\n\nFOR\n\nLefan Zhang, Xiaodan Wang, Yanhua Huang∗, Ruiwen Xu\nXiaohongshu Inc.\nShanghai, China\n{lefan,xiaodan2,yanhuahuang,ruiwenxu}@xiaohongshu.com\n\nABSTRACT\n\nSpeculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage tar-\nget LLM’s contextual information, such as hidden states and KV cache, have\nshown significant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing specula-\ntive sampling methods.\nIn this work, we propose a solution named HArmo-\nnized Speculative Sampling (HASS) that learns harmonized representations to\naddress these issues. HASS accelerates the decoding stage without adding infer-\nence overhead through harmonized objective distillation and harmonized context\nalignment. Experiments on four LLaMA models demonstrate that HASS achieves\n2.81x-4.05x wall-clock time speedup ratio averaging across three datasets, sur-\npassing EAGLE-2 by 8%-20%. The code is available at https://github.\ncom/HArmonizedSS/HASS.', 'RETHINKING CLASSIFIER RE-TRAINING IN LONG-\nTAILED RECOGNITION:\nLABEL OVER-SMOOTH CAN BALANCE\n\nSiyu Sun1,2#, Han Lu1,2#, Jiangtong Li3, Yichen Xie4, Tianjiao Li2\nXiaokang Yang1, Liqing Zhang1, Junchi Yan1∗\n1Department of CSE & MoE Key Lab of AI, Shanghai Jiao Tong University\n2Bilibili Inc\n{sunsiyu, sjtu luhan, xkyang, yanjunchi}@sjtu.edu.cn\njiangtongli@tongji.edu.cn, zhang-lq@cs.sjtu.edu.cn\nyichen xie@berkeley.edu, litianjiao01@bilibili.com\nhttps://github.com/Thinklab-SJTU/LOS\nCode:\n\n3Tongji University\n\n4UC Berkeley\n\nABSTRACT\n\nIn the field of long-tailed recognition, the Decoupled Training paradigm has shown\nexceptional promise by dividing training into two stages: representation learning\nand classifier re-training. While previous work has tried to improve both stages\nsimultaneously, this complicates isolating the effect of classifier re-training. Recent\nstudies reveal that simple regularization can produce strong feature representations,\nhighlighting the need to reassess classifier re-training methods. In this study, we\nrevisit classifier re-training methods based on a unified feature representation and\nre-evaluate their performances. We propose two new metrics, Logits Magnitude\nand Regularized Standard Deviation, to compare the differences and similarities\nbetween various methods. Using these two newly proposed metrics, we demonstrate\nthat when the Logits Magnitude across classes is nearly balanced, further reducing\nits overall value can effectively decrease errors and disturbances during training,\nleading to better model performance. Based on our analysis using these metrics,\nwe observe that adjusting the logits could improve model performance, leading\nus to develop a simple label over-smoothing approach to adjust the logits without\nrequiring prior knowledge of class distribution. This method softens the original\none-hot labels by assigning a probability slightly higher than 1\nK to the true class\nand slightly lower than 1\nK to the other classes, where K is the number of classes.\nOur method achieves state-of-the-art performance on various imbalanced datasets,\nincluding CIFAR100-LT, ImageNet-LT, and iNaturalist2018.']",383.46875,-134.09375,-134.3125,18.7714838981,7.9889321327000005,-2.7936196327000005,-2.7981772422999995,T9u56s7mbk,OeKp3AdiVO,"['speculative sampling', 'large language model', 'long-tailed recognition and decoupled training']",True,"LEARNING HARMONIZED REPRESENTATIONS
SPECULATIVE SAMPLING

FOR

Lefan Zhang, Xiaodan Wang, Yanhua Huang∗, Ruiwen Xu
Xiaohongshu Inc.
Shanghai, China
{lefan,xiaodan2,yanhuahuang,ruiwenxu}@xiaohongshu.com

ABSTRACT

Speculative sampling is a promising approach to accelerate the decoding stage
for Large Language Models (LLMs). Recent advancements that leverage tar-
get LLM’s contextual information, such as hidden states and KV cache, have
shown significant practical improvements. However, these approaches suffer from
inconsistent context between training and decoding. We also observe another
discrepancy between the training and decoding objectives in existing specula-
tive sampling methods.
In this work, we propose a solution named HArmo-
nized Speculative Sampling (HASS) that learns harmonized representations to
address these issues. HASS accelerates the decoding stage without adding infer-
ence overhead through harmonized objective distillation and harmonized context
alignment. Experiments on four LLaMA models demonstrate that HASS achieves
2.81x-4.05x wall-clock time speedup ratio averaging across three datasets, sur-
passing EAGLE-2 by 8%-20%. The code is available at https://github.
com/HArmonizedSS/HASS.","RETHINKING CLASSIFIER RE-TRAINING IN LONG-
TAILED RECOGNITION:
LABEL OVER-SMOOTH CAN BALANCE

Siyu Sun1,2#, Han Lu1,2#, Jiangtong Li3, Yichen Xie4, Tianjiao Li2
Xiaokang Yang1, Liqing Zhang1, Junchi Yan1∗
1Department of CSE & MoE Key Lab of AI, Shanghai Jiao Tong University
2Bilibili Inc
{sunsiyu, sjtu luhan, xkyang, yanjunchi}@sjtu.edu.cn
jiangtongli@tongji.edu.cn, zhang-lq@cs.sjtu.edu.cn
yichen xie@berkeley.edu, litianjiao01@bilibili.com
https://github.com/Thinklab-SJTU/LOS
Code:

3Tongji University

4UC Berkeley

ABSTRACT

In the field of long-tailed recognition, the Decoupled Training paradigm has shown
exceptional promise by dividing training into two stages: representation learning
and classifier re-training. While previous work has tried to improve both stages
simultaneously, this complicates isolating the effect of classifier re-training. Recent
studies reveal that simple regularization can produce strong feature representations,
highlighting the need to reassess classifier re-training methods. In this study, we
revisit classifier re-training methods based on a unified feature representation and
re-evaluate their performances. We propose two new metrics, Logits Magnitude
and Regularized Standard Deviation, to compare the differences and similarities
between various methods. Using these two newly proposed metrics, we demonstrate
that when the Logits Magnitude across classes is nearly balanced, further reducing
its overall value can effectively decrease errors and disturbances during training,
leading to better model performance. Based on our analysis using these metrics,
we observe that adjusting the logits could improve model performance, leading
us to develop a simple label over-smoothing approach to adjust the logits without
requiring prior knowledge of class distribution. This method softens the original
one-hot labels by assigning a probability slightly higher than 1
K to the true class
and slightly lower than 1
K to the other classes, where K is the number of classes.
Our method achieves state-of-the-art performance on various imbalanced datasets,
including CIFAR100-LT, ImageNet-LT, and iNaturalist2018.",11
117,"<insight>The way a model balances relying on current input versus its internal knowledge is shaped by its training history; this dynamic relationship emphasizes the need for targeted methods to ensure the internal knowledge being relied upon is accurate and trustworthy, thereby mitigating errors like hallucinations.</insight>",-592.625,-506.34375,-478.3125,-644.078125,1264.734375,-11.8524999619,-10.1268749237,-9.5662498474,-12.881562233,"['TOWARD UNDERSTANDING IN-CONTEXT VS.\nWEIGHT LEARNING\n\nIN-\n\nBryan Chan1∗, Xinyi Chen2∗, Andr´as Gy¨orgy2, Dale Schuurmans1,2\n1University of Alberta\nbryan.chan@ualberta.ca\n\n2Google DeepMind\n\n{xinyic,agyorgy,schuurmans}@google.com\n\nABSTRACT\n\nIt has recently been demonstrated empirically that in-context learning emerges in\ntransformers when certain distributional properties are present in the training data,\nbut this ability can also diminish upon further training. We provide a new theo-\nretical understanding of these phenomena by identifying simplified distributional\nproperties that give rise to the emergence and eventual disappearance of in-context\nlearning. We do so by first analyzing a simplified model that uses a gating mech-\nanism to choose between an in-weight and an in-context predictor. Through a\ncombination of a generalization error and regret analysis we identify conditions\nwhere in-context and in-weight learning emerge. These theoretical findings are\nthen corroborated experimentally by comparing the behaviour of a full transformer\non the simplified distributions to that of the stylized model, demonstrating aligned\nresults. We then extend the study to a full large language model, showing how\nfine-tuning on various collections of natural language prompts can elicit similar\nin-context and in-weight learning behaviour.', 'REFINE KNOWLEDGE OF LARGE LANGUAGE MODELS\nVIA ADAPTIVE CONTRASTIVE LEARNING\n\nYinghui Li1,∗, Haojing Huang1,∗, Jiayi Kuang2,∗, Yangning Li1, Shu-Yu Guo1\nChao Qu3,, Xiaoyu Tan3,†, Hai-Tao Zheng1,4,†, Ying Shen2,†, Philip S. Yu5\n1Tsinghua University, 2Sun-Yat Sen University\n3INFLY TECH (Shanghai) Co., Ltd., 4Peng Cheng Laboratory\n5University of Illinois Chicago\n{liyinghu20, hhj23}@mails.tsinghua.edu.cn\n\nABSTRACT\n\nHow to alleviate the hallucinations of Large Language Models (LLMs) has always\nbeen the fundamental goal pursued by the LLMs research community. Looking\nthrough numerous hallucination-related studies, a mainstream category of meth-\nods is to reduce hallucinations by optimizing the knowledge representation of\nLLMs to change their output. Considering that the core focus of these works\nis the knowledge acquired by models, and knowledge has long been a central\ntheme in human societal progress, we believe that the process of models refining\nknowledge can greatly benefit from the way humans learn. In our work, by im-\nitating the human learning process, we design an Adaptive Contrastive Learning\nstrategy. Our method flexibly constructs different positive and negative samples\nfor contrastive learning based on LLMs’ actual mastery of knowledge. This strat-\negy helps LLMs consolidate the correct knowledge they already possess, deepen\ntheir understanding of the correct knowledge they have encountered but not fully\ngrasped, forget the incorrect knowledge they previously learned, and honestly ac-\nknowledge the knowledge they lack. Extensive experiments and detailed analyses\non widely used datasets demonstrate the effectiveness of our method.']",620.65625,-23.421875,28.03125,25.2946872712,12.4131250382,-0.4684371947999981,0.5606250763000009,aKJr5NnN8U,HqjRlT65WX,"['in-context learning', 'generalization error', 'transformers', 'large language models', 'model hallucination', 'model alignment']",True,"TOWARD UNDERSTANDING IN-CONTEXT VS.
WEIGHT LEARNING

IN-

Bryan Chan1∗, Xinyi Chen2∗, Andr´as Gy¨orgy2, Dale Schuurmans1,2
1University of Alberta
bryan.chan@ualberta.ca

2Google DeepMind

{xinyic,agyorgy,schuurmans}@google.com

ABSTRACT

It has recently been demonstrated empirically that in-context learning emerges in
transformers when certain distributional properties are present in the training data,
but this ability can also diminish upon further training. We provide a new theo-
retical understanding of these phenomena by identifying simplified distributional
properties that give rise to the emergence and eventual disappearance of in-context
learning. We do so by first analyzing a simplified model that uses a gating mech-
anism to choose between an in-weight and an in-context predictor. Through a
combination of a generalization error and regret analysis we identify conditions
where in-context and in-weight learning emerge. These theoretical findings are
then corroborated experimentally by comparing the behaviour of a full transformer
on the simplified distributions to that of the stylized model, demonstrating aligned
results. We then extend the study to a full large language model, showing how
fine-tuning on various collections of natural language prompts can elicit similar
in-context and in-weight learning behaviour.","REFINE KNOWLEDGE OF LARGE LANGUAGE MODELS
VIA ADAPTIVE CONTRASTIVE LEARNING

Yinghui Li1,∗, Haojing Huang1,∗, Jiayi Kuang2,∗, Yangning Li1, Shu-Yu Guo1
Chao Qu3,, Xiaoyu Tan3,†, Hai-Tao Zheng1,4,†, Ying Shen2,†, Philip S. Yu5
1Tsinghua University, 2Sun-Yat Sen University
3INFLY TECH (Shanghai) Co., Ltd., 4Peng Cheng Laboratory
5University of Illinois Chicago
{liyinghu20, hhj23}@mails.tsinghua.edu.cn

ABSTRACT

How to alleviate the hallucinations of Large Language Models (LLMs) has always
been the fundamental goal pursued by the LLMs research community. Looking
through numerous hallucination-related studies, a mainstream category of meth-
ods is to reduce hallucinations by optimizing the knowledge representation of
LLMs to change their output. Considering that the core focus of these works
is the knowledge acquired by models, and knowledge has long been a central
theme in human societal progress, we believe that the process of models refining
knowledge can greatly benefit from the way humans learn. In our work, by im-
itating the human learning process, we design an Adaptive Contrastive Learning
strategy. Our method flexibly constructs different positive and negative samples
for contrastive learning based on LLMs’ actual mastery of knowledge. This strat-
egy helps LLMs consolidate the correct knowledge they already possess, deepen
their understanding of the correct knowledge they have encountered but not fully
grasped, forget the incorrect knowledge they previously learned, and honestly ac-
knowledge the knowledge they lack. Extensive experiments and detailed analyses
on widely used datasets demonstrate the effectiveness of our method.",12
117,<insight>The significance of refining a language model's internal knowledge base is heightened when the model's training has conditioned it to prioritize recalling stored information over adapting to information presented within the immediate context.</insight>,-329.96875,-265.59375,-336.0625,-340.625,600.125,-9.1657981873,-7.3776040077000005,-9.3350696564,-9.4618053436,"['TOWARD UNDERSTANDING IN-CONTEXT VS.\nWEIGHT LEARNING\n\nIN-\n\nBryan Chan1∗, Xinyi Chen2∗, Andr´as Gy¨orgy2, Dale Schuurmans1,2\n1University of Alberta\nbryan.chan@ualberta.ca\n\n2Google DeepMind\n\n{xinyic,agyorgy,schuurmans}@google.com\n\nABSTRACT\n\nIt has recently been demonstrated empirically that in-context learning emerges in\ntransformers when certain distributional properties are present in the training data,\nbut this ability can also diminish upon further training. We provide a new theo-\nretical understanding of these phenomena by identifying simplified distributional\nproperties that give rise to the emergence and eventual disappearance of in-context\nlearning. We do so by first analyzing a simplified model that uses a gating mech-\nanism to choose between an in-weight and an in-context predictor. Through a\ncombination of a generalization error and regret analysis we identify conditions\nwhere in-context and in-weight learning emerge. These theoretical findings are\nthen corroborated experimentally by comparing the behaviour of a full transformer\non the simplified distributions to that of the stylized model, demonstrating aligned\nresults. We then extend the study to a full large language model, showing how\nfine-tuning on various collections of natural language prompts can elicit similar\nin-context and in-weight learning behaviour.', 'REFINE KNOWLEDGE OF LARGE LANGUAGE MODELS\nVIA ADAPTIVE CONTRASTIVE LEARNING\n\nYinghui Li1,∗, Haojing Huang1,∗, Jiayi Kuang2,∗, Yangning Li1, Shu-Yu Guo1\nChao Qu3,, Xiaoyu Tan3,†, Hai-Tao Zheng1,4,†, Ying Shen2,†, Philip S. Yu5\n1Tsinghua University, 2Sun-Yat Sen University\n3INFLY TECH (Shanghai) Co., Ltd., 4Peng Cheng Laboratory\n5University of Illinois Chicago\n{liyinghu20, hhj23}@mails.tsinghua.edu.cn\n\nABSTRACT\n\nHow to alleviate the hallucinations of Large Language Models (LLMs) has always\nbeen the fundamental goal pursued by the LLMs research community. Looking\nthrough numerous hallucination-related studies, a mainstream category of meth-\nods is to reduce hallucinations by optimizing the knowledge representation of\nLLMs to change their output. Considering that the core focus of these works\nis the knowledge acquired by models, and knowledge has long been a central\ntheme in human societal progress, we believe that the process of models refining\nknowledge can greatly benefit from the way humans learn. In our work, by im-\nitating the human learning process, we design an Adaptive Contrastive Learning\nstrategy. Our method flexibly constructs different positive and negative samples\nfor contrastive learning based on LLMs’ actual mastery of knowledge. This strat-\negy helps LLMs consolidate the correct knowledge they already possess, deepen\ntheir understanding of the correct knowledge they have encountered but not fully\ngrasped, forget the incorrect knowledge they previously learned, and honestly ac-\nknowledge the knowledge they lack. Extensive experiments and detailed analyses\non widely used datasets demonstrate the effectiveness of our method.']",259.5,-81.125,-70.46875,16.670137882200002,7.208332538600001,-2.2534728049999995,-1.9574656486999995,aKJr5NnN8U,HqjRlT65WX,"['in-context learning', 'generalization error', 'transformers', 'large language models', 'model hallucination', 'model alignment']",True,"TOWARD UNDERSTANDING IN-CONTEXT VS.
WEIGHT LEARNING

IN-

Bryan Chan1∗, Xinyi Chen2∗, Andr´as Gy¨orgy2, Dale Schuurmans1,2
1University of Alberta
bryan.chan@ualberta.ca

2Google DeepMind

{xinyic,agyorgy,schuurmans}@google.com

ABSTRACT

It has recently been demonstrated empirically that in-context learning emerges in
transformers when certain distributional properties are present in the training data,
but this ability can also diminish upon further training. We provide a new theo-
retical understanding of these phenomena by identifying simplified distributional
properties that give rise to the emergence and eventual disappearance of in-context
learning. We do so by first analyzing a simplified model that uses a gating mech-
anism to choose between an in-weight and an in-context predictor. Through a
combination of a generalization error and regret analysis we identify conditions
where in-context and in-weight learning emerge. These theoretical findings are
then corroborated experimentally by comparing the behaviour of a full transformer
on the simplified distributions to that of the stylized model, demonstrating aligned
results. We then extend the study to a full large language model, showing how
fine-tuning on various collections of natural language prompts can elicit similar
in-context and in-weight learning behaviour.","REFINE KNOWLEDGE OF LARGE LANGUAGE MODELS
VIA ADAPTIVE CONTRASTIVE LEARNING

Yinghui Li1,∗, Haojing Huang1,∗, Jiayi Kuang2,∗, Yangning Li1, Shu-Yu Guo1
Chao Qu3,, Xiaoyu Tan3,†, Hai-Tao Zheng1,4,†, Ying Shen2,†, Philip S. Yu5
1Tsinghua University, 2Sun-Yat Sen University
3INFLY TECH (Shanghai) Co., Ltd., 4Peng Cheng Laboratory
5University of Illinois Chicago
{liyinghu20, hhj23}@mails.tsinghua.edu.cn

ABSTRACT

How to alleviate the hallucinations of Large Language Models (LLMs) has always
been the fundamental goal pursued by the LLMs research community. Looking
through numerous hallucination-related studies, a mainstream category of meth-
ods is to reduce hallucinations by optimizing the knowledge representation of
LLMs to change their output. Considering that the core focus of these works
is the knowledge acquired by models, and knowledge has long been a central
theme in human societal progress, we believe that the process of models refining
knowledge can greatly benefit from the way humans learn. In our work, by im-
itating the human learning process, we design an Adaptive Contrastive Learning
strategy. Our method flexibly constructs different positive and negative samples
for contrastive learning based on LLMs’ actual mastery of knowledge. This strat-
egy helps LLMs consolidate the correct knowledge they already possess, deepen
their understanding of the correct knowledge they have encountered but not fully
grasped, forget the incorrect knowledge they previously learned, and honestly ac-
knowledge the knowledge they lack. Extensive experiments and detailed analyses
on widely used datasets demonstrate the effectiveness of our method.",13
127,"<insight>Incorporating structural knowledge is a key strategy for improving complex predictive models; this structure can be derived either from the inherent physical properties (like geometry and periodicity) of the input domain to improve data representation, or from abstract, general principles (like reasoning guidelines) guiding the desired process to enhance generalization across tasks.</insight>",-528.359375,-553.625,-493.375,-822.34375,1410.953125,-8.1286058426,-8.5173072815,-7.5903844833,-12.6514425278,"['CIRT: GLOBAL SUBSEASONAL-TO-SEASONAL FORE-\nCASTING WITH GEOMETRY-INSPIRED TRANSFORMER\n\nYang Liu1,2∗, Zinan Zheng1∗, Jiashun Cheng1,2, Fugee Tsung1,2, Deli Zhao3, Yu Rong3†, Jia Li1,2†\n1The Hong Kong University of Science and Technology (Guangzhou)\n2The Hong Kong University of Science and Technology 3DAMO Academy, Alibaba Group\n\nABSTRACT\n\n(S2S) climate forecasting is pivotal\n\nfor\nAccurate Subseasonal-to-Seasonal\ndecision-making including agriculture planning and disaster preparedness but is\nknown to be challenging due to its chaotic nature. Although recent data-driven\nmodels have shown promising results, their performance is limited by inadequate\nconsideration of geometric inductive biases. Usually, they treat the spherical\nweather data as planar images, resulting in an inaccurate representation of lo-\ncations and spatial relations.\nIn this work, we propose the geometric-inspired\nCircular Transformer (CirT) to model the cyclic characteristic of the graticule,\nconsisting of two key designs: (1) Decomposing the weather data by latitude\ninto circular patches that serve as input tokens to the Transformer; (2) Leveraging\nFourier transform in self-attention to capture the global information and model the\nspatial periodicity. Extensive experiments on the Earth Reanalysis 5 (ERA5) re-\nanalysis dataset demonstrate our model yields a significant improvement over the\nadvanced data-driven models, including PanguWeather and GraphCast, as well as\nskillful ECMWF systems. Additionally, we empirically show the effectiveness of\nour model designs and high-quality prediction over spatial and temporal dimen-\nsions. The code link is: https://github.com/compasszzn/CirT.', 'REGENESIS: LLMS CAN GROW INTO REASONING GEN-\nERALISTS VIA SELF-IMPROVEMENT\n\nXiangyu Peng Congying Xia∗† Xinyi Yang∗ Caiming Xiong Chien-Sheng Wu Chen Xing†\n\n{becky.peng, c.xia, x.yang, cxiong, wu.json, cxing}@salesforce.com\n\nSalesforce AI Research\n\nABSTRACT\n\nPost-training Large Language Models (LLMs) with explicit reasoning trajectories\ncan enhance their reasoning abilities. However, acquiring such high-quality tra-\njectory data typically demands meticulous supervision from humans or superior\nmodels, which can be either expensive or license-constrained. In this paper, we\nexplore how far an LLM can improve its reasoning by self-synthesizing reasoning\npaths as training data without any additional supervision. Existing self-synthesizing\nmethods, such as STaR, suffer from poor generalization to out-of-domain (OOD)\nreasoning tasks. We hypothesize it is due to that their self-synthesized reasoning\npaths are too task-specific, lacking general task-agnostic reasoning guidance. To\naddress this, we propose Reasoning Generalist via Self-Improvement (ReGenesis),\na method to self-synthesize reasoning paths as post-training data by progressing\nfrom abstract to concrete. More specifically, ReGenesis self-synthesizes reasoning\npaths by converting general reasoning guidelines into task-specific ones, generating\nreasoning structures, and subsequently transforming these structures into reason-\ning paths, without the need for human-designed task-specific examples used in\nexisting methods. We show that ReGenesis achieves superior performance on all\nin-domain and OOD settings tested compared to existing methods. For six OOD\ntasks specifically, while previous methods exhibited an average performance de-\ncrease of approximately 4.6% after post training, ReGenesis delivers around 6.1%\nperformance improvement. We also conduct in-depth analysis of our framework\nand show ReGenesis is effective across various LLMs and design choices.']",588.609375,-233.734375,34.984375,21.706971168600003,9.055528640800002,-3.5959138869999983,0.5382213593000005,YslOW2SO6S,YUYJsHOf3c,"['weather and climate forecasting', 'llm', 'reasoning', 'generalization', 'self-improvement']",True,"CIRT: GLOBAL SUBSEASONAL-TO-SEASONAL FORE-
CASTING WITH GEOMETRY-INSPIRED TRANSFORMER

Yang Liu1,2∗, Zinan Zheng1∗, Jiashun Cheng1,2, Fugee Tsung1,2, Deli Zhao3, Yu Rong3†, Jia Li1,2†
1The Hong Kong University of Science and Technology (Guangzhou)
2The Hong Kong University of Science and Technology 3DAMO Academy, Alibaba Group

ABSTRACT

(S2S) climate forecasting is pivotal

for
Accurate Subseasonal-to-Seasonal
decision-making including agriculture planning and disaster preparedness but is
known to be challenging due to its chaotic nature. Although recent data-driven
models have shown promising results, their performance is limited by inadequate
consideration of geometric inductive biases. Usually, they treat the spherical
weather data as planar images, resulting in an inaccurate representation of lo-
cations and spatial relations.
In this work, we propose the geometric-inspired
Circular Transformer (CirT) to model the cyclic characteristic of the graticule,
consisting of two key designs: (1) Decomposing the weather data by latitude
into circular patches that serve as input tokens to the Transformer; (2) Leveraging
Fourier transform in self-attention to capture the global information and model the
spatial periodicity. Extensive experiments on the Earth Reanalysis 5 (ERA5) re-
analysis dataset demonstrate our model yields a significant improvement over the
advanced data-driven models, including PanguWeather and GraphCast, as well as
skillful ECMWF systems. Additionally, we empirically show the effectiveness of
our model designs and high-quality prediction over spatial and temporal dimen-
sions. The code link is: https://github.com/compasszzn/CirT.","REGENESIS: LLMS CAN GROW INTO REASONING GEN-
ERALISTS VIA SELF-IMPROVEMENT

Xiangyu Peng Congying Xia∗† Xinyi Yang∗ Caiming Xiong Chien-Sheng Wu Chen Xing†

{becky.peng, c.xia, x.yang, cxiong, wu.json, cxing}@salesforce.com

Salesforce AI Research

ABSTRACT

Post-training Large Language Models (LLMs) with explicit reasoning trajectories
can enhance their reasoning abilities. However, acquiring such high-quality tra-
jectory data typically demands meticulous supervision from humans or superior
models, which can be either expensive or license-constrained. In this paper, we
explore how far an LLM can improve its reasoning by self-synthesizing reasoning
paths as training data without any additional supervision. Existing self-synthesizing
methods, such as STaR, suffer from poor generalization to out-of-domain (OOD)
reasoning tasks. We hypothesize it is due to that their self-synthesized reasoning
paths are too task-specific, lacking general task-agnostic reasoning guidance. To
address this, we propose Reasoning Generalist via Self-Improvement (ReGenesis),
a method to self-synthesize reasoning paths as post-training data by progressing
from abstract to concrete. More specifically, ReGenesis self-synthesizes reasoning
paths by converting general reasoning guidelines into task-specific ones, generating
reasoning structures, and subsequently transforming these structures into reason-
ing paths, without the need for human-designed task-specific examples used in
existing methods. We show that ReGenesis achieves superior performance on all
in-domain and OOD settings tested compared to existing methods. For six OOD
tasks specifically, while previous methods exhibited an average performance de-
crease of approximately 4.6% after post training, ReGenesis delivers around 6.1%
performance improvement. We also conduct in-depth analysis of our framework
and show ReGenesis is effective across various LLMs and design choices.",14
127,<insight>Incorporating fundamental structural priors—such as inherent geometric properties for modeling physical systems or abstract procedural structures for tackling reasoning tasks—can significantly enhance the generalization capabilities of complex AI models across diverse domains.</insight>,-332.84375,-384.46875,-355.09375,-415.15625,777.375,-7.9248514175,-9.1540174484,-8.4546127319,-9.8846721649,"['CIRT: GLOBAL SUBSEASONAL-TO-SEASONAL FORE-\nCASTING WITH GEOMETRY-INSPIRED TRANSFORMER\n\nYang Liu1,2∗, Zinan Zheng1∗, Jiashun Cheng1,2, Fugee Tsung1,2, Deli Zhao3, Yu Rong3†, Jia Li1,2†\n1The Hong Kong University of Science and Technology (Guangzhou)\n2The Hong Kong University of Science and Technology 3DAMO Academy, Alibaba Group\n\nABSTRACT\n\n(S2S) climate forecasting is pivotal\n\nfor\nAccurate Subseasonal-to-Seasonal\ndecision-making including agriculture planning and disaster preparedness but is\nknown to be challenging due to its chaotic nature. Although recent data-driven\nmodels have shown promising results, their performance is limited by inadequate\nconsideration of geometric inductive biases. Usually, they treat the spherical\nweather data as planar images, resulting in an inaccurate representation of lo-\ncations and spatial relations.\nIn this work, we propose the geometric-inspired\nCircular Transformer (CirT) to model the cyclic characteristic of the graticule,\nconsisting of two key designs: (1) Decomposing the weather data by latitude\ninto circular patches that serve as input tokens to the Transformer; (2) Leveraging\nFourier transform in self-attention to capture the global information and model the\nspatial periodicity. Extensive experiments on the Earth Reanalysis 5 (ERA5) re-\nanalysis dataset demonstrate our model yields a significant improvement over the\nadvanced data-driven models, including PanguWeather and GraphCast, as well as\nskillful ECMWF systems. Additionally, we empirically show the effectiveness of\nour model designs and high-quality prediction over spatial and temporal dimen-\nsions. The code link is: https://github.com/compasszzn/CirT.', 'REGENESIS: LLMS CAN GROW INTO REASONING GEN-\nERALISTS VIA SELF-IMPROVEMENT\n\nXiangyu Peng Congying Xia∗† Xinyi Yang∗ Caiming Xiong Chien-Sheng Wu Chen Xing†\n\n{becky.peng, c.xia, x.yang, cxiong, wu.json, cxing}@salesforce.com\n\nSalesforce AI Research\n\nABSTRACT\n\nPost-training Large Language Models (LLMs) with explicit reasoning trajectories\ncan enhance their reasoning abilities. However, acquiring such high-quality tra-\njectory data typically demands meticulous supervision from humans or superior\nmodels, which can be either expensive or license-constrained. In this paper, we\nexplore how far an LLM can improve its reasoning by self-synthesizing reasoning\npaths as training data without any additional supervision. Existing self-synthesizing\nmethods, such as STaR, suffer from poor generalization to out-of-domain (OOD)\nreasoning tasks. We hypothesize it is due to that their self-synthesized reasoning\npaths are too task-specific, lacking general task-agnostic reasoning guidance. To\naddress this, we propose Reasoning Generalist via Self-Improvement (ReGenesis),\na method to self-synthesize reasoning paths as post-training data by progressing\nfrom abstract to concrete. More specifically, ReGenesis self-synthesizes reasoning\npaths by converting general reasoning guidelines into task-specific ones, generating\nreasoning structures, and subsequently transforming these structures into reason-\ning paths, without the need for human-designed task-specific examples used in\nexisting methods. We show that ReGenesis achieves superior performance on all\nin-domain and OOD settings tested compared to existing methods. For six OOD\ntasks specifically, while previous methods exhibited an average performance de-\ncrease of approximately 4.6% after post training, ReGenesis delivers around 6.1%\nperformance improvement. We also conduct in-depth analysis of our framework\nand show ReGenesis is effective across various LLMs and design choices.']",362.21875,-52.9375,-22.25,18.5089282989,8.624256134,-1.2604160308999983,-0.5297613143999991,YslOW2SO6S,YUYJsHOf3c,"['weather and climate forecasting', 'llm', 'reasoning', 'generalization', 'self-improvement']",True,"CIRT: GLOBAL SUBSEASONAL-TO-SEASONAL FORE-
CASTING WITH GEOMETRY-INSPIRED TRANSFORMER

Yang Liu1,2∗, Zinan Zheng1∗, Jiashun Cheng1,2, Fugee Tsung1,2, Deli Zhao3, Yu Rong3†, Jia Li1,2†
1The Hong Kong University of Science and Technology (Guangzhou)
2The Hong Kong University of Science and Technology 3DAMO Academy, Alibaba Group

ABSTRACT

(S2S) climate forecasting is pivotal

for
Accurate Subseasonal-to-Seasonal
decision-making including agriculture planning and disaster preparedness but is
known to be challenging due to its chaotic nature. Although recent data-driven
models have shown promising results, their performance is limited by inadequate
consideration of geometric inductive biases. Usually, they treat the spherical
weather data as planar images, resulting in an inaccurate representation of lo-
cations and spatial relations.
In this work, we propose the geometric-inspired
Circular Transformer (CirT) to model the cyclic characteristic of the graticule,
consisting of two key designs: (1) Decomposing the weather data by latitude
into circular patches that serve as input tokens to the Transformer; (2) Leveraging
Fourier transform in self-attention to capture the global information and model the
spatial periodicity. Extensive experiments on the Earth Reanalysis 5 (ERA5) re-
analysis dataset demonstrate our model yields a significant improvement over the
advanced data-driven models, including PanguWeather and GraphCast, as well as
skillful ECMWF systems. Additionally, we empirically show the effectiveness of
our model designs and high-quality prediction over spatial and temporal dimen-
sions. The code link is: https://github.com/compasszzn/CirT.","REGENESIS: LLMS CAN GROW INTO REASONING GEN-
ERALISTS VIA SELF-IMPROVEMENT

Xiangyu Peng Congying Xia∗† Xinyi Yang∗ Caiming Xiong Chien-Sheng Wu Chen Xing†

{becky.peng, c.xia, x.yang, cxiong, wu.json, cxing}@salesforce.com

Salesforce AI Research

ABSTRACT

Post-training Large Language Models (LLMs) with explicit reasoning trajectories
can enhance their reasoning abilities. However, acquiring such high-quality tra-
jectory data typically demands meticulous supervision from humans or superior
models, which can be either expensive or license-constrained. In this paper, we
explore how far an LLM can improve its reasoning by self-synthesizing reasoning
paths as training data without any additional supervision. Existing self-synthesizing
methods, such as STaR, suffer from poor generalization to out-of-domain (OOD)
reasoning tasks. We hypothesize it is due to that their self-synthesized reasoning
paths are too task-specific, lacking general task-agnostic reasoning guidance. To
address this, we propose Reasoning Generalist via Self-Improvement (ReGenesis),
a method to self-synthesize reasoning paths as post-training data by progressing
from abstract to concrete. More specifically, ReGenesis self-synthesizes reasoning
paths by converting general reasoning guidelines into task-specific ones, generating
reasoning structures, and subsequently transforming these structures into reason-
ing paths, without the need for human-designed task-specific examples used in
existing methods. We show that ReGenesis achieves superior performance on all
in-domain and OOD settings tested compared to existing methods. For six OOD
tasks specifically, while previous methods exhibited an average performance de-
crease of approximately 4.6% after post training, ReGenesis delivers around 6.1%
performance improvement. We also conduct in-depth analysis of our framework
and show ReGenesis is effective across various LLMs and design choices.",15
145,<insight>Mechanisms designed to align generative systems using weak supervisory signals can be crucial for directing highly expressive models—capable of producing diverse and novel structures—towards outputs that satisfy complex or difficult-to-formalize desirable criteria.</insight>,-512.0625,-402.625,-479.59375,-458.8828125,893.9765625,-10.894947052,-8.5664892197,-10.2041225433,-9.763463974,"['MAGNET: MOTIF-AGNOSTIC GENERATION OF\nMOLECULES FROM SCAFFOLDS\n\nLeon Hetzel∗1(cid:57)3, Johanna Sommer∗1,2, Bastian Rieck1,3,4, Fabian Theis1(cid:57)3 & Stephan G ¨unnemann1,2\n1 School of Computation, Information and Technology, Technical University of Munich\n2 Munich Data Science Institute, Technical University of Munich\n3 Center for Computation Health, Helmholtz Munich\n4 Department of Computer Science, University of Fribourg\n{l.hetzel, jm.sommer, b.rieck, f.theis, s.guennemann}@tum.de\n\nABSTRACT\n\nRecent advances in machine learning for molecules exhibit great potential for\nfacilitating drug discovery from in silico predictions. Most models for molecule\ngeneration rely on the decomposition of molecules into frequently occurring sub-\nstructures (motifs), from which they generate novel compounds. While motif\nrepresentations greatly aid in learning molecular distributions, such methods fail\nto represent substructures beyond their known motif set, posing a fundamental\nlimitation for discovering novel compounds. To address this limitation and enhance\nstructural expressivity, we propose to separate structure from features by abstract-\ning motifs to scaffolds and, subsequently, allocating atom and bond types. To this\nend, we introduce a novel factorisation of the molecules’ data distribution that\nconsiders the entire molecular context and facilitates learning adequate assignments\nof atoms and bonds to scaffolds. Complementary to this, we propose MAGNet,\nthe first model to freely learn motifs. Importantly, we demonstrate that MAGNet’s\nimproved expressivity leads to molecules with more structural diversity and, at the\nsame time, diverse atom and bond assignments.', 'MACPO: WEAK-TO-STRONG ALIGNMENT VIA MULTI-\nAGENT CONTRASTIVE PREFERENCE OPTIMIZATION\n\nYougang Lyu1\n\nLingyong Yan2\n\nZihan Wang1\n\nDawei Yin2\n\nPengjie Ren3\n\nMaarten de Rijke1\n\nZhaochun Ren4∗\n\n1University of Amsterdam\n3Shandong University\n{youganglyu,lingyongy,zihanwang.sdu}@gmail.com, yindawei@acm.org\njay.ren@outlook.com, m.derijke@uva.nl, z.ren@liacs.leidenuniv.nl\n\n2Baidu Inc.\n\n4Leiden University\n\nABSTRACT\n\nAs large language models (LLMs) are rapidly advancing and achieving near-human\ncapabilities on specific tasks, aligning them with human values is becoming more\nurgent. In scenarios where LLMs outperform humans, we face a weak-to-strong\nalignment problem where we need to effectively align strong student LLMs through\nweak supervision generated by weak teachers. Existing alignment methods mainly\nfocus on strong-to-weak alignment and self-alignment settings, and it is impractical\nto adapt them to the much harder weak-to-strong alignment setting. To fill this gap,\nwe propose a multi-agent contrastive preference optimization (MACPO) framework.\nMACPO facilitates weak teachers and strong students to learn from each other\nby iteratively reinforcing unfamiliar positive behaviors while penalizing familiar\nnegative ones. To get this, we devise a mutual positive behavior augmentation\nstrategy to encourage weak teachers and strong students to learn from each other’s\npositive behavior and further provide higher quality positive behavior for the next\niteration. Additionally, we propose a hard negative behavior construction strategy\nto induce weak teachers and strong students to generate familiar negative behavior\nby fine-tuning on negative behavioral data. Experimental results on the HH-RLHF\nand PKU-SafeRLHF datasets, evaluated using both automatic metrics and human\njudgments, demonstrate that MACPO simultaneously improves the alignment\nperformance of strong students and weak teachers. Moreover, as the number\nof weak teachers increases, MACPO achieves better weak-to-strong alignment\nperformance through more iteration optimization rounds.']",435.09375,-23.7890625,-76.96875,19.020777702399997,9.257313728399998,-0.5061502456000007,-1.6376333236000011,5FXKgOxmb2,x1Okv4kbVR,"['graph generative models', '2d molecules', 'weak-to-strong alignment', 'preference optimization']",True,"MAGNET: MOTIF-AGNOSTIC GENERATION OF
MOLECULES FROM SCAFFOLDS

Leon Hetzel∗1(cid:57)3, Johanna Sommer∗1,2, Bastian Rieck1,3,4, Fabian Theis1(cid:57)3 & Stephan G ¨unnemann1,2
1 School of Computation, Information and Technology, Technical University of Munich
2 Munich Data Science Institute, Technical University of Munich
3 Center for Computation Health, Helmholtz Munich
4 Department of Computer Science, University of Fribourg
{l.hetzel, jm.sommer, b.rieck, f.theis, s.guennemann}@tum.de

ABSTRACT

Recent advances in machine learning for molecules exhibit great potential for
facilitating drug discovery from in silico predictions. Most models for molecule
generation rely on the decomposition of molecules into frequently occurring sub-
structures (motifs), from which they generate novel compounds. While motif
representations greatly aid in learning molecular distributions, such methods fail
to represent substructures beyond their known motif set, posing a fundamental
limitation for discovering novel compounds. To address this limitation and enhance
structural expressivity, we propose to separate structure from features by abstract-
ing motifs to scaffolds and, subsequently, allocating atom and bond types. To this
end, we introduce a novel factorisation of the molecules’ data distribution that
considers the entire molecular context and facilitates learning adequate assignments
of atoms and bonds to scaffolds. Complementary to this, we propose MAGNet,
the first model to freely learn motifs. Importantly, we demonstrate that MAGNet’s
improved expressivity leads to molecules with more structural diversity and, at the
same time, diverse atom and bond assignments.","MACPO: WEAK-TO-STRONG ALIGNMENT VIA MULTI-
AGENT CONTRASTIVE PREFERENCE OPTIMIZATION

Yougang Lyu1

Lingyong Yan2

Zihan Wang1

Dawei Yin2

Pengjie Ren3

Maarten de Rijke1

Zhaochun Ren4∗

1University of Amsterdam
3Shandong University
{youganglyu,lingyongy,zihanwang.sdu}@gmail.com, yindawei@acm.org
jay.ren@outlook.com, m.derijke@uva.nl, z.ren@liacs.leidenuniv.nl

2Baidu Inc.

4Leiden University

ABSTRACT

As large language models (LLMs) are rapidly advancing and achieving near-human
capabilities on specific tasks, aligning them with human values is becoming more
urgent. In scenarios where LLMs outperform humans, we face a weak-to-strong
alignment problem where we need to effectively align strong student LLMs through
weak supervision generated by weak teachers. Existing alignment methods mainly
focus on strong-to-weak alignment and self-alignment settings, and it is impractical
to adapt them to the much harder weak-to-strong alignment setting. To fill this gap,
we propose a multi-agent contrastive preference optimization (MACPO) framework.
MACPO facilitates weak teachers and strong students to learn from each other
by iteratively reinforcing unfamiliar positive behaviors while penalizing familiar
negative ones. To get this, we devise a mutual positive behavior augmentation
strategy to encourage weak teachers and strong students to learn from each other’s
positive behavior and further provide higher quality positive behavior for the next
iteration. Additionally, we propose a hard negative behavior construction strategy
to induce weak teachers and strong students to generate familiar negative behavior
by fine-tuning on negative behavioral data. Experimental results on the HH-RLHF
and PKU-SafeRLHF datasets, evaluated using both automatic metrics and human
judgments, demonstrate that MACPO simultaneously improves the alignment
performance of strong students and weak teachers. Moreover, as the number
of weak teachers increases, MACPO achieves better weak-to-strong alignment
performance through more iteration optimization rounds.",16
145,"<insight>Structural diversity in generation tasks using flexible representations can be further enhanced through interactive learning among multiple models, where models mutually reinforce the exploration of novel structures and penalize redundant outputs.</insight>",-396.125,-309.125,-357.71875,-199.2265625,546.7578125,-11.3178567886,-8.8321428299,-10.2205352783,-5.6921873093,"['MAGNET: MOTIF-AGNOSTIC GENERATION OF\nMOLECULES FROM SCAFFOLDS\n\nLeon Hetzel∗1(cid:57)3, Johanna Sommer∗1,2, Bastian Rieck1,3,4, Fabian Theis1(cid:57)3 & Stephan G ¨unnemann1,2\n1 School of Computation, Information and Technology, Technical University of Munich\n2 Munich Data Science Institute, Technical University of Munich\n3 Center for Computation Health, Helmholtz Munich\n4 Department of Computer Science, University of Fribourg\n{l.hetzel, jm.sommer, b.rieck, f.theis, s.guennemann}@tum.de\n\nABSTRACT\n\nRecent advances in machine learning for molecules exhibit great potential for\nfacilitating drug discovery from in silico predictions. Most models for molecule\ngeneration rely on the decomposition of molecules into frequently occurring sub-\nstructures (motifs), from which they generate novel compounds. While motif\nrepresentations greatly aid in learning molecular distributions, such methods fail\nto represent substructures beyond their known motif set, posing a fundamental\nlimitation for discovering novel compounds. To address this limitation and enhance\nstructural expressivity, we propose to separate structure from features by abstract-\ning motifs to scaffolds and, subsequently, allocating atom and bond types. To this\nend, we introduce a novel factorisation of the molecules’ data distribution that\nconsiders the entire molecular context and facilitates learning adequate assignments\nof atoms and bonds to scaffolds. Complementary to this, we propose MAGNet,\nthe first model to freely learn motifs. Importantly, we demonstrate that MAGNet’s\nimproved expressivity leads to molecules with more structural diversity and, at the\nsame time, diverse atom and bond assignments.', 'MACPO: WEAK-TO-STRONG ALIGNMENT VIA MULTI-\nAGENT CONTRASTIVE PREFERENCE OPTIMIZATION\n\nYougang Lyu1\n\nLingyong Yan2\n\nZihan Wang1\n\nDawei Yin2\n\nPengjie Ren3\n\nMaarten de Rijke1\n\nZhaochun Ren4∗\n\n1University of Amsterdam\n3Shandong University\n{youganglyu,lingyongy,zihanwang.sdu}@gmail.com, yindawei@acm.org\njay.ren@outlook.com, m.derijke@uva.nl, z.ren@liacs.leidenuniv.nl\n\n2Baidu Inc.\n\n4Leiden University\n\nABSTRACT\n\nAs large language models (LLMs) are rapidly advancing and achieving near-human\ncapabilities on specific tasks, aligning them with human values is becoming more\nurgent. In scenarios where LLMs outperform humans, we face a weak-to-strong\nalignment problem where we need to effectively align strong student LLMs through\nweak supervision generated by weak teachers. Existing alignment methods mainly\nfocus on strong-to-weak alignment and self-alignment settings, and it is impractical\nto adapt them to the much harder weak-to-strong alignment setting. To fill this gap,\nwe propose a multi-agent contrastive preference optimization (MACPO) framework.\nMACPO facilitates weak teachers and strong students to learn from each other\nby iteratively reinforcing unfamiliar positive behaviors while penalizing familiar\nnegative ones. To get this, we devise a mutual positive behavior augmentation\nstrategy to encourage weak teachers and strong students to learn from each other’s\npositive behavior and further provide higher quality positive behavior for the next\niteration. Additionally, we propose a hard negative behavior construction strategy\nto induce weak teachers and strong students to generate familiar negative behavior\nby fine-tuning on negative behavioral data. Experimental results on the HH-RLHF\nand PKU-SafeRLHF datasets, evaluated using both automatic metrics and human\njudgments, demonstrate that MACPO simultaneously improves the alignment\nperformance of strong students and weak teachers. Moreover, as the number\nof weak teachers increases, MACPO achieves better weak-to-strong alignment\nperformance through more iteration optimization rounds.']",347.53125,148.3046875,-158.4921875,15.621651649500002,9.929464340200001,4.2372770309,-4.5283479689999995,5FXKgOxmb2,x1Okv4kbVR,"['graph generative models', '2d molecules', 'weak-to-strong alignment', 'preference optimization']",True,"MAGNET: MOTIF-AGNOSTIC GENERATION OF
MOLECULES FROM SCAFFOLDS

Leon Hetzel∗1(cid:57)3, Johanna Sommer∗1,2, Bastian Rieck1,3,4, Fabian Theis1(cid:57)3 & Stephan G ¨unnemann1,2
1 School of Computation, Information and Technology, Technical University of Munich
2 Munich Data Science Institute, Technical University of Munich
3 Center for Computation Health, Helmholtz Munich
4 Department of Computer Science, University of Fribourg
{l.hetzel, jm.sommer, b.rieck, f.theis, s.guennemann}@tum.de

ABSTRACT

Recent advances in machine learning for molecules exhibit great potential for
facilitating drug discovery from in silico predictions. Most models for molecule
generation rely on the decomposition of molecules into frequently occurring sub-
structures (motifs), from which they generate novel compounds. While motif
representations greatly aid in learning molecular distributions, such methods fail
to represent substructures beyond their known motif set, posing a fundamental
limitation for discovering novel compounds. To address this limitation and enhance
structural expressivity, we propose to separate structure from features by abstract-
ing motifs to scaffolds and, subsequently, allocating atom and bond types. To this
end, we introduce a novel factorisation of the molecules’ data distribution that
considers the entire molecular context and facilitates learning adequate assignments
of atoms and bonds to scaffolds. Complementary to this, we propose MAGNet,
the first model to freely learn motifs. Importantly, we demonstrate that MAGNet’s
improved expressivity leads to molecules with more structural diversity and, at the
same time, diverse atom and bond assignments.","MACPO: WEAK-TO-STRONG ALIGNMENT VIA MULTI-
AGENT CONTRASTIVE PREFERENCE OPTIMIZATION

Yougang Lyu1

Lingyong Yan2

Zihan Wang1

Dawei Yin2

Pengjie Ren3

Maarten de Rijke1

Zhaochun Ren4∗

1University of Amsterdam
3Shandong University
{youganglyu,lingyongy,zihanwang.sdu}@gmail.com, yindawei@acm.org
jay.ren@outlook.com, m.derijke@uva.nl, z.ren@liacs.leidenuniv.nl

2Baidu Inc.

4Leiden University

ABSTRACT

As large language models (LLMs) are rapidly advancing and achieving near-human
capabilities on specific tasks, aligning them with human values is becoming more
urgent. In scenarios where LLMs outperform humans, we face a weak-to-strong
alignment problem where we need to effectively align strong student LLMs through
weak supervision generated by weak teachers. Existing alignment methods mainly
focus on strong-to-weak alignment and self-alignment settings, and it is impractical
to adapt them to the much harder weak-to-strong alignment setting. To fill this gap,
we propose a multi-agent contrastive preference optimization (MACPO) framework.
MACPO facilitates weak teachers and strong students to learn from each other
by iteratively reinforcing unfamiliar positive behaviors while penalizing familiar
negative ones. To get this, we devise a mutual positive behavior augmentation
strategy to encourage weak teachers and strong students to learn from each other’s
positive behavior and further provide higher quality positive behavior for the next
iteration. Additionally, we propose a hard negative behavior construction strategy
to induce weak teachers and strong students to generate familiar negative behavior
by fine-tuning on negative behavioral data. Experimental results on the HH-RLHF
and PKU-SafeRLHF datasets, evaluated using both automatic metrics and human
judgments, demonstrate that MACPO simultaneously improves the alignment
performance of strong students and weak teachers. Moreover, as the number
of weak teachers increases, MACPO achieves better weak-to-strong alignment
performance through more iteration optimization rounds.",17
148,"<insight>Optimizing the computational efficiency of neural networks, for instance through quantization techniques applied to specialized architectures, becomes particularly valuable for enabling sophisticated text-rich visual understanding on resource-constrained devices, especially when these models are trained using large-scale, structured datasets derived from environments like user interfaces.</insight>",-665.53125,-750.09375,-755.375,-772.4375,1432.6875,-11.8844861984,-13.39453125,-13.4888391495,-13.7935266495,"['HARNESSING WEBPAGE UIS FOR TEXT-RICH VISUAL\nUNDERSTANDING\n\n⋄Junpeng Liu, ♡Tianyue Ou∗, ¶Yifan Song∗, ♡Yuxiao Qu∗,\n⋄Wai Lam, ♡Chenyan Xiong, ♣Wenhu Chen, ♡Graham Neubig, ♡Xiang Yue†\n♡Carnegie Mellon University, ⋄The Chinese University of Hong Kong\n¶Peking University, ♣University of Waterloo\njpliu@link.cuhk.edu.hk\n\nxyue2@andrew.cmu.edu\n\nABSTRACT\n\nText-rich visual understanding—the ability to process environments where dense\ntextual content is integrated with visuals—is crucial for multimodal large lan-\nguage models (MLLMs) to interact effectively with structured environments. To\nenhance this capability, we propose synthesizing general multimodal instructions\nfrom webpage UIs using text-based large language models (LLMs). Despite lack-\ning direct visual input, text-based LLMs are able to process structured text rep-\nresentations from webpage accessibility trees. These instructions are then paired\nwith UI screenshots to train multimodal models. We introduce MultiUI, a dataset\ncontaining 7.3 million samples from 1 million websites, covering diverse multi-\nmodal tasks and UI layouts. Models trained on MultiUI not only excel in web\nUI tasks—achieving up to a 48% improvement on VisualWebBench and a 19.1%\nboost in element accuracy on a web agent dataset Mind2Web—but also generalize\nsurprisingly well to non-web UI tasks and even to non-UI domains, such as doc-\nument understanding, OCR, and chart interpretation. These results highlight the\nbroad applicability of web UI data for advancing text-rich visual understanding\nacross various scenarios.', 'QUANTIZED SPIKE-DRIVEN TRANSFORMER\n\nXuerui Qiu1,2,3∗, Malu Zhang1†, Jieyuan Zhang1∗, Wenjie Wei1, Honglin Cao1,\nJunsheng Guo4, Rui-Jie Zhu5, Yimeng Shan6, Yang Yang1, Haizhou Li7\n\n1University of Electronic Science and Technology of China,\n2Institute of Automation, Chinese Academy of Sciences,\n3School of Future Technology, University of Chinese Academy of Sciences,\n4China Agricultural University, 5University of California, Santa Cruz,\n6Liaoning Technical University, 7Chinese University of Hong Kong (Shenzhen)\n\nABSTRACT\n\nSpiking neural networks (SNNs) are emerging as a promising energy-efficient\nalternative to traditional artificial neural networks (ANNs) due to their spike-driven\nparadigm. However, recent research in the SNN domain has mainly focused\non enhancing accuracy by designing large-scale Transformer structures, which\ntypically rely on substantial computational resources, limiting their deployment on\nresource-constrained devices. To overcome this challenge, we propose a quantized\nspike-driven Transformer baseline (QSD-Transformer), which achieves reduced\nresource demands by utilizing a low bit-width parameter. Regrettably, the QSD-\nTransformer often suffers from severe performance degradation. In this paper, we\nfirst conduct empirical analysis and find that the bimodal distribution of quantized\nspike-driven self-attention (Q-SDSA) leads to spike information distortion (SID)\nduring quantization, causing significant performance degradation. To mitigate\nthis issue, we take inspiration from mutual information entropy and propose a\nbi-level optimization strategy to rectify the information distribution in Q-SDSA.\nSpecifically, at the lower level, we introduce an information-enhanced LIF to rectify\nthe information distribution in Q-SDSA. At the upper level, we propose a fine-\ngrained distillation scheme for the QSD-Transformer to align the distribution in\nQ-SDSA with that in the counterpart ANN. By integrating the bi-level optimization\nstrategy, the QSD-Transformer can attain enhanced energy efficiency without\nsacrificing its high-performance advantage. We validate the QSD-Transformer on\nvarious visual tasks, and experimental results indicate that our method achieves\nstate-of-the-art results in the SNN domain. For instance, when compared to the\nprior SNN benchmark on ImageNet, the QSD-Transformer achieves 80.3% top-\n1 accuracy, accompanied by significant reductions of 6.0× and 8.1× in power\nconsumption and model size, respectively. Code is available at Quantized Spike-\ndriven Transformer.']",660.25,-112.1875,-89.84375,25.583704948399998,11.790178298899999,-2.0033483506000014,-1.604352951100001,IIsTO4P3Ag,5J9B7Sb8rO,"['multimodal', 'instruction-tuning', 'large language model', 'spiking neural network+spike-driven+quantized spiking transformer+ neuromorphic computing']",True,"HARNESSING WEBPAGE UIS FOR TEXT-RICH VISUAL
UNDERSTANDING

⋄Junpeng Liu, ♡Tianyue Ou∗, ¶Yifan Song∗, ♡Yuxiao Qu∗,
⋄Wai Lam, ♡Chenyan Xiong, ♣Wenhu Chen, ♡Graham Neubig, ♡Xiang Yue†
♡Carnegie Mellon University, ⋄The Chinese University of Hong Kong
¶Peking University, ♣University of Waterloo
jpliu@link.cuhk.edu.hk

xyue2@andrew.cmu.edu

ABSTRACT

Text-rich visual understanding—the ability to process environments where dense
textual content is integrated with visuals—is crucial for multimodal large lan-
guage models (MLLMs) to interact effectively with structured environments. To
enhance this capability, we propose synthesizing general multimodal instructions
from webpage UIs using text-based large language models (LLMs). Despite lack-
ing direct visual input, text-based LLMs are able to process structured text rep-
resentations from webpage accessibility trees. These instructions are then paired
with UI screenshots to train multimodal models. We introduce MultiUI, a dataset
containing 7.3 million samples from 1 million websites, covering diverse multi-
modal tasks and UI layouts. Models trained on MultiUI not only excel in web
UI tasks—achieving up to a 48% improvement on VisualWebBench and a 19.1%
boost in element accuracy on a web agent dataset Mind2Web—but also generalize
surprisingly well to non-web UI tasks and even to non-UI domains, such as doc-
ument understanding, OCR, and chart interpretation. These results highlight the
broad applicability of web UI data for advancing text-rich visual understanding
across various scenarios.","QUANTIZED SPIKE-DRIVEN TRANSFORMER

Xuerui Qiu1,2,3∗, Malu Zhang1†, Jieyuan Zhang1∗, Wenjie Wei1, Honglin Cao1,
Junsheng Guo4, Rui-Jie Zhu5, Yimeng Shan6, Yang Yang1, Haizhou Li7

1University of Electronic Science and Technology of China,
2Institute of Automation, Chinese Academy of Sciences,
3School of Future Technology, University of Chinese Academy of Sciences,
4China Agricultural University, 5University of California, Santa Cruz,
6Liaoning Technical University, 7Chinese University of Hong Kong (Shenzhen)

ABSTRACT

Spiking neural networks (SNNs) are emerging as a promising energy-efficient
alternative to traditional artificial neural networks (ANNs) due to their spike-driven
paradigm. However, recent research in the SNN domain has mainly focused
on enhancing accuracy by designing large-scale Transformer structures, which
typically rely on substantial computational resources, limiting their deployment on
resource-constrained devices. To overcome this challenge, we propose a quantized
spike-driven Transformer baseline (QSD-Transformer), which achieves reduced
resource demands by utilizing a low bit-width parameter. Regrettably, the QSD-
Transformer often suffers from severe performance degradation. In this paper, we
first conduct empirical analysis and find that the bimodal distribution of quantized
spike-driven self-attention (Q-SDSA) leads to spike information distortion (SID)
during quantization, causing significant performance degradation. To mitigate
this issue, we take inspiration from mutual information entropy and propose a
bi-level optimization strategy to rectify the information distribution in Q-SDSA.
Specifically, at the lower level, we introduce an information-enhanced LIF to rectify
the information distribution in Q-SDSA. At the upper level, we propose a fine-
grained distillation scheme for the QSD-Transformer to align the distribution in
Q-SDSA with that in the counterpart ANN. By integrating the bi-level optimization
strategy, the QSD-Transformer can attain enhanced energy efficiency without
sacrificing its high-performance advantage. We validate the QSD-Transformer on
various visual tasks, and experimental results indicate that our method achieves
state-of-the-art results in the SNN domain. For instance, when compared to the
prior SNN benchmark on ImageNet, the QSD-Transformer achieves 80.3% top-
1 accuracy, accompanied by significant reductions of 6.0× and 8.1× in power
consumption and model size, respectively. Code is available at Quantized Spike-
driven Transformer.",18
148,"<insight>Leveraging the structure of web interfaces provides a rich source for training systems to understand complex, text-heavy visuals, and parallel progress in creating highly efficient neural networks makes it feasible to deploy these advanced visual understanding capabilities in resource-limited settings.</insight>",-415.6875,-596.46875,-631.375,-477.484375,858.265625,-8.66015625,-12.4264326096,-13.1536455154,-9.947590827900001,"['HARNESSING WEBPAGE UIS FOR TEXT-RICH VISUAL\nUNDERSTANDING\n\n⋄Junpeng Liu, ♡Tianyue Ou∗, ¶Yifan Song∗, ♡Yuxiao Qu∗,\n⋄Wai Lam, ♡Chenyan Xiong, ♣Wenhu Chen, ♡Graham Neubig, ♡Xiang Yue†\n♡Carnegie Mellon University, ⋄The Chinese University of Hong Kong\n¶Peking University, ♣University of Waterloo\njpliu@link.cuhk.edu.hk\n\nxyue2@andrew.cmu.edu\n\nABSTRACT\n\nText-rich visual understanding—the ability to process environments where dense\ntextual content is integrated with visuals—is crucial for multimodal large lan-\nguage models (MLLMs) to interact effectively with structured environments. To\nenhance this capability, we propose synthesizing general multimodal instructions\nfrom webpage UIs using text-based large language models (LLMs). Despite lack-\ning direct visual input, text-based LLMs are able to process structured text rep-\nresentations from webpage accessibility trees. These instructions are then paired\nwith UI screenshots to train multimodal models. We introduce MultiUI, a dataset\ncontaining 7.3 million samples from 1 million websites, covering diverse multi-\nmodal tasks and UI layouts. Models trained on MultiUI not only excel in web\nUI tasks—achieving up to a 48% improvement on VisualWebBench and a 19.1%\nboost in element accuracy on a web agent dataset Mind2Web—but also generalize\nsurprisingly well to non-web UI tasks and even to non-UI domains, such as doc-\nument understanding, OCR, and chart interpretation. These results highlight the\nbroad applicability of web UI data for advancing text-rich visual understanding\nacross various scenarios.', 'QUANTIZED SPIKE-DRIVEN TRANSFORMER\n\nXuerui Qiu1,2,3∗, Malu Zhang1†, Jieyuan Zhang1∗, Wenjie Wei1, Honglin Cao1,\nJunsheng Guo4, Rui-Jie Zhu5, Yimeng Shan6, Yang Yang1, Haizhou Li7\n\n1University of Electronic Science and Technology of China,\n2Institute of Automation, Chinese Academy of Sciences,\n3School of Future Technology, University of Chinese Academy of Sciences,\n4China Agricultural University, 5University of California, Santa Cruz,\n6Liaoning Technical University, 7Chinese University of Hong Kong (Shenzhen)\n\nABSTRACT\n\nSpiking neural networks (SNNs) are emerging as a promising energy-efficient\nalternative to traditional artificial neural networks (ANNs) due to their spike-driven\nparadigm. However, recent research in the SNN domain has mainly focused\non enhancing accuracy by designing large-scale Transformer structures, which\ntypically rely on substantial computational resources, limiting their deployment on\nresource-constrained devices. To overcome this challenge, we propose a quantized\nspike-driven Transformer baseline (QSD-Transformer), which achieves reduced\nresource demands by utilizing a low bit-width parameter. Regrettably, the QSD-\nTransformer often suffers from severe performance degradation. In this paper, we\nfirst conduct empirical analysis and find that the bimodal distribution of quantized\nspike-driven self-attention (Q-SDSA) leads to spike information distortion (SID)\nduring quantization, causing significant performance degradation. To mitigate\nthis issue, we take inspiration from mutual information entropy and propose a\nbi-level optimization strategy to rectify the information distribution in Q-SDSA.\nSpecifically, at the lower level, we introduce an information-enhanced LIF to rectify\nthe information distribution in Q-SDSA. At the upper level, we propose a fine-\ngrained distillation scheme for the QSD-Transformer to align the distribution in\nQ-SDSA with that in the counterpart ANN. By integrating the bi-level optimization\nstrategy, the QSD-Transformer can attain enhanced energy efficiency without\nsacrificing its high-performance advantage. We validate the QSD-Transformer on\nvarious visual tasks, and experimental results indicate that our method achieves\nstate-of-the-art results in the SNN domain. For instance, when compared to the\nprior SNN benchmark on ImageNet, the QSD-Transformer achieves 80.3% top-\n1 accuracy, accompanied by significant reductions of 6.0× and 8.1× in power\nconsumption and model size, respectively. Code is available at Quantized Spike-\ndriven Transformer.']",380.78125,-96.703125,-215.6875,17.880534172100003,7.932943344200002,-2.014647483700001,-4.493489265399999,IIsTO4P3Ag,5J9B7Sb8rO,"['multimodal', 'instruction-tuning', 'large language model', 'spiking neural network+spike-driven+quantized spiking transformer+ neuromorphic computing']",True,"HARNESSING WEBPAGE UIS FOR TEXT-RICH VISUAL
UNDERSTANDING

⋄Junpeng Liu, ♡Tianyue Ou∗, ¶Yifan Song∗, ♡Yuxiao Qu∗,
⋄Wai Lam, ♡Chenyan Xiong, ♣Wenhu Chen, ♡Graham Neubig, ♡Xiang Yue†
♡Carnegie Mellon University, ⋄The Chinese University of Hong Kong
¶Peking University, ♣University of Waterloo
jpliu@link.cuhk.edu.hk

xyue2@andrew.cmu.edu

ABSTRACT

Text-rich visual understanding—the ability to process environments where dense
textual content is integrated with visuals—is crucial for multimodal large lan-
guage models (MLLMs) to interact effectively with structured environments. To
enhance this capability, we propose synthesizing general multimodal instructions
from webpage UIs using text-based large language models (LLMs). Despite lack-
ing direct visual input, text-based LLMs are able to process structured text rep-
resentations from webpage accessibility trees. These instructions are then paired
with UI screenshots to train multimodal models. We introduce MultiUI, a dataset
containing 7.3 million samples from 1 million websites, covering diverse multi-
modal tasks and UI layouts. Models trained on MultiUI not only excel in web
UI tasks—achieving up to a 48% improvement on VisualWebBench and a 19.1%
boost in element accuracy on a web agent dataset Mind2Web—but also generalize
surprisingly well to non-web UI tasks and even to non-UI domains, such as doc-
ument understanding, OCR, and chart interpretation. These results highlight the
broad applicability of web UI data for advancing text-rich visual understanding
across various scenarios.","QUANTIZED SPIKE-DRIVEN TRANSFORMER

Xuerui Qiu1,2,3∗, Malu Zhang1†, Jieyuan Zhang1∗, Wenjie Wei1, Honglin Cao1,
Junsheng Guo4, Rui-Jie Zhu5, Yimeng Shan6, Yang Yang1, Haizhou Li7

1University of Electronic Science and Technology of China,
2Institute of Automation, Chinese Academy of Sciences,
3School of Future Technology, University of Chinese Academy of Sciences,
4China Agricultural University, 5University of California, Santa Cruz,
6Liaoning Technical University, 7Chinese University of Hong Kong (Shenzhen)

ABSTRACT

Spiking neural networks (SNNs) are emerging as a promising energy-efficient
alternative to traditional artificial neural networks (ANNs) due to their spike-driven
paradigm. However, recent research in the SNN domain has mainly focused
on enhancing accuracy by designing large-scale Transformer structures, which
typically rely on substantial computational resources, limiting their deployment on
resource-constrained devices. To overcome this challenge, we propose a quantized
spike-driven Transformer baseline (QSD-Transformer), which achieves reduced
resource demands by utilizing a low bit-width parameter. Regrettably, the QSD-
Transformer often suffers from severe performance degradation. In this paper, we
first conduct empirical analysis and find that the bimodal distribution of quantized
spike-driven self-attention (Q-SDSA) leads to spike information distortion (SID)
during quantization, causing significant performance degradation. To mitigate
this issue, we take inspiration from mutual information entropy and propose a
bi-level optimization strategy to rectify the information distribution in Q-SDSA.
Specifically, at the lower level, we introduce an information-enhanced LIF to rectify
the information distribution in Q-SDSA. At the upper level, we propose a fine-
grained distillation scheme for the QSD-Transformer to align the distribution in
Q-SDSA with that in the counterpart ANN. By integrating the bi-level optimization
strategy, the QSD-Transformer can attain enhanced energy efficiency without
sacrificing its high-performance advantage. We validate the QSD-Transformer on
various visual tasks, and experimental results indicate that our method achieves
state-of-the-art results in the SNN domain. For instance, when compared to the
prior SNN benchmark on ImageNet, the QSD-Transformer achieves 80.3% top-
1 accuracy, accompanied by significant reductions of 6.0× and 8.1× in power
consumption and model size, respectively. Code is available at Quantized Spike-
driven Transformer.",19
1,"<insight>Effectively incorporating new information gained from generated data for targeted knowledge updates, while preserving unrelated knowledge, is enhanced by mechanisms that adaptively modify internal model representations based on the specific input being processed.</insight>",-354.75,-403.328125,-417.765625,-325.078125,665.390625,-9.3355264664,-10.6138982773,-10.9938325882,-8.5546875,"['TOWARDS A THEORETICAL UNDERSTANDING OF SYN-\nTHETIC DATA IN LLM POST-TRAINING:\nA REVERSE-BOTTLENECK PERSPECTIVE\n\nZeyu Gan, Yong Liu∗\nGaoling School of Artificial Intelligence\nRenmin University of China\nBeijing, China\n{zygan,liuyonggsai}@ruc.edu.cn\n\nABSTRACT\n\nSynthetic data has become a pivotal resource in post-training tasks for large lan-\nguage models (LLMs) due to the scarcity of high-quality, specific data. While\nvarious methods have been developed to generate synthetic data, there remains a\ndiscernible gap between the practical effects of synthetic data and our theoretical\ncomprehension. To address this challenge, we commence by presenting a detailed\nmodeling of the prevalent synthetic data generation process. Building upon this\nmodeling, we demonstrate that the generalization capability of the post-trained\nmodel is critically determined by the information gain derived from the generative\nmodel, as analyzed from a novel reverse-bottleneck perspective. Moreover, we in-\ntroduce the concept of Generalization Gain via Mutual Information (GGMI) and\nelucidate the relationship between generalization gain and information gain. This\nanalysis serves as a theoretical foundation for synthetic data generation and further\nhighlights its connection with the generalization capability of post-trained models,\noffering an understanding about the design of synthetic data generation techniques\nand the optimization of the post-training process. We open-source our code at\nhttps://github.com/ZyGan1999/Towards-a-Theoretical-U\nnderstanding-of-Synthetic-Data-in-LLM-Post-Training.', 'UNLOCKING EFFICIENT, SCALABLE, AND CONTIN-\nUAL KNOWLEDGE EDITING WITH BASIS-LEVEL REP-\nRESENTATION FINE-TUNING\n\nTianci Liu1, Ruirui Li2, Yunzhe Qi3, Hui Liu2, Xianfeng Tang2, Tianqi Zheng2, Qingyu Yin2,\nMonica Cheng2, Jun Huan4, Haoyu Wang5, Jing Gao1\n1Purdue University\n1{liu3351,jinggao}@purdue.edu\n\n2ruirul@amazon.com\n\n3UIUC 4AWS AI Lab\n\n5SUNY Albany\n\n2Amazon\n\n5hwang28@albany.edu\n\nABSTRACT\n\nLarge language models (LLMs) have achieved remarkable performance on vari-\nous natural language tasks. However, they are trained on static corpora and their\nknowledge can become outdated quickly in the fast-changing world. This moti-\nvates the development of knowledge editing methods designed to update certain\nknowledge in LLMs without changing unrelated others. To make selective edits,\nprevious efforts often sought to update a small amount of parameters in some spe-\ncific layer(s) of a LLM. Nonetheless, in challenging scenarios, they still fall short\nin making successful edits while preserving knowledge irrelevant to the updates\nsimultaneously, resulting in a notable editing-locality trade-off. In this work, we\nquestion if the trade-offs are caused by the fact that parameter-based updates have\na global effect, i.e., edited parameters affect all inputs indiscriminately. In light of\nthis, we explore the feasibility of representation fine-tuning, which applied some\nlinear update to a few representations in a learned subspace, for knowledge edit-\ning. While being effective to enhance an LLM’s general ability as demonstrated in\nthe previous work, we theoretically show that this linear update imposes a tension\nin editing-locality trade-off. Subsequently, BaFT is proposed to break the linear-\nity. BaFT computes a weight for each basis that spans a dimension of the subspace\nbased on the input representation. This input-dependent weighting mechanism al-\nlows BaFT to manage different types of knowledge in an adaptive way, thereby\nachieving a better editing-locality trade-off. Experiments on three LLMs with five\nediting benchmarks in diverse scenarios show the superiority of our method.']",340.3125,15.234375,-92.6875,17.5102796555,8.9555921555,0.40090465549999976,-2.4391450882,UxkznlcnHf,PITFO1ddeh,"['large language models', 'synthetic data', 'information bottleneck', 'knowledge editing', 'representation fine-tuning', 'large language models']",True,"TOWARDS A THEORETICAL UNDERSTANDING OF SYN-
THETIC DATA IN LLM POST-TRAINING:
A REVERSE-BOTTLENECK PERSPECTIVE

Zeyu Gan, Yong Liu∗
Gaoling School of Artificial Intelligence
Renmin University of China
Beijing, China
{zygan,liuyonggsai}@ruc.edu.cn

ABSTRACT

Synthetic data has become a pivotal resource in post-training tasks for large lan-
guage models (LLMs) due to the scarcity of high-quality, specific data. While
various methods have been developed to generate synthetic data, there remains a
discernible gap between the practical effects of synthetic data and our theoretical
comprehension. To address this challenge, we commence by presenting a detailed
modeling of the prevalent synthetic data generation process. Building upon this
modeling, we demonstrate that the generalization capability of the post-trained
model is critically determined by the information gain derived from the generative
model, as analyzed from a novel reverse-bottleneck perspective. Moreover, we in-
troduce the concept of Generalization Gain via Mutual Information (GGMI) and
elucidate the relationship between generalization gain and information gain. This
analysis serves as a theoretical foundation for synthetic data generation and further
highlights its connection with the generalization capability of post-trained models,
offering an understanding about the design of synthetic data generation techniques
and the optimization of the post-training process. We open-source our code at
https://github.com/ZyGan1999/Towards-a-Theoretical-U
nderstanding-of-Synthetic-Data-in-LLM-Post-Training.","UNLOCKING EFFICIENT, SCALABLE, AND CONTIN-
UAL KNOWLEDGE EDITING WITH BASIS-LEVEL REP-
RESENTATION FINE-TUNING

Tianci Liu1, Ruirui Li2, Yunzhe Qi3, Hui Liu2, Xianfeng Tang2, Tianqi Zheng2, Qingyu Yin2,
Monica Cheng2, Jun Huan4, Haoyu Wang5, Jing Gao1
1Purdue University
1{liu3351,jinggao}@purdue.edu

2ruirul@amazon.com

3UIUC 4AWS AI Lab

5SUNY Albany

2Amazon

5hwang28@albany.edu

ABSTRACT

Large language models (LLMs) have achieved remarkable performance on vari-
ous natural language tasks. However, they are trained on static corpora and their
knowledge can become outdated quickly in the fast-changing world. This moti-
vates the development of knowledge editing methods designed to update certain
knowledge in LLMs without changing unrelated others. To make selective edits,
previous efforts often sought to update a small amount of parameters in some spe-
cific layer(s) of a LLM. Nonetheless, in challenging scenarios, they still fall short
in making successful edits while preserving knowledge irrelevant to the updates
simultaneously, resulting in a notable editing-locality trade-off. In this work, we
question if the trade-offs are caused by the fact that parameter-based updates have
a global effect, i.e., edited parameters affect all inputs indiscriminately. In light of
this, we explore the feasibility of representation fine-tuning, which applied some
linear update to a few representations in a learned subspace, for knowledge edit-
ing. While being effective to enhance an LLM’s general ability as demonstrated in
the previous work, we theoretically show that this linear update imposes a tension
in editing-locality trade-off. Subsequently, BaFT is proposed to break the linear-
ity. BaFT computes a weight for each basis that spans a dimension of the subspace
based on the input representation. This input-dependent weighting mechanism al-
lows BaFT to manage different types of knowledge in an adaptive way, thereby
achieving a better editing-locality trade-off. Experiments on three LLMs with five
editing benchmarks in diverse scenarios show the superiority of our method.",20
7,"<insight>Current limitations in AI's ability to reason about the indirect consequences of actions may impede progress in understanding and optimizing complex, distributed learning systems where such indirect effects fundamentally determine overall performance and generalization.</insight>",-427.265625,-394.46875,-427.609375,-432.2109375,826.3359375,-10.955529213,-10.1145830154,-10.964343071,-11.0823316574,"['ACTIONREASONINGBENCH: REASONING ABOUT AC-\nTIONS WITH AND WITHOUT RAMIFICATION CON-\nSTRAINTS\n\nDivij Handa∗1, Pavel Dolin∗1, Shrinidhi Kumbhar∗ 1, Tran Cao Son2, Chitta Baral1\n1Arizona State University, 2New Mexico State University\n{dhanda,pdolin,skumbha4,chitta}@asu.edu, stran@nmsu.edu\n\nABSTRACT\n\nReasoning about Actions and Change (RAC) has historically played a pivotal role\nin solving foundational AI problems, such as the frame problem. It has driven\nadvancements in AI fields, such as non-monotonic and commonsense reasoning.\nRAC remains crucial for AI systems that operate in dynamic environments, en-\ngage in interactive scenarios, or rely on commonsense reasoning. Despite sub-\nstantial advances made by Large Language Models (LLMs) in various AI do-\nmains, their performance in RAC remains underexplored. To address this gap,\nwe introduce a new diagnostic benchmark, ACTIONREASONINGBENCH, which\nencompasses 8 domains and includes questions for up to 19 action sequences.\nThis benchmark rigorously evaluates LLMs across six key RAC dimensions: Flu-\nent Tracking, State Tracking, Action Executability, Effects of Actions, Numerical\nRAC, and Composite Questions. LLMs demonstrate average accuracy rates of\n73.55%, 65.63%, 58.73%, and 62.38% on the former four dimensions, which are\nfrequently discussed in RAC literature. However, the performance on the lat-\nter two dimensions, which introduce complex and novel reasoning questions, the\naverage performance of LLMs is lowered to 33.16% and 51.19%, respectively,\nreflecting a 17.9% performance decline. We also introduce new ramification con-\nstraints to capture the indirect effects of actions, providing deeper insights into\nRAC challenges. Our evaluation of state-of-the-art LLMs, including both open-\nsource and commercial models, reveals challenges across all RAC dimensions,\nparticularly in handling ramifications, with GPT-4o failing to solve any question\nand o1-preview achieving a score of only 18.4%.', 'UNDERSTANDING THE STABILITY-BASED GENERAL-\nIZATION OF PERSONALIZED FEDERATED LEARNING\n\nJie Tan4 Yifan Shi Li Shen1,2∗ Xiaochun Cao1\n\nYingqi Liu1,2 Qinglun Li3\n1School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, China\n2Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), China\n3College of Systems Engineering, National University of Defense Technology, China\n4Intelligent Game and Decision Lab, China\nliuyingqi1199@gmail.com; liqinglun@nudt.edu.cn; j.tanjie@outlook.com\nmathshenli@gmail.com; caoxiaochun@mail.sysu.edu.cn\n\nABSTRACT\n\nDespite great achievements in algorithm design for Personalized Federated Learn-\ning (PFL), research on the theoretical analysis of generalization is still in its early\nstages. Some theoretical results have investigated the generalization performance of\npersonalized models under the problem setting and hypothesis in convex conditions,\nwhich can not reflect the real iteration performance during non-convex training. To\nfurther understand the real performance from a generalization perspective, we pro-\npose the first algorithm-dependent generalization analysis with uniform stability for\nthe typical PFL method, Partial Model Personalization, on smooth and non-convex\nobjectives. Specifically, we decompose the generalization errors into aggregation\nerrors and fine-tuning errors, then creatively establish a generalization analysis\nframework corresponding to the gradient estimation process of the personalized\ntraining. This framework builds up the bridge among PFL, FL and Pure Local Train-\ning for personalized aims in heterogeneous scenarios, which clearly demonstrates\nthe effectiveness of PFL from the generalization perspective. Moreover, we demon-\nstrate the impact of trivial factors like learning steps, stepsizes and communication\ntopologies and obtain the excess risk analysis with optimization errors for PFL.\nPromising experiments on CIFAR datasets also corroborate our theoretical insights.\nOur code can be seen in https://github.com/YingqiLiu1999/Understanding-the-\nStability-based-Generalization-of-Personalized-Federated-Learning.']",394.125,-38.0859375,-33.140625,21.1881008148,10.1057691574,-0.9765625,-0.8497600556000009,NUD03NBDOE,znhZbonEoe,"['reasoning about actions and change (rac)', 'benchmark', 'large language models (llms)', 'stability analysis+generalization gap+excess risk+personalized federated learning']",True,"ACTIONREASONINGBENCH: REASONING ABOUT AC-
TIONS WITH AND WITHOUT RAMIFICATION CON-
STRAINTS

Divij Handa∗1, Pavel Dolin∗1, Shrinidhi Kumbhar∗ 1, Tran Cao Son2, Chitta Baral1
1Arizona State University, 2New Mexico State University
{dhanda,pdolin,skumbha4,chitta}@asu.edu, stran@nmsu.edu

ABSTRACT

Reasoning about Actions and Change (RAC) has historically played a pivotal role
in solving foundational AI problems, such as the frame problem. It has driven
advancements in AI fields, such as non-monotonic and commonsense reasoning.
RAC remains crucial for AI systems that operate in dynamic environments, en-
gage in interactive scenarios, or rely on commonsense reasoning. Despite sub-
stantial advances made by Large Language Models (LLMs) in various AI do-
mains, their performance in RAC remains underexplored. To address this gap,
we introduce a new diagnostic benchmark, ACTIONREASONINGBENCH, which
encompasses 8 domains and includes questions for up to 19 action sequences.
This benchmark rigorously evaluates LLMs across six key RAC dimensions: Flu-
ent Tracking, State Tracking, Action Executability, Effects of Actions, Numerical
RAC, and Composite Questions. LLMs demonstrate average accuracy rates of
73.55%, 65.63%, 58.73%, and 62.38% on the former four dimensions, which are
frequently discussed in RAC literature. However, the performance on the lat-
ter two dimensions, which introduce complex and novel reasoning questions, the
average performance of LLMs is lowered to 33.16% and 51.19%, respectively,
reflecting a 17.9% performance decline. We also introduce new ramification con-
straints to capture the indirect effects of actions, providing deeper insights into
RAC challenges. Our evaluation of state-of-the-art LLMs, including both open-
source and commercial models, reveals challenges across all RAC dimensions,
particularly in handling ramifications, with GPT-4o failing to solve any question
and o1-preview achieving a score of only 18.4%.","UNDERSTANDING THE STABILITY-BASED GENERAL-
IZATION OF PERSONALIZED FEDERATED LEARNING

Jie Tan4 Yifan Shi Li Shen1,2∗ Xiaochun Cao1

Yingqi Liu1,2 Qinglun Li3
1School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, China
2Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), China
3College of Systems Engineering, National University of Defense Technology, China
4Intelligent Game and Decision Lab, China
liuyingqi1199@gmail.com; liqinglun@nudt.edu.cn; j.tanjie@outlook.com
mathshenli@gmail.com; caoxiaochun@mail.sysu.edu.cn

ABSTRACT

Despite great achievements in algorithm design for Personalized Federated Learn-
ing (PFL), research on the theoretical analysis of generalization is still in its early
stages. Some theoretical results have investigated the generalization performance of
personalized models under the problem setting and hypothesis in convex conditions,
which can not reflect the real iteration performance during non-convex training. To
further understand the real performance from a generalization perspective, we pro-
pose the first algorithm-dependent generalization analysis with uniform stability for
the typical PFL method, Partial Model Personalization, on smooth and non-convex
objectives. Specifically, we decompose the generalization errors into aggregation
errors and fine-tuning errors, then creatively establish a generalization analysis
framework corresponding to the gradient estimation process of the personalized
training. This framework builds up the bridge among PFL, FL and Pure Local Train-
ing for personalized aims in heterogeneous scenarios, which clearly demonstrates
the effectiveness of PFL from the generalization perspective. Moreover, we demon-
strate the impact of trivial factors like learning steps, stepsizes and communication
topologies and obtain the excess risk analysis with optimization errors for PFL.
Promising experiments on CIFAR datasets also corroborate our theoretical insights.
Our code can be seen in https://github.com/YingqiLiu1999/Understanding-the-
Stability-based-Generalization-of-Personalized-Federated-Learning.",21
20,"<insight>The core architecture and adaptation mechanisms of large language models can be effectively leveraged either to achieve deep, specialized expertise in complex symbolic reasoning within their native text domain or, alternatively, to bridge into entirely new non-linguistic data modalities and perform diverse tasks using that distinct form of information.</insight>",-656.40625,-658.671875,-664.15625,-665.09375,1316.015625,-11.1255292892,-11.1639299393,-11.2568855286,-11.27277565,"['OPENMATHINSTRUCT-2: ACCELERATING AI FOR\nMATH WITH MASSIVE OPEN-SOURCE INSTRUCTION\nDATA\n\nShubham Toshniwal1 Wei Du1\nBranislav Kisacanin1,2,3 Alexan Ayrapetyan1\n{stoshniwal,wedu,imoshkov,bkisacanin,aayrapetyan,igitman}@nvidia.com\n\nIvan Moshkov1\n\nIgor Gitman1\n\n1NVIDIA\n2Institute for AI R&D of Serbia\n3Faculty of Technical Sciences, University of Novi Sad\n\nABSTRACT\n\nMathematical reasoning continues to be a critical challenge in large language\nmodel (LLM) development with significant interest. However, most of the cutting-\nedge progress in mathematical reasoning with LLMs has become closed-source\ndue to lack of access to training data. This lack of data access limits researchers\nfrom understanding the impact of different choices for synthesizing and utilizing\nthe data. With the goal of creating a high-quality finetuning (SFT) dataset for\nmath reasoning, we conduct careful ablation experiments on data synthesis us-\ning the recently released Llama3.1 family of models. Our experiments show\nthat: (a) solution format matters, with excessively verbose solutions proving detri-\nmental to SFT performance, (b) data generated by a strong teacher outperforms\nequally-sized data generated by a weak student model, (c) SFT is robust to low-\nquality solutions, allowing for imprecise data filtering, and (d) question diversity\nis crucial for achieving data scaling gains. Based on these insights, we create\nthe OpenMathInstruct-2 dataset which consists of 14M question-solution pairs (≈\n600K unique questions), making it nearly eight times larger than the previous largest\nopen-source math reasoning dataset. Finetuning the Llama-3.1-8B-Base us-\ning OpenMathInstruct-2 outperforms Llama3.1-8B-Instruct on MATH by\nan absolute 15.9% (51.9% → 67.8%). Finally, to accelerate the open-source efforts,\nwe release the code, the finetuned models, and the OpenMathInstruct-2 dataset\nunder a commercially permissive license.1', 'NEUROLM: A UNIVERSAL MULTI-TASK FOUNDATION\nMODEL FOR BRIDGING THE GAP BETWEEN LAN-\nGUAGE AND EEG SIGNALS\n\nWei-Bang Jiang1∗, Yansen Wang2, Bao-Liang Lu1, Dongsheng Li2\n1Shanghai Jiao Tong University\n2Microsoft Research Asia\n{935963004,bllu}@sjtu.edu.cn,{yansenwang,dongsli}@microsoft.com\nhttps://github.com/935963004/NeuroLM\n\nABSTRACT\n\nRecent advancements for large-scale pre-training with neural signals such as elec-\ntroencephalogram (EEG) have shown promising results, significantly boosting\nthe development of brain-computer interfaces (BCIs) and healthcare. However,\nthese pre-trained models often require full fine-tuning on each downstream task\nto achieve substantial improvements, limiting their versatility and usability, and\nleading to considerable resource wastage. To tackle these challenges, we propose\nNeuroLM, the first multi-task foundation model that leverages the capabilities\nof Large Language Models (LLMs) by regarding EEG signals as a foreign lan-\nguage, endowing the model with multi-task learning and inference capabilities.\nOur approach begins with learning a text-aligned neural tokenizer through vector-\nquantized temporal-frequency prediction, which encodes EEG signals into dis-\ncrete neural tokens. These EEG tokens, generated by the frozen vector-quantized\n(VQ) encoder, are then fed into an LLM that learns causal EEG information via\nmulti-channel autoregression. Consequently, NeuroLM can understand both EEG\nand language modalities. Finally, multi-task instruction tuning adapts NeuroLM\nto various downstream tasks. We are the first to demonstrate that, by specific\nincorporation with LLMs, NeuroLM unifies diverse EEG tasks within a single\nmodel through instruction tuning. The largest variant NeuroLM-XL has record-\nbreaking 1.7B parameters for EEG signal processing, and is pre-trained on a large-\nscale corpus comprising approximately 25,000-hour EEG data. When evaluated\non six diverse downstream datasets, NeuroLM showcases the huge potential of\nthis multi-task learning paradigm.']",650.921875,-14.171875,-7.75,22.3053493499,11.0325736999,-0.24020195010000123,-0.13135623940000052,mTCbq2QssD,Io9yFt7XH7,"['math reasoning', 'synthetic data', 'eeg', 'large language model', 'multi-task learning']",True,"OPENMATHINSTRUCT-2: ACCELERATING AI FOR
MATH WITH MASSIVE OPEN-SOURCE INSTRUCTION
DATA

Shubham Toshniwal1 Wei Du1
Branislav Kisacanin1,2,3 Alexan Ayrapetyan1
{stoshniwal,wedu,imoshkov,bkisacanin,aayrapetyan,igitman}@nvidia.com

Ivan Moshkov1

Igor Gitman1

1NVIDIA
2Institute for AI R&D of Serbia
3Faculty of Technical Sciences, University of Novi Sad

ABSTRACT

Mathematical reasoning continues to be a critical challenge in large language
model (LLM) development with significant interest. However, most of the cutting-
edge progress in mathematical reasoning with LLMs has become closed-source
due to lack of access to training data. This lack of data access limits researchers
from understanding the impact of different choices for synthesizing and utilizing
the data. With the goal of creating a high-quality finetuning (SFT) dataset for
math reasoning, we conduct careful ablation experiments on data synthesis us-
ing the recently released Llama3.1 family of models. Our experiments show
that: (a) solution format matters, with excessively verbose solutions proving detri-
mental to SFT performance, (b) data generated by a strong teacher outperforms
equally-sized data generated by a weak student model, (c) SFT is robust to low-
quality solutions, allowing for imprecise data filtering, and (d) question diversity
is crucial for achieving data scaling gains. Based on these insights, we create
the OpenMathInstruct-2 dataset which consists of 14M question-solution pairs (≈
600K unique questions), making it nearly eight times larger than the previous largest
open-source math reasoning dataset. Finetuning the Llama-3.1-8B-Base us-
ing OpenMathInstruct-2 outperforms Llama3.1-8B-Instruct on MATH by
an absolute 15.9% (51.9% → 67.8%). Finally, to accelerate the open-source efforts,
we release the code, the finetuned models, and the OpenMathInstruct-2 dataset
under a commercially permissive license.1","NEUROLM: A UNIVERSAL MULTI-TASK FOUNDATION
MODEL FOR BRIDGING THE GAP BETWEEN LAN-
GUAGE AND EEG SIGNALS

Wei-Bang Jiang1∗, Yansen Wang2, Bao-Liang Lu1, Dongsheng Li2
1Shanghai Jiao Tong University
2Microsoft Research Asia
{935963004,bllu}@sjtu.edu.cn,{yansenwang,dongsli}@microsoft.com
https://github.com/935963004/NeuroLM

ABSTRACT

Recent advancements for large-scale pre-training with neural signals such as elec-
troencephalogram (EEG) have shown promising results, significantly boosting
the development of brain-computer interfaces (BCIs) and healthcare. However,
these pre-trained models often require full fine-tuning on each downstream task
to achieve substantial improvements, limiting their versatility and usability, and
leading to considerable resource wastage. To tackle these challenges, we propose
NeuroLM, the first multi-task foundation model that leverages the capabilities
of Large Language Models (LLMs) by regarding EEG signals as a foreign lan-
guage, endowing the model with multi-task learning and inference capabilities.
Our approach begins with learning a text-aligned neural tokenizer through vector-
quantized temporal-frequency prediction, which encodes EEG signals into dis-
crete neural tokens. These EEG tokens, generated by the frozen vector-quantized
(VQ) encoder, are then fed into an LLM that learns causal EEG information via
multi-channel autoregression. Consequently, NeuroLM can understand both EEG
and language modalities. Finally, multi-task instruction tuning adapts NeuroLM
to various downstream tasks. We are the first to demonstrate that, by specific
incorporation with LLMs, NeuroLM unifies diverse EEG tasks within a single
model through instruction tuning. The largest variant NeuroLM-XL has record-
breaking 1.7B parameters for EEG signal processing, and is pre-trained on a large-
scale corpus comprising approximately 25,000-hour EEG data. When evaluated
on six diverse downstream datasets, NeuroLM showcases the huge potential of
this multi-task learning paradigm.",22
98,"<insight>Architectural simplification can yield effective models for representation learning by focusing on core mechanisms, yet even highly complex contemporary AI systems struggle significantly when reasoning about the indirect consequences or ramifications of actions.</insight>",-278.453125,-418.0,-342.25,-414.078125,768.28125,-7.3277139664,-11.0,-9.0065793991,-10.8967924118,"['DECONSTRUCTING DENOISING DIFFUSION MODELS\nFOR SELF-SUPERVISED LEARNING\n\nXinlei Chen1\n\nZhuang Liu1,2\n\nSaining Xie3\n\nKaiming He1,4\n\n1FAIR, Meta\n\n2Princeton University\n\n3New York University\n\n4MIT\n\nABSTRACT\n\nIn this study, we examine the representation learning abilities of Denoising Dif-\nfusion Models (DDM) that were originally purposed for image generation. Our\nphilosophy is to deconstruct a DDM, gradually transforming it into a classical\nDenoising Autoencoder (DAE). This deconstructive process allows us to explore\nhow various components of modern DDMs influence self-supervised representation\nlearning. We observe that only a very few modern components are critical for\nlearning good representations, while many others are nonessential. Our study\nultimately arrives at an approach that is highly simplified and to a large extent\nresembles a classical DAE. We hope our study will rekindle interest in a family of\nclassical methods within the realm of modern self-supervised learning.', 'ACTIONREASONINGBENCH: REASONING ABOUT AC-\nTIONS WITH AND WITHOUT RAMIFICATION CON-\nSTRAINTS\n\nDivij Handa∗1, Pavel Dolin∗1, Shrinidhi Kumbhar∗ 1, Tran Cao Son2, Chitta Baral1\n1Arizona State University, 2New Mexico State University\n{dhanda,pdolin,skumbha4,chitta}@asu.edu, stran@nmsu.edu\n\nABSTRACT\n\nReasoning about Actions and Change (RAC) has historically played a pivotal role\nin solving foundational AI problems, such as the frame problem. It has driven\nadvancements in AI fields, such as non-monotonic and commonsense reasoning.\nRAC remains crucial for AI systems that operate in dynamic environments, en-\ngage in interactive scenarios, or rely on commonsense reasoning. Despite sub-\nstantial advances made by Large Language Models (LLMs) in various AI do-\nmains, their performance in RAC remains underexplored. To address this gap,\nwe introduce a new diagnostic benchmark, ACTIONREASONINGBENCH, which\nencompasses 8 domains and includes questions for up to 19 action sequences.\nThis benchmark rigorously evaluates LLMs across six key RAC dimensions: Flu-\nent Tracking, State Tracking, Action Executability, Effects of Actions, Numerical\nRAC, and Composite Questions. LLMs demonstrate average accuracy rates of\n73.55%, 65.63%, 58.73%, and 62.38% on the former four dimensions, which are\nfrequently discussed in RAC literature. However, the performance on the lat-\nter two dimensions, which introduce complex and novel reasoning questions, the\naverage performance of LLMs is lowered to 33.16% and 51.19%, respectively,\nreflecting a 17.9% performance decline. We also introduce new ramification con-\nstraints to capture the indirect effects of actions, providing deeper insights into\nRAC challenges. Our evaluation of state-of-the-art LLMs, including both open-\nsource and commercial models, reveals challenges across all RAC dimensions,\nparticularly in handling ramifications, with GPT-4o failing to solve any question\nand o1-preview achieving a score of only 18.4%.']",354.203125,-59.875,-63.796875,20.2179269791,9.3211345673,-1.5756578444999967,-1.6788654326999994,9oMB6wnFYM,NUD03NBDOE,"['denoising diffusion models', 'denoising autoencoder', 'self-supervised learning', 'reasoning about actions and change (rac)', 'benchmark', 'large language models (llms)']",True,"DECONSTRUCTING DENOISING DIFFUSION MODELS
FOR SELF-SUPERVISED LEARNING

Xinlei Chen1

Zhuang Liu1,2

Saining Xie3

Kaiming He1,4

1FAIR, Meta

2Princeton University

3New York University

4MIT

ABSTRACT

In this study, we examine the representation learning abilities of Denoising Dif-
fusion Models (DDM) that were originally purposed for image generation. Our
philosophy is to deconstruct a DDM, gradually transforming it into a classical
Denoising Autoencoder (DAE). This deconstructive process allows us to explore
how various components of modern DDMs influence self-supervised representation
learning. We observe that only a very few modern components are critical for
learning good representations, while many others are nonessential. Our study
ultimately arrives at an approach that is highly simplified and to a large extent
resembles a classical DAE. We hope our study will rekindle interest in a family of
classical methods within the realm of modern self-supervised learning.","ACTIONREASONINGBENCH: REASONING ABOUT AC-
TIONS WITH AND WITHOUT RAMIFICATION CON-
STRAINTS

Divij Handa∗1, Pavel Dolin∗1, Shrinidhi Kumbhar∗ 1, Tran Cao Son2, Chitta Baral1
1Arizona State University, 2New Mexico State University
{dhanda,pdolin,skumbha4,chitta}@asu.edu, stran@nmsu.edu

ABSTRACT

Reasoning about Actions and Change (RAC) has historically played a pivotal role
in solving foundational AI problems, such as the frame problem. It has driven
advancements in AI fields, such as non-monotonic and commonsense reasoning.
RAC remains crucial for AI systems that operate in dynamic environments, en-
gage in interactive scenarios, or rely on commonsense reasoning. Despite sub-
stantial advances made by Large Language Models (LLMs) in various AI do-
mains, their performance in RAC remains underexplored. To address this gap,
we introduce a new diagnostic benchmark, ACTIONREASONINGBENCH, which
encompasses 8 domains and includes questions for up to 19 action sequences.
This benchmark rigorously evaluates LLMs across six key RAC dimensions: Flu-
ent Tracking, State Tracking, Action Executability, Effects of Actions, Numerical
RAC, and Composite Questions. LLMs demonstrate average accuracy rates of
73.55%, 65.63%, 58.73%, and 62.38% on the former four dimensions, which are
frequently discussed in RAC literature. However, the performance on the lat-
ter two dimensions, which introduce complex and novel reasoning questions, the
average performance of LLMs is lowered to 33.16% and 51.19%, respectively,
reflecting a 17.9% performance decline. We also introduce new ramification con-
straints to capture the indirect effects of actions, providing deeper insights into
RAC challenges. Our evaluation of state-of-the-art LLMs, including both open-
source and commercial models, reveals challenges across all RAC dimensions,
particularly in handling ramifications, with GPT-4o failing to solve any question
and o1-preview achieving a score of only 18.4%.",23
114,"<insight>Improving model performance can be achieved by refining the training process itself, either through better alignment between training and inference conditions or by adjusting training targets to foster more stable and effective internal model states.</insight>",-373.484375,-388.375,-386.734375,-381.53125,756.65625,-9.8285360336,-10.2203950882,-10.1772203445,-10.0402956009,"['LEARNING HARMONIZED REPRESENTATIONS\nSPECULATIVE SAMPLING\n\nFOR\n\nLefan Zhang, Xiaodan Wang, Yanhua Huang∗, Ruiwen Xu\nXiaohongshu Inc.\nShanghai, China\n{lefan,xiaodan2,yanhuahuang,ruiwenxu}@xiaohongshu.com\n\nABSTRACT\n\nSpeculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage tar-\nget LLM’s contextual information, such as hidden states and KV cache, have\nshown significant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing specula-\ntive sampling methods.\nIn this work, we propose a solution named HArmo-\nnized Speculative Sampling (HASS) that learns harmonized representations to\naddress these issues. HASS accelerates the decoding stage without adding infer-\nence overhead through harmonized objective distillation and harmonized context\nalignment. Experiments on four LLaMA models demonstrate that HASS achieves\n2.81x-4.05x wall-clock time speedup ratio averaging across three datasets, sur-\npassing EAGLE-2 by 8%-20%. The code is available at https://github.\ncom/HArmonizedSS/HASS.', 'RETHINKING CLASSIFIER RE-TRAINING IN LONG-\nTAILED RECOGNITION:\nLABEL OVER-SMOOTH CAN BALANCE\n\nSiyu Sun1,2#, Han Lu1,2#, Jiangtong Li3, Yichen Xie4, Tianjiao Li2\nXiaokang Yang1, Liqing Zhang1, Junchi Yan1∗\n1Department of CSE & MoE Key Lab of AI, Shanghai Jiao Tong University\n2Bilibili Inc\n{sunsiyu, sjtu luhan, xkyang, yanjunchi}@sjtu.edu.cn\njiangtongli@tongji.edu.cn, zhang-lq@cs.sjtu.edu.cn\nyichen xie@berkeley.edu, litianjiao01@bilibili.com\nhttps://github.com/Thinklab-SJTU/LOS\nCode:\n\n3Tongji University\n\n4UC Berkeley\n\nABSTRACT\n\nIn the field of long-tailed recognition, the Decoupled Training paradigm has shown\nexceptional promise by dividing training into two stages: representation learning\nand classifier re-training. While previous work has tried to improve both stages\nsimultaneously, this complicates isolating the effect of classifier re-training. Recent\nstudies reveal that simple regularization can produce strong feature representations,\nhighlighting the need to reassess classifier re-training methods. In this study, we\nrevisit classifier re-training methods based on a unified feature representation and\nre-evaluate their performances. We propose two new metrics, Logits Magnitude\nand Regularized Standard Deviation, to compare the differences and similarities\nbetween various methods. Using these two newly proposed metrics, we demonstrate\nthat when the Logits Magnitude across classes is nearly balanced, further reducing\nits overall value can effectively decrease errors and disturbances during training,\nleading to better model performance. Based on our analysis using these metrics,\nwe observe that adjusting the logits could improve model performance, leading\nus to develop a simple label over-smoothing approach to adjust the logits without\nrequiring prior knowledge of class distribution. This method softens the original\none-hot labels by assigning a probability slightly higher than 1\nK to the true class\nand slightly lower than 1\nK to the other classes, where K is the number of classes.\nOur method achieves state-of-the-art performance on various imbalanced datasets,\nincluding CIFAR100-LT, ImageNet-LT, and iNaturalist2018.']",375.125,-6.40625,-13.25,19.9120063782,9.8717107773,-0.1685848236000016,-0.3486843108999995,T9u56s7mbk,OeKp3AdiVO,"['speculative sampling', 'large language model', 'long-tailed recognition and decoupled training']",True,"LEARNING HARMONIZED REPRESENTATIONS
SPECULATIVE SAMPLING

FOR

Lefan Zhang, Xiaodan Wang, Yanhua Huang∗, Ruiwen Xu
Xiaohongshu Inc.
Shanghai, China
{lefan,xiaodan2,yanhuahuang,ruiwenxu}@xiaohongshu.com

ABSTRACT

Speculative sampling is a promising approach to accelerate the decoding stage
for Large Language Models (LLMs). Recent advancements that leverage tar-
get LLM’s contextual information, such as hidden states and KV cache, have
shown significant practical improvements. However, these approaches suffer from
inconsistent context between training and decoding. We also observe another
discrepancy between the training and decoding objectives in existing specula-
tive sampling methods.
In this work, we propose a solution named HArmo-
nized Speculative Sampling (HASS) that learns harmonized representations to
address these issues. HASS accelerates the decoding stage without adding infer-
ence overhead through harmonized objective distillation and harmonized context
alignment. Experiments on four LLaMA models demonstrate that HASS achieves
2.81x-4.05x wall-clock time speedup ratio averaging across three datasets, sur-
passing EAGLE-2 by 8%-20%. The code is available at https://github.
com/HArmonizedSS/HASS.","RETHINKING CLASSIFIER RE-TRAINING IN LONG-
TAILED RECOGNITION:
LABEL OVER-SMOOTH CAN BALANCE

Siyu Sun1,2#, Han Lu1,2#, Jiangtong Li3, Yichen Xie4, Tianjiao Li2
Xiaokang Yang1, Liqing Zhang1, Junchi Yan1∗
1Department of CSE & MoE Key Lab of AI, Shanghai Jiao Tong University
2Bilibili Inc
{sunsiyu, sjtu luhan, xkyang, yanjunchi}@sjtu.edu.cn
jiangtongli@tongji.edu.cn, zhang-lq@cs.sjtu.edu.cn
yichen xie@berkeley.edu, litianjiao01@bilibili.com
https://github.com/Thinklab-SJTU/LOS
Code:

3Tongji University

4UC Berkeley

ABSTRACT

In the field of long-tailed recognition, the Decoupled Training paradigm has shown
exceptional promise by dividing training into two stages: representation learning
and classifier re-training. While previous work has tried to improve both stages
simultaneously, this complicates isolating the effect of classifier re-training. Recent
studies reveal that simple regularization can produce strong feature representations,
highlighting the need to reassess classifier re-training methods. In this study, we
revisit classifier re-training methods based on a unified feature representation and
re-evaluate their performances. We propose two new metrics, Logits Magnitude
and Regularized Standard Deviation, to compare the differences and similarities
between various methods. Using these two newly proposed metrics, we demonstrate
that when the Logits Magnitude across classes is nearly balanced, further reducing
its overall value can effectively decrease errors and disturbances during training,
leading to better model performance. Based on our analysis using these metrics,
we observe that adjusting the logits could improve model performance, leading
us to develop a simple label over-smoothing approach to adjust the logits without
requiring prior knowledge of class distribution. This method softens the original
one-hot labels by assigning a probability slightly higher than 1
K to the true class
and slightly lower than 1
K to the other classes, where K is the number of classes.
Our method achieves state-of-the-art performance on various imbalanced datasets,
including CIFAR100-LT, ImageNet-LT, and iNaturalist2018.",24
114,"<insight>Significant performance improvements in machine learning systems, encompassing both efficiency (like inference speed) and effectiveness (like accuracy on imbalanced data), can be achieved by actively managing and reducing inconsistencies or imbalances, either between distinct operational phases (such as training and inference) or within specific components by adjusting their internal states or outputs based on targeted analysis.</insight>",-499.3125,-951.25,-964.4375,-964.9375,1451.0625,-7.236413002,-13.7862319946,-13.9773550034,-13.9846010208,"['LEARNING HARMONIZED REPRESENTATIONS\nSPECULATIVE SAMPLING\n\nFOR\n\nLefan Zhang, Xiaodan Wang, Yanhua Huang∗, Ruiwen Xu\nXiaohongshu Inc.\nShanghai, China\n{lefan,xiaodan2,yanhuahuang,ruiwenxu}@xiaohongshu.com\n\nABSTRACT\n\nSpeculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage tar-\nget LLM’s contextual information, such as hidden states and KV cache, have\nshown significant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing specula-\ntive sampling methods.\nIn this work, we propose a solution named HArmo-\nnized Speculative Sampling (HASS) that learns harmonized representations to\naddress these issues. HASS accelerates the decoding stage without adding infer-\nence overhead through harmonized objective distillation and harmonized context\nalignment. Experiments on four LLaMA models demonstrate that HASS achieves\n2.81x-4.05x wall-clock time speedup ratio averaging across three datasets, sur-\npassing EAGLE-2 by 8%-20%. The code is available at https://github.\ncom/HArmonizedSS/HASS.', 'RETHINKING CLASSIFIER RE-TRAINING IN LONG-\nTAILED RECOGNITION:\nLABEL OVER-SMOOTH CAN BALANCE\n\nSiyu Sun1,2#, Han Lu1,2#, Jiangtong Li3, Yichen Xie4, Tianjiao Li2\nXiaokang Yang1, Liqing Zhang1, Junchi Yan1∗\n1Department of CSE & MoE Key Lab of AI, Shanghai Jiao Tong University\n2Bilibili Inc\n{sunsiyu, sjtu luhan, xkyang, yanjunchi}@sjtu.edu.cn\njiangtongli@tongji.edu.cn, zhang-lq@cs.sjtu.edu.cn\nyichen xie@berkeley.edu, litianjiao01@bilibili.com\nhttps://github.com/Thinklab-SJTU/LOS\nCode:\n\n3Tongji University\n\n4UC Berkeley\n\nABSTRACT\n\nIn the field of long-tailed recognition, the Decoupled Training paradigm has shown\nexceptional promise by dividing training into two stages: representation learning\nand classifier re-training. While previous work has tried to improve both stages\nsimultaneously, this complicates isolating the effect of classifier re-training. Recent\nstudies reveal that simple regularization can produce strong feature representations,\nhighlighting the need to reassess classifier re-training methods. In this study, we\nrevisit classifier re-training methods based on a unified feature representation and\nre-evaluate their performances. We propose two new metrics, Logits Magnitude\nand Regularized Standard Deviation, to compare the differences and similarities\nbetween various methods. Using these two newly proposed metrics, we demonstrate\nthat when the Logits Magnitude across classes is nearly balanced, further reducing\nits overall value can effectively decrease errors and disturbances during training,\nleading to better model performance. Based on our analysis using these metrics,\nwe observe that adjusting the logits could improve model performance, leading\nus to develop a simple label over-smoothing approach to adjust the logits without\nrequiring prior knowledge of class distribution. This method softens the original\none-hot labels by assigning a probability slightly higher than 1\nK to the true class\nand slightly lower than 1\nK to the other classes, where K is the number of classes.\nOur method achieves state-of-the-art performance on various imbalanced datasets,\nincluding CIFAR100-LT, ImageNet-LT, and iNaturalist2018.']",486.125,-478.8125,-465.125,21.029891014,7.0452899932,-6.9393110276000005,-6.7409420014,T9u56s7mbk,OeKp3AdiVO,"['speculative sampling', 'large language model', 'long-tailed recognition and decoupled training']",True,"LEARNING HARMONIZED REPRESENTATIONS
SPECULATIVE SAMPLING

FOR

Lefan Zhang, Xiaodan Wang, Yanhua Huang∗, Ruiwen Xu
Xiaohongshu Inc.
Shanghai, China
{lefan,xiaodan2,yanhuahuang,ruiwenxu}@xiaohongshu.com

ABSTRACT

Speculative sampling is a promising approach to accelerate the decoding stage
for Large Language Models (LLMs). Recent advancements that leverage tar-
get LLM’s contextual information, such as hidden states and KV cache, have
shown significant practical improvements. However, these approaches suffer from
inconsistent context between training and decoding. We also observe another
discrepancy between the training and decoding objectives in existing specula-
tive sampling methods.
In this work, we propose a solution named HArmo-
nized Speculative Sampling (HASS) that learns harmonized representations to
address these issues. HASS accelerates the decoding stage without adding infer-
ence overhead through harmonized objective distillation and harmonized context
alignment. Experiments on four LLaMA models demonstrate that HASS achieves
2.81x-4.05x wall-clock time speedup ratio averaging across three datasets, sur-
passing EAGLE-2 by 8%-20%. The code is available at https://github.
com/HArmonizedSS/HASS.","RETHINKING CLASSIFIER RE-TRAINING IN LONG-
TAILED RECOGNITION:
LABEL OVER-SMOOTH CAN BALANCE

Siyu Sun1,2#, Han Lu1,2#, Jiangtong Li3, Yichen Xie4, Tianjiao Li2
Xiaokang Yang1, Liqing Zhang1, Junchi Yan1∗
1Department of CSE & MoE Key Lab of AI, Shanghai Jiao Tong University
2Bilibili Inc
{sunsiyu, sjtu luhan, xkyang, yanjunchi}@sjtu.edu.cn
jiangtongli@tongji.edu.cn, zhang-lq@cs.sjtu.edu.cn
yichen xie@berkeley.edu, litianjiao01@bilibili.com
https://github.com/Thinklab-SJTU/LOS
Code:

3Tongji University

4UC Berkeley

ABSTRACT

In the field of long-tailed recognition, the Decoupled Training paradigm has shown
exceptional promise by dividing training into two stages: representation learning
and classifier re-training. While previous work has tried to improve both stages
simultaneously, this complicates isolating the effect of classifier re-training. Recent
studies reveal that simple regularization can produce strong feature representations,
highlighting the need to reassess classifier re-training methods. In this study, we
revisit classifier re-training methods based on a unified feature representation and
re-evaluate their performances. We propose two new metrics, Logits Magnitude
and Regularized Standard Deviation, to compare the differences and similarities
between various methods. Using these two newly proposed metrics, we demonstrate
that when the Logits Magnitude across classes is nearly balanced, further reducing
its overall value can effectively decrease errors and disturbances during training,
leading to better model performance. Based on our analysis using these metrics,
we observe that adjusting the logits could improve model performance, leading
us to develop a simple label over-smoothing approach to adjust the logits without
requiring prior knowledge of class distribution. This method softens the original
one-hot labels by assigning a probability slightly higher than 1
K to the true class
and slightly lower than 1
K to the other classes, where K is the number of classes.
Our method achieves state-of-the-art performance on various imbalanced datasets,
including CIFAR100-LT, ImageNet-LT, and iNaturalist2018.",25
127,<insight>Incorporating representations or generation processes that mirror the underlying structure of a problem domain—be it the geometric nature of physical data or the logical flow of abstract reasoning—enhances the performance and generalization capabilities of complex computational models applied to that domain.</insight>,-335.375,-470.96875,-437.1875,-604.0625,973.21875,-6.5759801865,-9.2346811295,-8.572303772,-11.8443632126,"['CIRT: GLOBAL SUBSEASONAL-TO-SEASONAL FORE-\nCASTING WITH GEOMETRY-INSPIRED TRANSFORMER\n\nYang Liu1,2∗, Zinan Zheng1∗, Jiashun Cheng1,2, Fugee Tsung1,2, Deli Zhao3, Yu Rong3†, Jia Li1,2†\n1The Hong Kong University of Science and Technology (Guangzhou)\n2The Hong Kong University of Science and Technology 3DAMO Academy, Alibaba Group\n\nABSTRACT\n\n(S2S) climate forecasting is pivotal\n\nfor\nAccurate Subseasonal-to-Seasonal\ndecision-making including agriculture planning and disaster preparedness but is\nknown to be challenging due to its chaotic nature. Although recent data-driven\nmodels have shown promising results, their performance is limited by inadequate\nconsideration of geometric inductive biases. Usually, they treat the spherical\nweather data as planar images, resulting in an inaccurate representation of lo-\ncations and spatial relations.\nIn this work, we propose the geometric-inspired\nCircular Transformer (CirT) to model the cyclic characteristic of the graticule,\nconsisting of two key designs: (1) Decomposing the weather data by latitude\ninto circular patches that serve as input tokens to the Transformer; (2) Leveraging\nFourier transform in self-attention to capture the global information and model the\nspatial periodicity. Extensive experiments on the Earth Reanalysis 5 (ERA5) re-\nanalysis dataset demonstrate our model yields a significant improvement over the\nadvanced data-driven models, including PanguWeather and GraphCast, as well as\nskillful ECMWF systems. Additionally, we empirically show the effectiveness of\nour model designs and high-quality prediction over spatial and temporal dimen-\nsions. The code link is: https://github.com/compasszzn/CirT.', 'REGENESIS: LLMS CAN GROW INTO REASONING GEN-\nERALISTS VIA SELF-IMPROVEMENT\n\nXiangyu Peng Congying Xia∗† Xinyi Yang∗ Caiming Xiong Chien-Sheng Wu Chen Xing†\n\n{becky.peng, c.xia, x.yang, cxiong, wu.json, cxing}@salesforce.com\n\nSalesforce AI Research\n\nABSTRACT\n\nPost-training Large Language Models (LLMs) with explicit reasoning trajectories\ncan enhance their reasoning abilities. However, acquiring such high-quality tra-\njectory data typically demands meticulous supervision from humans or superior\nmodels, which can be either expensive or license-constrained. In this paper, we\nexplore how far an LLM can improve its reasoning by self-synthesizing reasoning\npaths as training data without any additional supervision. Existing self-synthesizing\nmethods, such as STaR, suffer from poor generalization to out-of-domain (OOD)\nreasoning tasks. We hypothesize it is due to that their self-synthesized reasoning\npaths are too task-specific, lacking general task-agnostic reasoning guidance. To\naddress this, we propose Reasoning Generalist via Self-Improvement (ReGenesis),\na method to self-synthesize reasoning paths as post-training data by progressing\nfrom abstract to concrete. More specifically, ReGenesis self-synthesizes reasoning\npaths by converting general reasoning guidelines into task-specific ones, generating\nreasoning structures, and subsequently transforming these structures into reason-\ning paths, without the need for human-designed task-specific examples used in\nexisting methods. We show that ReGenesis achieves superior performance on all\nin-domain and OOD settings tested compared to existing methods. For six OOD\ntasks specifically, while previous methods exhibited an average performance de-\ncrease of approximately 4.6% after post training, ReGenesis delivers around 6.1%\nperformance improvement. We also conduct in-depth analysis of our framework\nand show ReGenesis is effective across various LLMs and design choices.']",369.15625,-234.90625,-101.8125,19.0827207566,7.238357544,-4.606005668599998,-1.9963235854999999,YslOW2SO6S,YUYJsHOf3c,"['weather and climate forecasting', 'llm', 'reasoning', 'generalization', 'self-improvement']",True,"CIRT: GLOBAL SUBSEASONAL-TO-SEASONAL FORE-
CASTING WITH GEOMETRY-INSPIRED TRANSFORMER

Yang Liu1,2∗, Zinan Zheng1∗, Jiashun Cheng1,2, Fugee Tsung1,2, Deli Zhao3, Yu Rong3†, Jia Li1,2†
1The Hong Kong University of Science and Technology (Guangzhou)
2The Hong Kong University of Science and Technology 3DAMO Academy, Alibaba Group

ABSTRACT

(S2S) climate forecasting is pivotal

for
Accurate Subseasonal-to-Seasonal
decision-making including agriculture planning and disaster preparedness but is
known to be challenging due to its chaotic nature. Although recent data-driven
models have shown promising results, their performance is limited by inadequate
consideration of geometric inductive biases. Usually, they treat the spherical
weather data as planar images, resulting in an inaccurate representation of lo-
cations and spatial relations.
In this work, we propose the geometric-inspired
Circular Transformer (CirT) to model the cyclic characteristic of the graticule,
consisting of two key designs: (1) Decomposing the weather data by latitude
into circular patches that serve as input tokens to the Transformer; (2) Leveraging
Fourier transform in self-attention to capture the global information and model the
spatial periodicity. Extensive experiments on the Earth Reanalysis 5 (ERA5) re-
analysis dataset demonstrate our model yields a significant improvement over the
advanced data-driven models, including PanguWeather and GraphCast, as well as
skillful ECMWF systems. Additionally, we empirically show the effectiveness of
our model designs and high-quality prediction over spatial and temporal dimen-
sions. The code link is: https://github.com/compasszzn/CirT.","REGENESIS: LLMS CAN GROW INTO REASONING GEN-
ERALISTS VIA SELF-IMPROVEMENT

Xiangyu Peng Congying Xia∗† Xinyi Yang∗ Caiming Xiong Chien-Sheng Wu Chen Xing†

{becky.peng, c.xia, x.yang, cxiong, wu.json, cxing}@salesforce.com

Salesforce AI Research

ABSTRACT

Post-training Large Language Models (LLMs) with explicit reasoning trajectories
can enhance their reasoning abilities. However, acquiring such high-quality tra-
jectory data typically demands meticulous supervision from humans or superior
models, which can be either expensive or license-constrained. In this paper, we
explore how far an LLM can improve its reasoning by self-synthesizing reasoning
paths as training data without any additional supervision. Existing self-synthesizing
methods, such as STaR, suffer from poor generalization to out-of-domain (OOD)
reasoning tasks. We hypothesize it is due to that their self-synthesized reasoning
paths are too task-specific, lacking general task-agnostic reasoning guidance. To
address this, we propose Reasoning Generalist via Self-Improvement (ReGenesis),
a method to self-synthesize reasoning paths as post-training data by progressing
from abstract to concrete. More specifically, ReGenesis self-synthesizes reasoning
paths by converting general reasoning guidelines into task-specific ones, generating
reasoning structures, and subsequently transforming these structures into reason-
ing paths, without the need for human-designed task-specific examples used in
existing methods. We show that ReGenesis achieves superior performance on all
in-domain and OOD settings tested compared to existing methods. For six OOD
tasks specifically, while previous methods exhibited an average performance de-
crease of approximately 4.6% after post training, ReGenesis delivers around 6.1%
performance improvement. We also conduct in-depth analysis of our framework
and show ReGenesis is effective across various LLMs and design choices.",26
145,"<insight>Machine learning models designed for complex generation or alignment tasks can achieve enhanced outcomes, such as greater diversity or better adherence to specific values, by employing mechanisms that explicitly separate foundational structures or behaviors from the assignment of specific features or value-based preferences.</insight>",-495.3125,-409.34375,-479.25,-419.203125,844.609375,-10.5385637283,-8.709441185,-10.196808815,-8.9192152023,"['MAGNET: MOTIF-AGNOSTIC GENERATION OF\nMOLECULES FROM SCAFFOLDS\n\nLeon Hetzel∗1(cid:57)3, Johanna Sommer∗1,2, Bastian Rieck1,3,4, Fabian Theis1(cid:57)3 & Stephan G ¨unnemann1,2\n1 School of Computation, Information and Technology, Technical University of Munich\n2 Munich Data Science Institute, Technical University of Munich\n3 Center for Computation Health, Helmholtz Munich\n4 Department of Computer Science, University of Fribourg\n{l.hetzel, jm.sommer, b.rieck, f.theis, s.guennemann}@tum.de\n\nABSTRACT\n\nRecent advances in machine learning for molecules exhibit great potential for\nfacilitating drug discovery from in silico predictions. Most models for molecule\ngeneration rely on the decomposition of molecules into frequently occurring sub-\nstructures (motifs), from which they generate novel compounds. While motif\nrepresentations greatly aid in learning molecular distributions, such methods fail\nto represent substructures beyond their known motif set, posing a fundamental\nlimitation for discovering novel compounds. To address this limitation and enhance\nstructural expressivity, we propose to separate structure from features by abstract-\ning motifs to scaffolds and, subsequently, allocating atom and bond types. To this\nend, we introduce a novel factorisation of the molecules’ data distribution that\nconsiders the entire molecular context and facilitates learning adequate assignments\nof atoms and bonds to scaffolds. Complementary to this, we propose MAGNet,\nthe first model to freely learn motifs. Importantly, we demonstrate that MAGNet’s\nimproved expressivity leads to molecules with more structural diversity and, at the\nsame time, diverse atom and bond assignments.', 'MACPO: WEAK-TO-STRONG ALIGNMENT VIA MULTI-\nAGENT CONTRASTIVE PREFERENCE OPTIMIZATION\n\nYougang Lyu1\n\nLingyong Yan2\n\nZihan Wang1\n\nDawei Yin2\n\nPengjie Ren3\n\nMaarten de Rijke1\n\nZhaochun Ren4∗\n\n1University of Amsterdam\n3Shandong University\n{youganglyu,lingyongy,zihanwang.sdu}@gmail.com, yindawei@acm.org\njay.ren@outlook.com, m.derijke@uva.nl, z.ren@liacs.leidenuniv.nl\n\n2Baidu Inc.\n\n4Leiden University\n\nABSTRACT\n\nAs large language models (LLMs) are rapidly advancing and achieving near-human\ncapabilities on specific tasks, aligning them with human values is becoming more\nurgent. In scenarios where LLMs outperform humans, we face a weak-to-strong\nalignment problem where we need to effectively align strong student LLMs through\nweak supervision generated by weak teachers. Existing alignment methods mainly\nfocus on strong-to-weak alignment and self-alignment settings, and it is impractical\nto adapt them to the much harder weak-to-strong alignment setting. To fill this gap,\nwe propose a multi-agent contrastive preference optimization (MACPO) framework.\nMACPO facilitates weak teachers and strong students to learn from each other\nby iteratively reinforcing unfamiliar positive behaviors while penalizing familiar\nnegative ones. To get this, we devise a mutual positive behavior augmentation\nstrategy to encourage weak teachers and strong students to learn from each other’s\npositive behavior and further provide higher quality positive behavior for the next\niteration. Additionally, we propose a hard negative behavior construction strategy\nto induce weak teachers and strong students to generate familiar negative behavior\nby fine-tuning on negative behavioral data. Experimental results on the HH-RLHF\nand PKU-SafeRLHF datasets, evaluated using both automatic metrics and human\njudgments, demonstrate that MACPO simultaneously improves the alignment\nperformance of strong students and weak teachers. Moreover, as the number\nof weak teachers increases, MACPO achieves better weak-to-strong alignment\nperformance through more iteration optimization rounds.']",425.40625,6.203125,-69.90625,17.9704113006,9.051196098299998,0.13198089599999996,-1.4873676300000014,5FXKgOxmb2,x1Okv4kbVR,"['graph generative models', '2d molecules', 'weak-to-strong alignment', 'preference optimization']",True,"MAGNET: MOTIF-AGNOSTIC GENERATION OF
MOLECULES FROM SCAFFOLDS

Leon Hetzel∗1(cid:57)3, Johanna Sommer∗1,2, Bastian Rieck1,3,4, Fabian Theis1(cid:57)3 & Stephan G ¨unnemann1,2
1 School of Computation, Information and Technology, Technical University of Munich
2 Munich Data Science Institute, Technical University of Munich
3 Center for Computation Health, Helmholtz Munich
4 Department of Computer Science, University of Fribourg
{l.hetzel, jm.sommer, b.rieck, f.theis, s.guennemann}@tum.de

ABSTRACT

Recent advances in machine learning for molecules exhibit great potential for
facilitating drug discovery from in silico predictions. Most models for molecule
generation rely on the decomposition of molecules into frequently occurring sub-
structures (motifs), from which they generate novel compounds. While motif
representations greatly aid in learning molecular distributions, such methods fail
to represent substructures beyond their known motif set, posing a fundamental
limitation for discovering novel compounds. To address this limitation and enhance
structural expressivity, we propose to separate structure from features by abstract-
ing motifs to scaffolds and, subsequently, allocating atom and bond types. To this
end, we introduce a novel factorisation of the molecules’ data distribution that
considers the entire molecular context and facilitates learning adequate assignments
of atoms and bonds to scaffolds. Complementary to this, we propose MAGNet,
the first model to freely learn motifs. Importantly, we demonstrate that MAGNet’s
improved expressivity leads to molecules with more structural diversity and, at the
same time, diverse atom and bond assignments.","MACPO: WEAK-TO-STRONG ALIGNMENT VIA MULTI-
AGENT CONTRASTIVE PREFERENCE OPTIMIZATION

Yougang Lyu1

Lingyong Yan2

Zihan Wang1

Dawei Yin2

Pengjie Ren3

Maarten de Rijke1

Zhaochun Ren4∗

1University of Amsterdam
3Shandong University
{youganglyu,lingyongy,zihanwang.sdu}@gmail.com, yindawei@acm.org
jay.ren@outlook.com, m.derijke@uva.nl, z.ren@liacs.leidenuniv.nl

2Baidu Inc.

4Leiden University

ABSTRACT

As large language models (LLMs) are rapidly advancing and achieving near-human
capabilities on specific tasks, aligning them with human values is becoming more
urgent. In scenarios where LLMs outperform humans, we face a weak-to-strong
alignment problem where we need to effectively align strong student LLMs through
weak supervision generated by weak teachers. Existing alignment methods mainly
focus on strong-to-weak alignment and self-alignment settings, and it is impractical
to adapt them to the much harder weak-to-strong alignment setting. To fill this gap,
we propose a multi-agent contrastive preference optimization (MACPO) framework.
MACPO facilitates weak teachers and strong students to learn from each other
by iteratively reinforcing unfamiliar positive behaviors while penalizing familiar
negative ones. To get this, we devise a mutual positive behavior augmentation
strategy to encourage weak teachers and strong students to learn from each other’s
positive behavior and further provide higher quality positive behavior for the next
iteration. Additionally, we propose a hard negative behavior construction strategy
to induce weak teachers and strong students to generate familiar negative behavior
by fine-tuning on negative behavioral data. Experimental results on the HH-RLHF
and PKU-SafeRLHF datasets, evaluated using both automatic metrics and human
judgments, demonstrate that MACPO simultaneously improves the alignment
performance of strong students and weak teachers. Moreover, as the number
of weak teachers increases, MACPO achieves better weak-to-strong alignment
performance through more iteration optimization rounds.",27
